I0209 09:07:51.607027      25 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-815025086
I0209 09:07:51.607066      25 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0209 09:07:51.607323      25 e2e.go:129] Starting e2e run "7d186960-2624-4d69-aed6-1f0d1a2b9b5a" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1612861669 - Will randomize all specs
Will run 311 of 5667 specs

E0209 09:07:51.635641      25 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
Feb  9 09:07:51.635: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:07:51.638: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb  9 09:07:51.693: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb  9 09:07:51.765: INFO: 21 / 21 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb  9 09:07:51.765: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Feb  9 09:07:51.765: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb  9 09:07:51.780: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb  9 09:07:51.780: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Feb  9 09:07:51.780: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
Feb  9 09:07:51.780: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'magnum-auto-healer' (0 seconds elapsed)
Feb  9 09:07:51.780: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
Feb  9 09:07:51.780: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Feb  9 09:07:51.780: INFO: e2e test version: v1.20.2
Feb  9 09:07:51.783: INFO: kube-apiserver version: v1.20.2
Feb  9 09:07:51.783: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:07:51.789: INFO: Cluster IP family: ipv4
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:07:51.789: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
Feb  9 09:07:51.828: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Feb  9 09:07:51.840: INFO: PSP annotation exists on dry run pod: "magnum.privileged"; assuming PodSecurityPolicy is enabled
Feb  9 09:07:51.858: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1299
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1299
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1299
STEP: creating replication controller externalsvc in namespace services-1299
I0209 09:07:52.031025      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1299, replica count: 2
I0209 09:07:55.082334      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 09:07:58.082668      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 09:08:01.083020      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb  9 09:08:01.126: INFO: Creating new exec pod
Feb  9 09:08:03.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-1299 exec execpodph9m5 -- /bin/sh -x -c nslookup clusterip-service.services-1299.svc.cluster.local'
Feb  9 09:08:03.900: INFO: stderr: "+ nslookup clusterip-service.services-1299.svc.cluster.local\n"
Feb  9 09:08:03.900: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-1299.svc.cluster.local\tcanonical name = externalsvc.services-1299.svc.cluster.local.\nName:\texternalsvc.services-1299.svc.cluster.local\nAddress: 10.254.41.78\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1299, will wait for the garbage collector to delete the pods
Feb  9 09:08:03.965: INFO: Deleting ReplicationController externalsvc took: 10.627357ms
Feb  9 09:08:04.966: INFO: Terminating ReplicationController externalsvc pods took: 1.000256452s
Feb  9 09:09:03.704: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:09:03.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1299" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:71.951 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":1,"skipped":10,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:09:03.741: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:09:03.977: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb  9 09:09:06.056: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:09:07.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5779" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":2,"skipped":30,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:09:07.081: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-qbt2
STEP: Creating a pod to test atomic-volume-subpath
Feb  9 09:09:07.273: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qbt2" in namespace "subpath-844" to be "Succeeded or Failed"
Feb  9 09:09:07.282: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.343096ms
Feb  9 09:09:09.294: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 2.020896223s
Feb  9 09:09:11.307: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 4.034396888s
Feb  9 09:09:13.317: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 6.044533551s
Feb  9 09:09:15.331: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 8.058317644s
Feb  9 09:09:17.341: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 10.068471926s
Feb  9 09:09:19.348: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 12.074728941s
Feb  9 09:09:21.369: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 14.096154549s
Feb  9 09:09:23.384: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 16.111112407s
Feb  9 09:09:25.396: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 18.122821612s
Feb  9 09:09:27.404: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 20.131349313s
Feb  9 09:09:29.420: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Running", Reason="", readiness=true. Elapsed: 22.147331431s
Feb  9 09:09:31.434: INFO: Pod "pod-subpath-test-downwardapi-qbt2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.161087992s
STEP: Saw pod success
Feb  9 09:09:31.434: INFO: Pod "pod-subpath-test-downwardapi-qbt2" satisfied condition "Succeeded or Failed"
Feb  9 09:09:31.438: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-subpath-test-downwardapi-qbt2 container test-container-subpath-downwardapi-qbt2: <nil>
STEP: delete the pod
Feb  9 09:09:31.574: INFO: Waiting for pod pod-subpath-test-downwardapi-qbt2 to disappear
Feb  9 09:09:31.580: INFO: Pod pod-subpath-test-downwardapi-qbt2 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qbt2
Feb  9 09:09:31.580: INFO: Deleting pod "pod-subpath-test-downwardapi-qbt2" in namespace "subpath-844"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:09:31.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-844" for this suite.

• [SLOW TEST:24.514 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":3,"skipped":42,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:09:31.596: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8567
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:09:42.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8567" for this suite.

• [SLOW TEST:11.300 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":4,"skipped":59,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:09:42.901: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5197
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Feb  9 09:09:43.067: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5197 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:09:43.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5197" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":5,"skipped":114,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:09:43.185: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb  9 09:09:43.348: INFO: Waiting up to 5m0s for pod "pod-d5ddbf1a-a2d2-4887-8897-73106fe215c8" in namespace "emptydir-4111" to be "Succeeded or Failed"
Feb  9 09:09:43.359: INFO: Pod "pod-d5ddbf1a-a2d2-4887-8897-73106fe215c8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.513636ms
Feb  9 09:09:45.372: INFO: Pod "pod-d5ddbf1a-a2d2-4887-8897-73106fe215c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023854653s
STEP: Saw pod success
Feb  9 09:09:45.372: INFO: Pod "pod-d5ddbf1a-a2d2-4887-8897-73106fe215c8" satisfied condition "Succeeded or Failed"
Feb  9 09:09:45.375: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-d5ddbf1a-a2d2-4887-8897-73106fe215c8 container test-container: <nil>
STEP: delete the pod
Feb  9 09:09:45.496: INFO: Waiting for pod pod-d5ddbf1a-a2d2-4887-8897-73106fe215c8 to disappear
Feb  9 09:09:45.500: INFO: Pod pod-d5ddbf1a-a2d2-4887-8897-73106fe215c8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:09:45.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4111" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":126,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:09:45.514: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1213
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1213
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1213
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1213
Feb  9 09:09:45.696: INFO: Found 0 stateful pods, waiting for 1
Feb  9 09:09:55.711: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb  9 09:10:05.717: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb  9 09:10:05.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  9 09:10:06.064: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  9 09:10:06.064: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  9 09:10:06.064: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  9 09:10:06.072: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb  9 09:10:16.088: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  9 09:10:16.088: INFO: Waiting for statefulset status.replicas updated to 0
Feb  9 09:10:16.122: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999951s
Feb  9 09:10:17.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.983224495s
Feb  9 09:10:18.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.972821811s
Feb  9 09:10:19.149: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.966276195s
Feb  9 09:10:20.161: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.956346488s
Feb  9 09:10:21.171: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.944150163s
Feb  9 09:10:22.182: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.933732461s
Feb  9 09:10:23.193: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.922723353s
Feb  9 09:10:24.197: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.91234069s
Feb  9 09:10:25.211: INFO: Verifying statefulset ss doesn't scale past 1 for another 907.761104ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1213
Feb  9 09:10:26.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:10:26.531: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  9 09:10:26.531: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  9 09:10:26.531: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  9 09:10:26.537: INFO: Found 1 stateful pods, waiting for 3
Feb  9 09:10:36.554: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 09:10:36.554: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 09:10:36.554: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb  9 09:10:36.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  9 09:10:36.872: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  9 09:10:36.873: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  9 09:10:36.873: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  9 09:10:36.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  9 09:10:37.225: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  9 09:10:37.225: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  9 09:10:37.225: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  9 09:10:37.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  9 09:10:37.580: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  9 09:10:37.580: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  9 09:10:37.580: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  9 09:10:37.580: INFO: Waiting for statefulset status.replicas updated to 0
Feb  9 09:10:37.588: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb  9 09:10:47.602: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  9 09:10:47.602: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb  9 09:10:47.602: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb  9 09:10:47.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999942s
Feb  9 09:10:48.629: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99381008s
Feb  9 09:10:49.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982844877s
Feb  9 09:10:50.651: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972673387s
Feb  9 09:10:51.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.960595528s
Feb  9 09:10:52.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.950723364s
Feb  9 09:10:53.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.938532785s
Feb  9 09:10:54.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.925409857s
Feb  9 09:10:55.708: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.917188675s
Feb  9 09:10:56.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.228204ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1213
Feb  9 09:10:57.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:10:58.027: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  9 09:10:58.027: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  9 09:10:58.027: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  9 09:10:58.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:10:58.340: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  9 09:10:58.340: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  9 09:10:58.340: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  9 09:10:58.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:10:58.606: INFO: rc: 1
Feb  9 09:10:58.606: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Feb  9 09:11:08.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:11:08.816: INFO: rc: 1
Feb  9 09:11:08.816: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb  9 09:11:18.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:11:19.030: INFO: rc: 1
Feb  9 09:11:19.030: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb  9 09:11:29.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:11:29.268: INFO: rc: 1
Feb  9 09:11:29.268: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb  9 09:11:39.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:11:39.520: INFO: rc: 1
Feb  9 09:11:39.520: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb  9 09:11:49.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:11:49.754: INFO: rc: 1
Feb  9 09:11:49.754: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb  9 09:11:59.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:11:59.882: INFO: rc: 1
Feb  9 09:11:59.882: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:12:09.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:12:09.993: INFO: rc: 1
Feb  9 09:12:09.993: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:12:19.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:12:20.128: INFO: rc: 1
Feb  9 09:12:20.128: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:12:30.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:12:30.251: INFO: rc: 1
Feb  9 09:12:30.251: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:12:40.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:12:40.390: INFO: rc: 1
Feb  9 09:12:40.390: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:12:50.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:12:50.537: INFO: rc: 1
Feb  9 09:12:50.537: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:13:00.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:13:00.680: INFO: rc: 1
Feb  9 09:13:00.680: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:13:10.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:13:10.810: INFO: rc: 1
Feb  9 09:13:10.810: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:13:20.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:13:20.939: INFO: rc: 1
Feb  9 09:13:20.939: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:13:30.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:13:31.070: INFO: rc: 1
Feb  9 09:13:31.070: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:13:41.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:13:41.203: INFO: rc: 1
Feb  9 09:13:41.203: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:13:51.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:13:51.341: INFO: rc: 1
Feb  9 09:13:51.341: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:14:01.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:14:01.464: INFO: rc: 1
Feb  9 09:14:01.464: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:14:11.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:14:11.594: INFO: rc: 1
Feb  9 09:14:11.594: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:14:21.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:14:21.742: INFO: rc: 1
Feb  9 09:14:21.742: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:14:31.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:14:31.862: INFO: rc: 1
Feb  9 09:14:31.862: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:14:41.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:14:41.991: INFO: rc: 1
Feb  9 09:14:41.991: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:14:51.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:14:52.108: INFO: rc: 1
Feb  9 09:14:52.108: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:15:02.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:15:02.242: INFO: rc: 1
Feb  9 09:15:02.242: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:15:12.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:15:12.377: INFO: rc: 1
Feb  9 09:15:12.377: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:15:22.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:15:22.498: INFO: rc: 1
Feb  9 09:15:22.498: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:15:32.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:15:32.627: INFO: rc: 1
Feb  9 09:15:32.627: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:15:42.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:15:42.743: INFO: rc: 1
Feb  9 09:15:42.743: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:15:52.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:15:52.857: INFO: rc: 1
Feb  9 09:15:52.857: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Feb  9 09:16:02.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-1213 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:16:02.975: INFO: rc: 1
Feb  9 09:16:02.976: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Feb  9 09:16:02.976: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb  9 09:16:03.011: INFO: Deleting all statefulset in ns statefulset-1213
Feb  9 09:16:03.017: INFO: Scaling statefulset ss to 0
Feb  9 09:16:03.030: INFO: Waiting for statefulset status.replicas updated to 0
Feb  9 09:16:03.033: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:16:03.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1213" for this suite.

• [SLOW TEST:377.556 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":7,"skipped":138,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:16:03.070: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-f8677ffc-1094-4429-9d81-8ed2d14f9af8
STEP: Creating a pod to test consume configMaps
Feb  9 09:16:03.244: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8090bc4a-a5e8-4c68-9651-ee0ad433f39a" in namespace "projected-741" to be "Succeeded or Failed"
Feb  9 09:16:03.257: INFO: Pod "pod-projected-configmaps-8090bc4a-a5e8-4c68-9651-ee0ad433f39a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.653964ms
Feb  9 09:16:05.265: INFO: Pod "pod-projected-configmaps-8090bc4a-a5e8-4c68-9651-ee0ad433f39a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020989441s
STEP: Saw pod success
Feb  9 09:16:05.265: INFO: Pod "pod-projected-configmaps-8090bc4a-a5e8-4c68-9651-ee0ad433f39a" satisfied condition "Succeeded or Failed"
Feb  9 09:16:05.269: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-projected-configmaps-8090bc4a-a5e8-4c68-9651-ee0ad433f39a container agnhost-container: <nil>
STEP: delete the pod
Feb  9 09:16:05.394: INFO: Waiting for pod pod-projected-configmaps-8090bc4a-a5e8-4c68-9651-ee0ad433f39a to disappear
Feb  9 09:16:05.396: INFO: Pod pod-projected-configmaps-8090bc4a-a5e8-4c68-9651-ee0ad433f39a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:16:05.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-741" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":156,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:16:05.404: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-427
Feb  9 09:16:09.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-427 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb  9 09:16:09.941: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb  9 09:16:09.941: INFO: stdout: "iptables"
Feb  9 09:16:09.941: INFO: proxyMode: iptables
Feb  9 09:16:09.959: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb  9 09:16:09.962: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-427
STEP: creating replication controller affinity-nodeport-timeout in namespace services-427
I0209 09:16:09.988860      25 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-427, replica count: 3
I0209 09:16:13.039539      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 09:16:16.039729      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 09:16:19.040053      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 09:16:19.061: INFO: Creating new exec pod
Feb  9 09:16:22.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-427 exec execpod-affinity8b9ch -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Feb  9 09:16:22.443: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb  9 09:16:22.443: INFO: stdout: ""
Feb  9 09:16:22.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-427 exec execpod-affinity8b9ch -- /bin/sh -x -c nc -zv -t -w 2 10.254.248.144 80'
Feb  9 09:16:22.767: INFO: stderr: "+ nc -zv -t -w 2 10.254.248.144 80\nConnection to 10.254.248.144 80 port [tcp/http] succeeded!\n"
Feb  9 09:16:22.767: INFO: stdout: ""
Feb  9 09:16:22.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-427 exec execpod-affinity8b9ch -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.5 30835'
Feb  9 09:16:23.086: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.5 30835\nConnection to 10.0.0.5 30835 port [tcp/30835] succeeded!\n"
Feb  9 09:16:23.086: INFO: stdout: ""
Feb  9 09:16:23.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-427 exec execpod-affinity8b9ch -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.176 30835'
Feb  9 09:16:23.394: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.176 30835\nConnection to 10.0.0.176 30835 port [tcp/30835] succeeded!\n"
Feb  9 09:16:23.394: INFO: stdout: ""
Feb  9 09:16:23.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-427 exec execpod-affinity8b9ch -- /bin/sh -x -c nc -zv -t -w 2 10.6.0.92 30835'
Feb  9 09:16:23.708: INFO: stderr: "+ nc -zv -t -w 2 10.6.0.92 30835\nConnection to 10.6.0.92 30835 port [tcp/30835] succeeded!\n"
Feb  9 09:16:23.708: INFO: stdout: ""
Feb  9 09:16:23.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-427 exec execpod-affinity8b9ch -- /bin/sh -x -c nc -zv -t -w 2 10.6.0.143 30835'
Feb  9 09:16:24.027: INFO: stderr: "+ nc -zv -t -w 2 10.6.0.143 30835\nConnection to 10.6.0.143 30835 port [tcp/30835] succeeded!\n"
Feb  9 09:16:24.027: INFO: stdout: ""
Feb  9 09:16:24.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-427 exec execpod-affinity8b9ch -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.248:30835/ ; done'
Feb  9 09:16:24.440: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n"
Feb  9 09:16:24.440: INFO: stdout: "\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c\naffinity-nodeport-timeout-ssz5c"
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.440: INFO: Received response from host: affinity-nodeport-timeout-ssz5c
Feb  9 09:16:24.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-427 exec execpod-affinity8b9ch -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.248:30835/'
Feb  9 09:16:24.751: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n"
Feb  9 09:16:24.751: INFO: stdout: "affinity-nodeport-timeout-ssz5c"
Feb  9 09:16:44.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-427 exec execpod-affinity8b9ch -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.248:30835/'
Feb  9 09:16:45.069: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.248:30835/\n"
Feb  9 09:16:45.069: INFO: stdout: "affinity-nodeport-timeout-qn5kn"
Feb  9 09:16:45.069: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-427, will wait for the garbage collector to delete the pods
Feb  9 09:16:45.160: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.587451ms
Feb  9 09:16:46.161: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 1.00041658s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:17:50.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-427" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:104.974 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":9,"skipped":172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:17:50.381: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2768
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-gn4m
STEP: Creating a pod to test atomic-volume-subpath
Feb  9 09:17:50.702: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-gn4m" in namespace "subpath-2768" to be "Succeeded or Failed"
Feb  9 09:17:50.710: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035733ms
Feb  9 09:17:52.720: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Running", Reason="", readiness=true. Elapsed: 2.018153397s
Feb  9 09:17:54.735: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Running", Reason="", readiness=true. Elapsed: 4.032933866s
Feb  9 09:17:56.746: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Running", Reason="", readiness=true. Elapsed: 6.044030657s
Feb  9 09:17:58.757: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Running", Reason="", readiness=true. Elapsed: 8.055712923s
Feb  9 09:18:00.765: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Running", Reason="", readiness=true. Elapsed: 10.063283637s
Feb  9 09:18:02.774: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Running", Reason="", readiness=true. Elapsed: 12.072304124s
Feb  9 09:18:04.788: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Running", Reason="", readiness=true. Elapsed: 14.086000706s
Feb  9 09:18:06.800: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Running", Reason="", readiness=true. Elapsed: 16.098464622s
Feb  9 09:18:08.813: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Running", Reason="", readiness=true. Elapsed: 18.111033887s
Feb  9 09:18:10.821: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Running", Reason="", readiness=true. Elapsed: 20.119747617s
Feb  9 09:18:12.834: INFO: Pod "pod-subpath-test-secret-gn4m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.132566249s
STEP: Saw pod success
Feb  9 09:18:12.834: INFO: Pod "pod-subpath-test-secret-gn4m" satisfied condition "Succeeded or Failed"
Feb  9 09:18:12.838: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-subpath-test-secret-gn4m container test-container-subpath-secret-gn4m: <nil>
STEP: delete the pod
Feb  9 09:18:12.910: INFO: Waiting for pod pod-subpath-test-secret-gn4m to disappear
Feb  9 09:18:12.913: INFO: Pod pod-subpath-test-secret-gn4m no longer exists
STEP: Deleting pod pod-subpath-test-secret-gn4m
Feb  9 09:18:12.913: INFO: Deleting pod "pod-subpath-test-secret-gn4m" in namespace "subpath-2768"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:18:12.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2768" for this suite.

• [SLOW TEST:22.545 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":10,"skipped":202,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:18:12.926: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:18:13.090: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7fc3f53e-aa52-486e-912e-1776f1a5c86b" in namespace "security-context-test-1681" to be "Succeeded or Failed"
Feb  9 09:18:13.104: INFO: Pod "busybox-privileged-false-7fc3f53e-aa52-486e-912e-1776f1a5c86b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.3448ms
Feb  9 09:18:15.109: INFO: Pod "busybox-privileged-false-7fc3f53e-aa52-486e-912e-1776f1a5c86b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019025297s
Feb  9 09:18:17.126: INFO: Pod "busybox-privileged-false-7fc3f53e-aa52-486e-912e-1776f1a5c86b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035778882s
Feb  9 09:18:19.139: INFO: Pod "busybox-privileged-false-7fc3f53e-aa52-486e-912e-1776f1a5c86b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04888174s
Feb  9 09:18:19.139: INFO: Pod "busybox-privileged-false-7fc3f53e-aa52-486e-912e-1776f1a5c86b" satisfied condition "Succeeded or Failed"
Feb  9 09:18:19.150: INFO: Got logs for pod "busybox-privileged-false-7fc3f53e-aa52-486e-912e-1776f1a5c86b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:18:19.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1681" for this suite.

• [SLOW TEST:6.241 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":11,"skipped":224,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:18:19.171: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7818
STEP: creating service affinity-clusterip-transition in namespace services-7818
STEP: creating replication controller affinity-clusterip-transition in namespace services-7818
I0209 09:18:19.357303      25 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-7818, replica count: 3
I0209 09:18:22.408881      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 09:18:25.409648      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 09:18:25.421: INFO: Creating new exec pod
Feb  9 09:18:28.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-7818 exec execpod-affinitylwc8z -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Feb  9 09:18:29.134: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb  9 09:18:29.134: INFO: stdout: ""
Feb  9 09:18:29.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-7818 exec execpod-affinitylwc8z -- /bin/sh -x -c nc -zv -t -w 2 10.254.184.248 80'
Feb  9 09:18:29.434: INFO: stderr: "+ nc -zv -t -w 2 10.254.184.248 80\nConnection to 10.254.184.248 80 port [tcp/http] succeeded!\n"
Feb  9 09:18:29.434: INFO: stdout: ""
Feb  9 09:18:29.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-7818 exec execpod-affinitylwc8z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.184.248:80/ ; done'
Feb  9 09:18:29.882: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n"
Feb  9 09:18:29.882: INFO: stdout: "\naffinity-clusterip-transition-phjhp\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-phjhp\naffinity-clusterip-transition-spttv\naffinity-clusterip-transition-phjhp\naffinity-clusterip-transition-spttv\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-spttv\naffinity-clusterip-transition-phjhp\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-phjhp\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-phjhp"
Feb  9 09:18:29.882: INFO: Received response from host: affinity-clusterip-transition-phjhp
Feb  9 09:18:29.882: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-phjhp
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-spttv
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-phjhp
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-spttv
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-spttv
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-phjhp
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-phjhp
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:29.883: INFO: Received response from host: affinity-clusterip-transition-phjhp
Feb  9 09:18:29.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-7818 exec execpod-affinitylwc8z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.184.248:80/ ; done'
Feb  9 09:18:30.456: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.184.248:80/\n"
Feb  9 09:18:30.457: INFO: stdout: "\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d\naffinity-clusterip-transition-gsj5d"
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Received response from host: affinity-clusterip-transition-gsj5d
Feb  9 09:18:30.457: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7818, will wait for the garbage collector to delete the pods
Feb  9 09:18:30.616: INFO: Deleting ReplicationController affinity-clusterip-transition took: 72.348955ms
Feb  9 09:18:31.617: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 1.000271995s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:19:24.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7818" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:65.800 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":12,"skipped":242,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:19:24.973: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3022
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb  9 09:19:25.179: INFO: Waiting up to 5m0s for pod "pod-cc43ca93-9bdd-4592-9fde-29f4f6487ab9" in namespace "emptydir-3022" to be "Succeeded or Failed"
Feb  9 09:19:25.183: INFO: Pod "pod-cc43ca93-9bdd-4592-9fde-29f4f6487ab9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.734617ms
Feb  9 09:19:27.196: INFO: Pod "pod-cc43ca93-9bdd-4592-9fde-29f4f6487ab9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017634244s
STEP: Saw pod success
Feb  9 09:19:27.196: INFO: Pod "pod-cc43ca93-9bdd-4592-9fde-29f4f6487ab9" satisfied condition "Succeeded or Failed"
Feb  9 09:19:27.199: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-cc43ca93-9bdd-4592-9fde-29f4f6487ab9 container test-container: <nil>
STEP: delete the pod
Feb  9 09:19:27.226: INFO: Waiting for pod pod-cc43ca93-9bdd-4592-9fde-29f4f6487ab9 to disappear
Feb  9 09:19:27.232: INFO: Pod pod-cc43ca93-9bdd-4592-9fde-29f4f6487ab9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:19:27.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3022" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":13,"skipped":246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:19:27.250: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4481
STEP: Creating secret with name secret-test-c9b71f85-fec3-4344-98f1-2cced5fbd24d
STEP: Creating a pod to test consume secrets
Feb  9 09:19:27.596: INFO: Waiting up to 5m0s for pod "pod-secrets-87863f40-b97b-4cd9-9df1-f6328c2f998a" in namespace "secrets-2784" to be "Succeeded or Failed"
Feb  9 09:19:27.605: INFO: Pod "pod-secrets-87863f40-b97b-4cd9-9df1-f6328c2f998a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.847141ms
Feb  9 09:19:29.610: INFO: Pod "pod-secrets-87863f40-b97b-4cd9-9df1-f6328c2f998a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013771167s
STEP: Saw pod success
Feb  9 09:19:29.610: INFO: Pod "pod-secrets-87863f40-b97b-4cd9-9df1-f6328c2f998a" satisfied condition "Succeeded or Failed"
Feb  9 09:19:29.613: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-secrets-87863f40-b97b-4cd9-9df1-f6328c2f998a container secret-volume-test: <nil>
STEP: delete the pod
Feb  9 09:19:29.639: INFO: Waiting for pod pod-secrets-87863f40-b97b-4cd9-9df1-f6328c2f998a to disappear
Feb  9 09:19:29.644: INFO: Pod pod-secrets-87863f40-b97b-4cd9-9df1-f6328c2f998a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:19:29.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2784" for this suite.
STEP: Destroying namespace "secret-namespace-4481" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":330,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:19:29.669: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb  9 09:19:29.848: INFO: Waiting up to 5m0s for pod "pod-789ceef2-0ba5-4c0f-abc0-7e28de743e19" in namespace "emptydir-5262" to be "Succeeded or Failed"
Feb  9 09:19:29.853: INFO: Pod "pod-789ceef2-0ba5-4c0f-abc0-7e28de743e19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968433ms
Feb  9 09:19:31.863: INFO: Pod "pod-789ceef2-0ba5-4c0f-abc0-7e28de743e19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015581486s
STEP: Saw pod success
Feb  9 09:19:31.864: INFO: Pod "pod-789ceef2-0ba5-4c0f-abc0-7e28de743e19" satisfied condition "Succeeded or Failed"
Feb  9 09:19:31.867: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-789ceef2-0ba5-4c0f-abc0-7e28de743e19 container test-container: <nil>
STEP: delete the pod
Feb  9 09:19:31.893: INFO: Waiting for pod pod-789ceef2-0ba5-4c0f-abc0-7e28de743e19 to disappear
Feb  9 09:19:31.900: INFO: Pod pod-789ceef2-0ba5-4c0f-abc0-7e28de743e19 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:19:31.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5262" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":15,"skipped":348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:19:31.919: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-13d6d412-8e3e-4431-82ac-710597d9700a in namespace container-probe-4429
Feb  9 09:19:34.125: INFO: Started pod liveness-13d6d412-8e3e-4431-82ac-710597d9700a in namespace container-probe-4429
STEP: checking the pod's current state and verifying that restartCount is present
Feb  9 09:19:34.129: INFO: Initial restart count of pod liveness-13d6d412-8e3e-4431-82ac-710597d9700a is 0
Feb  9 09:19:52.227: INFO: Restart count of pod container-probe-4429/liveness-13d6d412-8e3e-4431-82ac-710597d9700a is now 1 (18.097853422s elapsed)
Feb  9 09:20:12.344: INFO: Restart count of pod container-probe-4429/liveness-13d6d412-8e3e-4431-82ac-710597d9700a is now 2 (38.215487565s elapsed)
Feb  9 09:20:30.480: INFO: Restart count of pod container-probe-4429/liveness-13d6d412-8e3e-4431-82ac-710597d9700a is now 3 (56.351277736s elapsed)
Feb  9 09:20:50.624: INFO: Restart count of pod container-probe-4429/liveness-13d6d412-8e3e-4431-82ac-710597d9700a is now 4 (1m16.495133083s elapsed)
Feb  9 09:21:51.027: INFO: Restart count of pod container-probe-4429/liveness-13d6d412-8e3e-4431-82ac-710597d9700a is now 5 (2m16.898243729s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:21:51.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4429" for this suite.

• [SLOW TEST:139.142 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":384,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:21:51.061: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7187
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-bcfc5f64-e44d-4eb4-81a7-91b519e368bf
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:21:57.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7187" for this suite.

• [SLOW TEST:6.297 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":17,"skipped":388,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:21:57.362: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7568
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb  9 09:21:57.548: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  9 09:22:57.620: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:22:57.625: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-9900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:22:57.805: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Feb  9 09:22:57.809: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:22:57.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9900" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:22:57.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7568" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.549 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":18,"skipped":390,"failed":0}
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:22:57.911: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3386
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Feb  9 09:22:58.089: INFO: Waiting up to 5m0s for pod "pod-a8907370-000d-4e39-9b1f-2cd49bb1e513" in namespace "emptydir-3386" to be "Succeeded or Failed"
Feb  9 09:22:58.115: INFO: Pod "pod-a8907370-000d-4e39-9b1f-2cd49bb1e513": Phase="Pending", Reason="", readiness=false. Elapsed: 25.779538ms
Feb  9 09:23:00.135: INFO: Pod "pod-a8907370-000d-4e39-9b1f-2cd49bb1e513": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045443955s
STEP: Saw pod success
Feb  9 09:23:00.135: INFO: Pod "pod-a8907370-000d-4e39-9b1f-2cd49bb1e513" satisfied condition "Succeeded or Failed"
Feb  9 09:23:00.139: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-a8907370-000d-4e39-9b1f-2cd49bb1e513 container test-container: <nil>
STEP: delete the pod
Feb  9 09:23:00.299: INFO: Waiting for pod pod-a8907370-000d-4e39-9b1f-2cd49bb1e513 to disappear
Feb  9 09:23:00.303: INFO: Pod pod-a8907370-000d-4e39-9b1f-2cd49bb1e513 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:23:00.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3386" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":19,"skipped":390,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:23:00.316: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4417
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Feb  9 09:23:00.501: INFO: Found 0 stateful pods, waiting for 3
Feb  9 09:23:10.523: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 09:23:10.523: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 09:23:10.523: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 09:23:10.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-4417 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  9 09:23:10.875: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  9 09:23:10.875: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  9 09:23:10.875: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb  9 09:23:20.937: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb  9 09:23:30.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-4417 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:23:31.303: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  9 09:23:31.303: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  9 09:23:31.303: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  9 09:23:41.352: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:23:41.353: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:23:41.353: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:23:41.353: INFO: Waiting for Pod statefulset-4417/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:23:51.389: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:23:51.389: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:23:51.389: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:24:01.378: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:24:01.378: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:24:01.378: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:24:11.371: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:24:11.371: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:24:11.371: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:24:21.377: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:24:21.377: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:24:31.388: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:24:31.388: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:24:41.364: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:24:41.364: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:24:51.370: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:24:51.370: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:25:01.386: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:25:01.386: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 09:25:11.383: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
STEP: Rolling back to a previous revision
Feb  9 09:25:21.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-4417 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  9 09:25:21.721: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  9 09:25:21.721: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  9 09:25:21.721: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  9 09:25:31.770: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb  9 09:25:41.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-4417 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 09:25:42.120: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  9 09:25:42.120: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  9 09:25:42.120: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  9 09:25:52.162: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:25:52.162: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:25:52.162: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:25:52.162: INFO: Waiting for Pod statefulset-4417/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:02.191: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:26:02.191: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:02.191: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:02.191: INFO: Waiting for Pod statefulset-4417/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:12.181: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:26:12.181: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:12.181: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:12.181: INFO: Waiting for Pod statefulset-4417/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:22.181: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:26:22.181: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:22.181: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:22.181: INFO: Waiting for Pod statefulset-4417/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:32.194: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:26:32.194: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:32.194: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:32.194: INFO: Waiting for Pod statefulset-4417/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:42.185: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:26:42.185: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:42.185: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:42.185: INFO: Waiting for Pod statefulset-4417/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:52.287: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:26:52.287: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:26:52.287: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:27:02.191: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:27:02.191: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:27:02.191: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:27:12.193: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:27:12.193: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:27:12.193: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:27:22.177: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:27:22.177: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:27:22.177: INFO: Waiting for Pod statefulset-4417/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:27:32.187: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:27:32.187: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:27:42.185: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:27:42.185: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:27:52.193: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:27:52.193: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb  9 09:28:02.188: INFO: Waiting for StatefulSet statefulset-4417/ss2 to complete update
Feb  9 09:28:02.188: INFO: Waiting for Pod statefulset-4417/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb  9 09:28:12.176: INFO: Deleting all statefulset in ns statefulset-4417
Feb  9 09:28:12.180: INFO: Scaling statefulset ss2 to 0
Feb  9 09:30:12.221: INFO: Waiting for statefulset status.replicas updated to 0
Feb  9 09:30:12.226: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:30:12.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4417" for this suite.

• [SLOW TEST:431.948 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":20,"skipped":393,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:30:12.265: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:30:25.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2741" for this suite.

• [SLOW TEST:13.312 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":21,"skipped":421,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:30:25.577: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-1432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb  9 09:30:25.868: INFO: starting watch
STEP: patching
STEP: updating
Feb  9 09:30:25.893: INFO: waiting for watch events with expected annotations
Feb  9 09:30:25.893: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:30:25.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1432" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":22,"skipped":423,"failed":0}
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:30:25.939: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:30:26.151: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2300
I0209 09:30:26.186067      25 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2300, replica count: 1
I0209 09:30:27.237022      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 09:30:28.238519      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 09:30:29.239897      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 09:30:30.240335      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 09:30:31.240892      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 09:30:32.243388      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 09:30:32.357: INFO: Created: latency-svc-f64qw
Feb  9 09:30:32.366: INFO: Got endpoints: latency-svc-f64qw [22.772631ms]
Feb  9 09:30:32.384: INFO: Created: latency-svc-7jk2c
Feb  9 09:30:32.395: INFO: Got endpoints: latency-svc-7jk2c [28.475665ms]
Feb  9 09:30:32.396: INFO: Created: latency-svc-dkcnn
Feb  9 09:30:32.399: INFO: Created: latency-svc-chn6q
Feb  9 09:30:32.400: INFO: Got endpoints: latency-svc-dkcnn [34.059013ms]
Feb  9 09:30:32.404: INFO: Created: latency-svc-6slzr
Feb  9 09:30:32.409: INFO: Got endpoints: latency-svc-chn6q [41.794295ms]
Feb  9 09:30:32.411: INFO: Created: latency-svc-27vkn
Feb  9 09:30:32.418: INFO: Got endpoints: latency-svc-27vkn [50.886802ms]
Feb  9 09:30:32.419: INFO: Got endpoints: latency-svc-6slzr [51.122258ms]
Feb  9 09:30:32.422: INFO: Created: latency-svc-v7j5l
Feb  9 09:30:32.426: INFO: Created: latency-svc-d7bbl
Feb  9 09:30:32.432: INFO: Got endpoints: latency-svc-v7j5l [64.433917ms]
Feb  9 09:30:32.439: INFO: Got endpoints: latency-svc-d7bbl [71.100321ms]
Feb  9 09:30:32.445: INFO: Created: latency-svc-55gkn
Feb  9 09:30:32.445: INFO: Got endpoints: latency-svc-55gkn [78.182291ms]
Feb  9 09:30:32.456: INFO: Created: latency-svc-f9ksx
Feb  9 09:30:32.459: INFO: Got endpoints: latency-svc-f9ksx [91.599087ms]
Feb  9 09:30:32.462: INFO: Created: latency-svc-rhjbw
Feb  9 09:30:32.469: INFO: Got endpoints: latency-svc-rhjbw [101.179038ms]
Feb  9 09:30:32.471: INFO: Created: latency-svc-l9mwv
Feb  9 09:30:32.476: INFO: Created: latency-svc-t6m6k
Feb  9 09:30:32.484: INFO: Got endpoints: latency-svc-t6m6k [115.902221ms]
Feb  9 09:30:32.489: INFO: Created: latency-svc-4rb42
Feb  9 09:30:32.494: INFO: Created: latency-svc-kh5ln
Feb  9 09:30:32.498: INFO: Got endpoints: latency-svc-l9mwv [130.097849ms]
Feb  9 09:30:32.499: INFO: Got endpoints: latency-svc-4rb42 [29.89765ms]
Feb  9 09:30:32.506: INFO: Created: latency-svc-z4dhh
Feb  9 09:30:32.506: INFO: Got endpoints: latency-svc-kh5ln [139.169657ms]
Feb  9 09:30:32.507: INFO: Created: latency-svc-fd6fd
Feb  9 09:30:32.515: INFO: Got endpoints: latency-svc-z4dhh [147.161566ms]
Feb  9 09:30:32.516: INFO: Created: latency-svc-j95zx
Feb  9 09:30:32.522: INFO: Created: latency-svc-c6qv7
Feb  9 09:30:32.523: INFO: Got endpoints: latency-svc-fd6fd [154.830168ms]
Feb  9 09:30:32.530: INFO: Created: latency-svc-j8764
Feb  9 09:30:32.530: INFO: Got endpoints: latency-svc-j95zx [134.951605ms]
Feb  9 09:30:32.534: INFO: Got endpoints: latency-svc-j8764 [125.862888ms]
Feb  9 09:30:32.534: INFO: Got endpoints: latency-svc-c6qv7 [133.890866ms]
Feb  9 09:30:32.541: INFO: Created: latency-svc-qfhdv
Feb  9 09:30:32.546: INFO: Created: latency-svc-8sf6f
Feb  9 09:30:32.555: INFO: Created: latency-svc-bfzsb
Feb  9 09:30:32.559: INFO: Got endpoints: latency-svc-qfhdv [140.259884ms]
Feb  9 09:30:32.559: INFO: Got endpoints: latency-svc-8sf6f [140.337814ms]
Feb  9 09:30:32.569: INFO: Created: latency-svc-ml6zb
Feb  9 09:30:32.569: INFO: Got endpoints: latency-svc-bfzsb [136.922601ms]
Feb  9 09:30:32.579: INFO: Created: latency-svc-vr7qc
Feb  9 09:30:32.579: INFO: Got endpoints: latency-svc-ml6zb [140.438162ms]
Feb  9 09:30:32.587: INFO: Got endpoints: latency-svc-vr7qc [141.020655ms]
Feb  9 09:30:32.658: INFO: Created: latency-svc-ccbjm
Feb  9 09:30:32.659: INFO: Created: latency-svc-dq9sn
Feb  9 09:30:32.659: INFO: Created: latency-svc-8w8ds
Feb  9 09:30:32.659: INFO: Created: latency-svc-dfpnq
Feb  9 09:30:32.659: INFO: Created: latency-svc-dj2bn
Feb  9 09:30:32.660: INFO: Created: latency-svc-fkqj9
Feb  9 09:30:32.660: INFO: Got endpoints: latency-svc-fkqj9 [176.349412ms]
Feb  9 09:30:32.661: INFO: Created: latency-svc-r4nzt
Feb  9 09:30:32.660: INFO: Created: latency-svc-44xrl
Feb  9 09:30:32.660: INFO: Got endpoints: latency-svc-ccbjm [201.154861ms]
Feb  9 09:30:32.661: INFO: Got endpoints: latency-svc-44xrl [163.121084ms]
Feb  9 09:30:32.662: INFO: Got endpoints: latency-svc-dj2bn [163.072885ms]
Feb  9 09:30:32.662: INFO: Got endpoints: latency-svc-dfpnq [147.212605ms]
Feb  9 09:30:32.663: INFO: Got endpoints: latency-svc-dq9sn [156.782596ms]
Feb  9 09:30:32.664: INFO: Created: latency-svc-j4jgh
Feb  9 09:30:32.667: INFO: Created: latency-svc-jrmth
Feb  9 09:30:32.669: INFO: Got endpoints: latency-svc-8w8ds [146.492682ms]
Feb  9 09:30:32.671: INFO: Got endpoints: latency-svc-r4nzt [140.57777ms]
Feb  9 09:30:32.689: INFO: Got endpoints: latency-svc-jrmth [154.818389ms]
Feb  9 09:30:32.690: INFO: Got endpoints: latency-svc-j4jgh [155.46054ms]
Feb  9 09:30:32.691: INFO: Created: latency-svc-9ms8j
Feb  9 09:30:32.691: INFO: Got endpoints: latency-svc-9ms8j [132.198817ms]
Feb  9 09:30:32.691: INFO: Created: latency-svc-5vd9s
Feb  9 09:30:32.701: INFO: Created: latency-svc-mz92c
Feb  9 09:30:32.703: INFO: Created: latency-svc-kk4v2
Feb  9 09:30:32.807: INFO: Created: latency-svc-jqtp6
Feb  9 09:30:32.812: INFO: Got endpoints: latency-svc-mz92c [253.150089ms]
Feb  9 09:30:32.812: INFO: Got endpoints: latency-svc-5vd9s [225.625922ms]
Feb  9 09:30:32.833: INFO: Created: latency-svc-46r5f
Feb  9 09:30:32.834: INFO: Got endpoints: latency-svc-kk4v2 [264.917524ms]
Feb  9 09:30:32.835: INFO: Created: latency-svc-8gz2t
Feb  9 09:30:32.838: INFO: Created: latency-svc-v7tvt
Feb  9 09:30:32.849: INFO: Created: latency-svc-ckhxd
Feb  9 09:30:32.858: INFO: Created: latency-svc-jbg2n
Feb  9 09:30:32.864: INFO: Got endpoints: latency-svc-jqtp6 [284.922768ms]
Feb  9 09:30:32.869: INFO: Created: latency-svc-jhdc7
Feb  9 09:30:32.878: INFO: Created: latency-svc-vdwbg
Feb  9 09:30:32.882: INFO: Created: latency-svc-6jwzc
Feb  9 09:30:32.887: INFO: Created: latency-svc-vxpn9
Feb  9 09:30:32.892: INFO: Created: latency-svc-wnrtz
Feb  9 09:30:32.896: INFO: Created: latency-svc-gxn5g
Feb  9 09:30:32.904: INFO: Created: latency-svc-64jmn
Feb  9 09:30:32.914: INFO: Created: latency-svc-r482v
Feb  9 09:30:32.918: INFO: Created: latency-svc-t6fcv
Feb  9 09:30:32.924: INFO: Created: latency-svc-bj4n2
Feb  9 09:30:32.925: INFO: Got endpoints: latency-svc-46r5f [264.40811ms]
Feb  9 09:30:32.941: INFO: Created: latency-svc-5kbd5
Feb  9 09:30:32.975: INFO: Got endpoints: latency-svc-8gz2t [313.360864ms]
Feb  9 09:30:32.986: INFO: Created: latency-svc-t46cj
Feb  9 09:30:33.017: INFO: Got endpoints: latency-svc-v7tvt [356.428324ms]
Feb  9 09:30:33.028: INFO: Created: latency-svc-9xd58
Feb  9 09:30:33.064: INFO: Got endpoints: latency-svc-ckhxd [402.154775ms]
Feb  9 09:30:33.078: INFO: Created: latency-svc-hv7jk
Feb  9 09:30:33.121: INFO: Got endpoints: latency-svc-jbg2n [457.862928ms]
Feb  9 09:30:33.130: INFO: Created: latency-svc-h6jv8
Feb  9 09:30:33.167: INFO: Got endpoints: latency-svc-jhdc7 [497.207791ms]
Feb  9 09:30:33.178: INFO: Created: latency-svc-ddh8d
Feb  9 09:30:33.217: INFO: Got endpoints: latency-svc-vdwbg [546.206894ms]
Feb  9 09:30:33.230: INFO: Created: latency-svc-5qgv7
Feb  9 09:30:33.269: INFO: Got endpoints: latency-svc-6jwzc [606.35333ms]
Feb  9 09:30:33.277: INFO: Created: latency-svc-8bxvh
Feb  9 09:30:33.317: INFO: Got endpoints: latency-svc-vxpn9 [627.936863ms]
Feb  9 09:30:33.333: INFO: Created: latency-svc-fxckd
Feb  9 09:30:33.371: INFO: Got endpoints: latency-svc-wnrtz [679.908052ms]
Feb  9 09:30:33.382: INFO: Created: latency-svc-wsb78
Feb  9 09:30:33.419: INFO: Got endpoints: latency-svc-gxn5g [729.105753ms]
Feb  9 09:30:33.428: INFO: Created: latency-svc-zqsbp
Feb  9 09:30:33.466: INFO: Got endpoints: latency-svc-64jmn [652.98606ms]
Feb  9 09:30:33.481: INFO: Created: latency-svc-5cxwt
Feb  9 09:30:33.515: INFO: Got endpoints: latency-svc-r482v [703.226637ms]
Feb  9 09:30:33.530: INFO: Created: latency-svc-jgfx2
Feb  9 09:30:33.569: INFO: Got endpoints: latency-svc-t6fcv [735.633658ms]
Feb  9 09:30:33.621: INFO: Got endpoints: latency-svc-bj4n2 [756.440232ms]
Feb  9 09:30:33.656: INFO: Created: latency-svc-drp7t
Feb  9 09:30:33.662: INFO: Created: latency-svc-d4z7v
Feb  9 09:30:33.665: INFO: Got endpoints: latency-svc-5kbd5 [740.548151ms]
Feb  9 09:30:33.674: INFO: Created: latency-svc-5tghk
Feb  9 09:30:33.716: INFO: Got endpoints: latency-svc-t46cj [741.365024ms]
Feb  9 09:30:33.727: INFO: Created: latency-svc-f7cgg
Feb  9 09:30:33.766: INFO: Got endpoints: latency-svc-9xd58 [748.350384ms]
Feb  9 09:30:33.776: INFO: Created: latency-svc-bwjz9
Feb  9 09:30:33.814: INFO: Got endpoints: latency-svc-hv7jk [749.444071ms]
Feb  9 09:30:33.826: INFO: Created: latency-svc-t25l6
Feb  9 09:30:33.863: INFO: Got endpoints: latency-svc-h6jv8 [741.880097ms]
Feb  9 09:30:33.878: INFO: Created: latency-svc-46vdx
Feb  9 09:30:33.914: INFO: Got endpoints: latency-svc-ddh8d [747.445914ms]
Feb  9 09:30:33.928: INFO: Created: latency-svc-vnwv6
Feb  9 09:30:33.964: INFO: Got endpoints: latency-svc-5qgv7 [746.938301ms]
Feb  9 09:30:33.979: INFO: Created: latency-svc-rzkn8
Feb  9 09:30:34.015: INFO: Got endpoints: latency-svc-8bxvh [745.510186ms]
Feb  9 09:30:34.029: INFO: Created: latency-svc-7p78j
Feb  9 09:30:34.065: INFO: Got endpoints: latency-svc-fxckd [747.509882ms]
Feb  9 09:30:34.077: INFO: Created: latency-svc-6r8tl
Feb  9 09:30:34.120: INFO: Got endpoints: latency-svc-wsb78 [749.43179ms]
Feb  9 09:30:34.128: INFO: Created: latency-svc-pfkqz
Feb  9 09:30:34.173: INFO: Got endpoints: latency-svc-zqsbp [753.468884ms]
Feb  9 09:30:34.182: INFO: Created: latency-svc-jmjrp
Feb  9 09:30:34.220: INFO: Got endpoints: latency-svc-5cxwt [753.632254ms]
Feb  9 09:30:34.237: INFO: Created: latency-svc-xnmjb
Feb  9 09:30:34.264: INFO: Got endpoints: latency-svc-jgfx2 [748.338724ms]
Feb  9 09:30:34.275: INFO: Created: latency-svc-2jp2h
Feb  9 09:30:34.321: INFO: Got endpoints: latency-svc-drp7t [699.894056ms]
Feb  9 09:30:34.331: INFO: Created: latency-svc-snqzp
Feb  9 09:30:34.368: INFO: Got endpoints: latency-svc-d4z7v [797.564314ms]
Feb  9 09:30:34.383: INFO: Created: latency-svc-g8rbv
Feb  9 09:30:34.417: INFO: Got endpoints: latency-svc-5tghk [751.445799ms]
Feb  9 09:30:34.429: INFO: Created: latency-svc-bq59j
Feb  9 09:30:34.481: INFO: Got endpoints: latency-svc-f7cgg [764.872785ms]
Feb  9 09:30:34.490: INFO: Created: latency-svc-m62x8
Feb  9 09:30:34.519: INFO: Got endpoints: latency-svc-bwjz9 [753.289238ms]
Feb  9 09:30:34.528: INFO: Created: latency-svc-qqpbj
Feb  9 09:30:34.570: INFO: Got endpoints: latency-svc-t25l6 [755.974786ms]
Feb  9 09:30:34.577: INFO: Created: latency-svc-kcd6x
Feb  9 09:30:34.617: INFO: Got endpoints: latency-svc-46vdx [753.908211ms]
Feb  9 09:30:34.627: INFO: Created: latency-svc-69xqq
Feb  9 09:30:34.664: INFO: Got endpoints: latency-svc-vnwv6 [749.58828ms]
Feb  9 09:30:34.676: INFO: Created: latency-svc-299rl
Feb  9 09:30:34.720: INFO: Got endpoints: latency-svc-rzkn8 [755.587001ms]
Feb  9 09:30:34.728: INFO: Created: latency-svc-tpsqc
Feb  9 09:30:34.766: INFO: Got endpoints: latency-svc-7p78j [750.943665ms]
Feb  9 09:30:34.777: INFO: Created: latency-svc-kq58f
Feb  9 09:30:34.815: INFO: Got endpoints: latency-svc-6r8tl [749.564061ms]
Feb  9 09:30:34.826: INFO: Created: latency-svc-vpmz6
Feb  9 09:30:34.867: INFO: Got endpoints: latency-svc-pfkqz [747.146527ms]
Feb  9 09:30:34.881: INFO: Created: latency-svc-w8s5x
Feb  9 09:30:34.914: INFO: Got endpoints: latency-svc-jmjrp [741.510572ms]
Feb  9 09:30:34.924: INFO: Created: latency-svc-nnzd7
Feb  9 09:30:34.964: INFO: Got endpoints: latency-svc-xnmjb [743.844844ms]
Feb  9 09:30:34.975: INFO: Created: latency-svc-rc7fc
Feb  9 09:30:35.017: INFO: Got endpoints: latency-svc-2jp2h [752.349598ms]
Feb  9 09:30:35.033: INFO: Created: latency-svc-t5nnb
Feb  9 09:30:35.065: INFO: Got endpoints: latency-svc-snqzp [743.656506ms]
Feb  9 09:30:35.076: INFO: Created: latency-svc-xm6lb
Feb  9 09:30:35.118: INFO: Got endpoints: latency-svc-g8rbv [750.757945ms]
Feb  9 09:30:35.129: INFO: Created: latency-svc-kwqt8
Feb  9 09:30:35.170: INFO: Got endpoints: latency-svc-bq59j [752.730963ms]
Feb  9 09:30:35.181: INFO: Created: latency-svc-rn79z
Feb  9 09:30:35.215: INFO: Got endpoints: latency-svc-m62x8 [734.209975ms]
Feb  9 09:30:35.229: INFO: Created: latency-svc-7xtrh
Feb  9 09:30:35.265: INFO: Got endpoints: latency-svc-qqpbj [746.110088ms]
Feb  9 09:30:35.281: INFO: Created: latency-svc-n9hh7
Feb  9 09:30:35.315: INFO: Got endpoints: latency-svc-kcd6x [745.335107ms]
Feb  9 09:30:35.331: INFO: Created: latency-svc-kxtfh
Feb  9 09:30:35.365: INFO: Got endpoints: latency-svc-69xqq [747.80977ms]
Feb  9 09:30:35.383: INFO: Created: latency-svc-pcswf
Feb  9 09:30:35.418: INFO: Got endpoints: latency-svc-299rl [753.564773ms]
Feb  9 09:30:35.431: INFO: Created: latency-svc-7624w
Feb  9 09:30:35.479: INFO: Got endpoints: latency-svc-tpsqc [758.805215ms]
Feb  9 09:30:35.521: INFO: Got endpoints: latency-svc-kq58f [754.79508ms]
Feb  9 09:30:35.565: INFO: Got endpoints: latency-svc-vpmz6 [749.711508ms]
Feb  9 09:30:35.574: INFO: Created: latency-svc-bbdjq
Feb  9 09:30:35.594: INFO: Created: latency-svc-85xkb
Feb  9 09:30:35.599: INFO: Created: latency-svc-74mnr
Feb  9 09:30:35.706: INFO: Got endpoints: latency-svc-w8s5x [838.733635ms]
Feb  9 09:30:35.707: INFO: Got endpoints: latency-svc-nnzd7 [792.268154ms]
Feb  9 09:30:35.720: INFO: Got endpoints: latency-svc-rc7fc [756.091575ms]
Feb  9 09:30:35.734: INFO: Created: latency-svc-v2pdp
Feb  9 09:30:35.754: INFO: Created: latency-svc-fj6d2
Feb  9 09:30:35.761: INFO: Created: latency-svc-9fl8s
Feb  9 09:30:35.764: INFO: Got endpoints: latency-svc-t5nnb [747.790981ms]
Feb  9 09:30:35.774: INFO: Created: latency-svc-kkmq9
Feb  9 09:30:35.853: INFO: Got endpoints: latency-svc-xm6lb [788.314149ms]
Feb  9 09:30:35.869: INFO: Created: latency-svc-2kslm
Feb  9 09:30:35.874: INFO: Got endpoints: latency-svc-kwqt8 [755.626881ms]
Feb  9 09:30:35.885: INFO: Created: latency-svc-6q28w
Feb  9 09:30:35.912: INFO: Got endpoints: latency-svc-rn79z [742.374582ms]
Feb  9 09:30:35.924: INFO: Created: latency-svc-9zx9v
Feb  9 09:30:35.967: INFO: Got endpoints: latency-svc-7xtrh [750.810056ms]
Feb  9 09:30:35.977: INFO: Created: latency-svc-6t6sg
Feb  9 09:30:36.016: INFO: Got endpoints: latency-svc-n9hh7 [750.442811ms]
Feb  9 09:30:36.034: INFO: Created: latency-svc-tjb28
Feb  9 09:30:36.068: INFO: Got endpoints: latency-svc-kxtfh [753.214658ms]
Feb  9 09:30:36.077: INFO: Created: latency-svc-qfrbn
Feb  9 09:30:36.115: INFO: Got endpoints: latency-svc-pcswf [749.611109ms]
Feb  9 09:30:36.127: INFO: Created: latency-svc-l6jlb
Feb  9 09:30:36.163: INFO: Got endpoints: latency-svc-7624w [744.993142ms]
Feb  9 09:30:36.178: INFO: Created: latency-svc-mz88j
Feb  9 09:30:36.220: INFO: Got endpoints: latency-svc-bbdjq [741.118716ms]
Feb  9 09:30:36.232: INFO: Created: latency-svc-4vdtd
Feb  9 09:30:36.272: INFO: Got endpoints: latency-svc-85xkb [751.500558ms]
Feb  9 09:30:36.281: INFO: Created: latency-svc-qhk27
Feb  9 09:30:36.319: INFO: Got endpoints: latency-svc-74mnr [754.194276ms]
Feb  9 09:30:36.330: INFO: Created: latency-svc-5pvd8
Feb  9 09:30:36.371: INFO: Got endpoints: latency-svc-v2pdp [664.820194ms]
Feb  9 09:30:36.386: INFO: Created: latency-svc-8vtth
Feb  9 09:30:36.415: INFO: Got endpoints: latency-svc-fj6d2 [695.512855ms]
Feb  9 09:30:36.425: INFO: Created: latency-svc-95xwz
Feb  9 09:30:36.469: INFO: Got endpoints: latency-svc-9fl8s [762.646521ms]
Feb  9 09:30:36.478: INFO: Created: latency-svc-h54p9
Feb  9 09:30:36.521: INFO: Got endpoints: latency-svc-kkmq9 [756.281733ms]
Feb  9 09:30:36.530: INFO: Created: latency-svc-b26js
Feb  9 09:30:36.567: INFO: Got endpoints: latency-svc-2kslm [713.461512ms]
Feb  9 09:30:36.580: INFO: Created: latency-svc-pwtp4
Feb  9 09:30:36.620: INFO: Got endpoints: latency-svc-6q28w [746.257188ms]
Feb  9 09:30:36.630: INFO: Created: latency-svc-h46sj
Feb  9 09:30:36.669: INFO: Got endpoints: latency-svc-9zx9v [756.602609ms]
Feb  9 09:30:36.678: INFO: Created: latency-svc-k4f2g
Feb  9 09:30:36.722: INFO: Got endpoints: latency-svc-6t6sg [755.084237ms]
Feb  9 09:30:36.731: INFO: Created: latency-svc-ld2j2
Feb  9 09:30:36.769: INFO: Got endpoints: latency-svc-tjb28 [752.758174ms]
Feb  9 09:30:36.779: INFO: Created: latency-svc-gdzgr
Feb  9 09:30:36.816: INFO: Got endpoints: latency-svc-qfrbn [747.400274ms]
Feb  9 09:30:36.828: INFO: Created: latency-svc-zvwrx
Feb  9 09:30:36.869: INFO: Got endpoints: latency-svc-l6jlb [754.450954ms]
Feb  9 09:30:36.881: INFO: Created: latency-svc-vwhqz
Feb  9 09:30:36.917: INFO: Got endpoints: latency-svc-mz88j [753.245218ms]
Feb  9 09:30:36.927: INFO: Created: latency-svc-4622v
Feb  9 09:30:36.966: INFO: Got endpoints: latency-svc-4vdtd [745.786923ms]
Feb  9 09:30:36.978: INFO: Created: latency-svc-5wf6q
Feb  9 09:30:37.019: INFO: Got endpoints: latency-svc-qhk27 [746.154197ms]
Feb  9 09:30:37.032: INFO: Created: latency-svc-8zpnx
Feb  9 09:30:37.067: INFO: Got endpoints: latency-svc-5pvd8 [747.814311ms]
Feb  9 09:30:37.079: INFO: Created: latency-svc-74qsx
Feb  9 09:30:37.116: INFO: Got endpoints: latency-svc-8vtth [744.713835ms]
Feb  9 09:30:37.127: INFO: Created: latency-svc-gh62j
Feb  9 09:30:37.167: INFO: Got endpoints: latency-svc-95xwz [751.807385ms]
Feb  9 09:30:37.177: INFO: Created: latency-svc-84j2q
Feb  9 09:30:37.217: INFO: Got endpoints: latency-svc-h54p9 [747.85ms]
Feb  9 09:30:37.228: INFO: Created: latency-svc-z7ltt
Feb  9 09:30:37.272: INFO: Got endpoints: latency-svc-b26js [750.752597ms]
Feb  9 09:30:37.283: INFO: Created: latency-svc-nr8sx
Feb  9 09:30:37.316: INFO: Got endpoints: latency-svc-pwtp4 [748.537211ms]
Feb  9 09:30:37.327: INFO: Created: latency-svc-9cf6j
Feb  9 09:30:37.366: INFO: Got endpoints: latency-svc-h46sj [745.591125ms]
Feb  9 09:30:37.384: INFO: Created: latency-svc-ffpmw
Feb  9 09:30:37.416: INFO: Got endpoints: latency-svc-k4f2g [747.361346ms]
Feb  9 09:30:37.432: INFO: Created: latency-svc-7ldzr
Feb  9 09:30:37.479: INFO: Got endpoints: latency-svc-ld2j2 [757.610658ms]
Feb  9 09:30:37.504: INFO: Created: latency-svc-7zzq9
Feb  9 09:30:37.514: INFO: Got endpoints: latency-svc-gdzgr [744.757174ms]
Feb  9 09:30:37.521: INFO: Created: latency-svc-f5pll
Feb  9 09:30:37.571: INFO: Got endpoints: latency-svc-zvwrx [755.451393ms]
Feb  9 09:30:37.581: INFO: Created: latency-svc-xb8h8
Feb  9 09:30:37.621: INFO: Got endpoints: latency-svc-vwhqz [751.467238ms]
Feb  9 09:30:37.729: INFO: Created: latency-svc-29lf5
Feb  9 09:30:37.729: INFO: Got endpoints: latency-svc-5wf6q [763.203055ms]
Feb  9 09:30:37.729: INFO: Got endpoints: latency-svc-4622v [811.882361ms]
Feb  9 09:30:37.747: INFO: Created: latency-svc-hk7rd
Feb  9 09:30:37.756: INFO: Created: latency-svc-xzmvn
Feb  9 09:30:37.766: INFO: Got endpoints: latency-svc-8zpnx [747.546593ms]
Feb  9 09:30:37.776: INFO: Created: latency-svc-xpczl
Feb  9 09:30:37.816: INFO: Got endpoints: latency-svc-74qsx [748.983796ms]
Feb  9 09:30:37.834: INFO: Created: latency-svc-d9dmh
Feb  9 09:30:37.867: INFO: Got endpoints: latency-svc-gh62j [751.182771ms]
Feb  9 09:30:37.879: INFO: Created: latency-svc-bzcmw
Feb  9 09:30:37.918: INFO: Got endpoints: latency-svc-84j2q [750.35658ms]
Feb  9 09:30:37.930: INFO: Created: latency-svc-7d9wr
Feb  9 09:30:37.966: INFO: Got endpoints: latency-svc-z7ltt [749.131354ms]
Feb  9 09:30:37.978: INFO: Created: latency-svc-fqwcj
Feb  9 09:30:38.022: INFO: Got endpoints: latency-svc-nr8sx [749.826337ms]
Feb  9 09:30:38.034: INFO: Created: latency-svc-g6vjt
Feb  9 09:30:38.073: INFO: Got endpoints: latency-svc-9cf6j [757.081424ms]
Feb  9 09:30:38.088: INFO: Created: latency-svc-lt6jc
Feb  9 09:30:38.115: INFO: Got endpoints: latency-svc-ffpmw [748.368523ms]
Feb  9 09:30:38.130: INFO: Created: latency-svc-v8rgn
Feb  9 09:30:38.172: INFO: Got endpoints: latency-svc-7ldzr [755.700551ms]
Feb  9 09:30:38.182: INFO: Created: latency-svc-wrwhw
Feb  9 09:30:38.217: INFO: Got endpoints: latency-svc-7zzq9 [737.140021ms]
Feb  9 09:30:38.228: INFO: Created: latency-svc-pjtkc
Feb  9 09:30:38.270: INFO: Got endpoints: latency-svc-f5pll [756.58074ms]
Feb  9 09:30:38.280: INFO: Created: latency-svc-zpkwq
Feb  9 09:30:38.324: INFO: Got endpoints: latency-svc-xb8h8 [752.840453ms]
Feb  9 09:30:38.336: INFO: Created: latency-svc-p8fcg
Feb  9 09:30:38.378: INFO: Got endpoints: latency-svc-29lf5 [756.814158ms]
Feb  9 09:30:38.388: INFO: Created: latency-svc-dzcq4
Feb  9 09:30:38.415: INFO: Got endpoints: latency-svc-hk7rd [685.532848ms]
Feb  9 09:30:38.426: INFO: Created: latency-svc-k266l
Feb  9 09:30:38.463: INFO: Got endpoints: latency-svc-xzmvn [733.164027ms]
Feb  9 09:30:38.502: INFO: Created: latency-svc-tjd6x
Feb  9 09:30:38.516: INFO: Got endpoints: latency-svc-xpczl [750.027445ms]
Feb  9 09:30:38.526: INFO: Created: latency-svc-5cllm
Feb  9 09:30:38.571: INFO: Got endpoints: latency-svc-d9dmh [754.785061ms]
Feb  9 09:30:38.584: INFO: Created: latency-svc-ll4xw
Feb  9 09:30:38.625: INFO: Got endpoints: latency-svc-bzcmw [758.336111ms]
Feb  9 09:30:38.637: INFO: Created: latency-svc-7nwjb
Feb  9 09:30:38.669: INFO: Got endpoints: latency-svc-7d9wr [751.585648ms]
Feb  9 09:30:38.680: INFO: Created: latency-svc-kfvjv
Feb  9 09:30:38.716: INFO: Got endpoints: latency-svc-fqwcj [749.61093ms]
Feb  9 09:30:38.730: INFO: Created: latency-svc-8x47w
Feb  9 09:30:38.770: INFO: Got endpoints: latency-svc-g6vjt [748.380234ms]
Feb  9 09:30:38.780: INFO: Created: latency-svc-tjbsx
Feb  9 09:30:38.819: INFO: Got endpoints: latency-svc-lt6jc [746.196019ms]
Feb  9 09:30:38.829: INFO: Created: latency-svc-7z5vp
Feb  9 09:30:38.866: INFO: Got endpoints: latency-svc-v8rgn [751.20977ms]
Feb  9 09:30:38.877: INFO: Created: latency-svc-ch5lh
Feb  9 09:30:38.919: INFO: Got endpoints: latency-svc-wrwhw [746.370745ms]
Feb  9 09:30:38.928: INFO: Created: latency-svc-psmnl
Feb  9 09:30:38.966: INFO: Got endpoints: latency-svc-pjtkc [748.647011ms]
Feb  9 09:30:38.978: INFO: Created: latency-svc-cnrw5
Feb  9 09:30:39.018: INFO: Got endpoints: latency-svc-zpkwq [747.332776ms]
Feb  9 09:30:39.031: INFO: Created: latency-svc-2hcpz
Feb  9 09:30:39.068: INFO: Got endpoints: latency-svc-p8fcg [743.867094ms]
Feb  9 09:30:39.080: INFO: Created: latency-svc-dsjnf
Feb  9 09:30:39.114: INFO: Got endpoints: latency-svc-dzcq4 [735.997064ms]
Feb  9 09:30:39.127: INFO: Created: latency-svc-sw76l
Feb  9 09:30:39.163: INFO: Got endpoints: latency-svc-k266l [748.294785ms]
Feb  9 09:30:39.174: INFO: Created: latency-svc-hfj59
Feb  9 09:30:39.214: INFO: Got endpoints: latency-svc-tjd6x [751.371009ms]
Feb  9 09:30:39.241: INFO: Created: latency-svc-xpt9q
Feb  9 09:30:39.269: INFO: Got endpoints: latency-svc-5cllm [752.856682ms]
Feb  9 09:30:39.277: INFO: Created: latency-svc-bl8s7
Feb  9 09:30:39.318: INFO: Got endpoints: latency-svc-ll4xw [747.252676ms]
Feb  9 09:30:39.326: INFO: Created: latency-svc-ggsbl
Feb  9 09:30:39.366: INFO: Got endpoints: latency-svc-7nwjb [740.797249ms]
Feb  9 09:30:39.380: INFO: Created: latency-svc-5khwp
Feb  9 09:30:39.414: INFO: Got endpoints: latency-svc-kfvjv [744.302619ms]
Feb  9 09:30:39.423: INFO: Created: latency-svc-sz7kn
Feb  9 09:30:39.486: INFO: Got endpoints: latency-svc-8x47w [769.234916ms]
Feb  9 09:30:39.512: INFO: Created: latency-svc-hg9qd
Feb  9 09:30:39.515: INFO: Got endpoints: latency-svc-tjbsx [744.923462ms]
Feb  9 09:30:39.530: INFO: Created: latency-svc-t4wn8
Feb  9 09:30:39.570: INFO: Got endpoints: latency-svc-7z5vp [751.115712ms]
Feb  9 09:30:39.595: INFO: Created: latency-svc-jl76v
Feb  9 09:30:39.618: INFO: Got endpoints: latency-svc-ch5lh [750.938674ms]
Feb  9 09:30:39.632: INFO: Created: latency-svc-kkr54
Feb  9 09:30:39.666: INFO: Got endpoints: latency-svc-psmnl [747.047208ms]
Feb  9 09:30:39.678: INFO: Created: latency-svc-89rjg
Feb  9 09:30:39.719: INFO: Got endpoints: latency-svc-cnrw5 [753.627814ms]
Feb  9 09:30:39.731: INFO: Created: latency-svc-khqc4
Feb  9 09:30:39.769: INFO: Got endpoints: latency-svc-2hcpz [750.359751ms]
Feb  9 09:30:39.778: INFO: Created: latency-svc-z5q7j
Feb  9 09:30:39.824: INFO: Got endpoints: latency-svc-dsjnf [756.084815ms]
Feb  9 09:30:39.837: INFO: Created: latency-svc-kn9m7
Feb  9 09:30:39.863: INFO: Got endpoints: latency-svc-sw76l [749.100375ms]
Feb  9 09:30:39.874: INFO: Created: latency-svc-f8ltf
Feb  9 09:30:39.918: INFO: Got endpoints: latency-svc-hfj59 [754.433114ms]
Feb  9 09:30:39.929: INFO: Created: latency-svc-rbv8j
Feb  9 09:30:39.966: INFO: Got endpoints: latency-svc-xpt9q [751.754925ms]
Feb  9 09:30:39.981: INFO: Created: latency-svc-bkt7q
Feb  9 09:30:40.019: INFO: Got endpoints: latency-svc-bl8s7 [749.104785ms]
Feb  9 09:30:40.029: INFO: Created: latency-svc-s7jv6
Feb  9 09:30:40.065: INFO: Got endpoints: latency-svc-ggsbl [747.135347ms]
Feb  9 09:30:40.076: INFO: Created: latency-svc-fm845
Feb  9 09:30:40.123: INFO: Got endpoints: latency-svc-5khwp [756.738628ms]
Feb  9 09:30:40.138: INFO: Created: latency-svc-zrnd8
Feb  9 09:30:40.168: INFO: Got endpoints: latency-svc-sz7kn [754.189267ms]
Feb  9 09:30:40.180: INFO: Created: latency-svc-d8jrz
Feb  9 09:30:40.220: INFO: Got endpoints: latency-svc-hg9qd [734.070616ms]
Feb  9 09:30:40.266: INFO: Got endpoints: latency-svc-t4wn8 [751.202031ms]
Feb  9 09:30:40.316: INFO: Got endpoints: latency-svc-jl76v [745.140521ms]
Feb  9 09:30:40.371: INFO: Got endpoints: latency-svc-kkr54 [753.065161ms]
Feb  9 09:30:40.415: INFO: Got endpoints: latency-svc-89rjg [748.874457ms]
Feb  9 09:30:40.477: INFO: Got endpoints: latency-svc-khqc4 [757.890256ms]
Feb  9 09:30:40.519: INFO: Got endpoints: latency-svc-z5q7j [750.756356ms]
Feb  9 09:30:40.581: INFO: Got endpoints: latency-svc-kn9m7 [756.343043ms]
Feb  9 09:30:40.630: INFO: Got endpoints: latency-svc-f8ltf [766.414108ms]
Feb  9 09:30:40.666: INFO: Got endpoints: latency-svc-rbv8j [747.747401ms]
Feb  9 09:30:40.718: INFO: Got endpoints: latency-svc-bkt7q [752.219239ms]
Feb  9 09:30:40.772: INFO: Got endpoints: latency-svc-s7jv6 [751.581367ms]
Feb  9 09:30:40.814: INFO: Got endpoints: latency-svc-fm845 [748.569031ms]
Feb  9 09:30:40.866: INFO: Got endpoints: latency-svc-zrnd8 [742.466021ms]
Feb  9 09:30:40.915: INFO: Got endpoints: latency-svc-d8jrz [747.431005ms]
Feb  9 09:30:40.915: INFO: Latencies: [28.475665ms 29.89765ms 34.059013ms 41.794295ms 50.886802ms 51.122258ms 64.433917ms 71.100321ms 78.182291ms 91.599087ms 101.179038ms 115.902221ms 125.862888ms 130.097849ms 132.198817ms 133.890866ms 134.951605ms 136.922601ms 139.169657ms 140.259884ms 140.337814ms 140.438162ms 140.57777ms 141.020655ms 146.492682ms 147.161566ms 147.212605ms 154.818389ms 154.830168ms 155.46054ms 156.782596ms 163.072885ms 163.121084ms 176.349412ms 201.154861ms 225.625922ms 253.150089ms 264.40811ms 264.917524ms 284.922768ms 313.360864ms 356.428324ms 402.154775ms 457.862928ms 497.207791ms 546.206894ms 606.35333ms 627.936863ms 652.98606ms 664.820194ms 679.908052ms 685.532848ms 695.512855ms 699.894056ms 703.226637ms 713.461512ms 729.105753ms 733.164027ms 734.070616ms 734.209975ms 735.633658ms 735.997064ms 737.140021ms 740.548151ms 740.797249ms 741.118716ms 741.365024ms 741.510572ms 741.880097ms 742.374582ms 742.466021ms 743.656506ms 743.844844ms 743.867094ms 744.302619ms 744.713835ms 744.757174ms 744.923462ms 744.993142ms 745.140521ms 745.335107ms 745.510186ms 745.591125ms 745.786923ms 746.110088ms 746.154197ms 746.196019ms 746.257188ms 746.370745ms 746.938301ms 747.047208ms 747.135347ms 747.146527ms 747.252676ms 747.332776ms 747.361346ms 747.400274ms 747.431005ms 747.445914ms 747.509882ms 747.546593ms 747.747401ms 747.790981ms 747.80977ms 747.814311ms 747.85ms 748.294785ms 748.338724ms 748.350384ms 748.368523ms 748.380234ms 748.537211ms 748.569031ms 748.647011ms 748.874457ms 748.983796ms 749.100375ms 749.104785ms 749.131354ms 749.43179ms 749.444071ms 749.564061ms 749.58828ms 749.61093ms 749.611109ms 749.711508ms 749.826337ms 750.027445ms 750.35658ms 750.359751ms 750.442811ms 750.752597ms 750.756356ms 750.757945ms 750.810056ms 750.938674ms 750.943665ms 751.115712ms 751.182771ms 751.202031ms 751.20977ms 751.371009ms 751.445799ms 751.467238ms 751.500558ms 751.581367ms 751.585648ms 751.754925ms 751.807385ms 752.219239ms 752.349598ms 752.730963ms 752.758174ms 752.840453ms 752.856682ms 753.065161ms 753.214658ms 753.245218ms 753.289238ms 753.468884ms 753.564773ms 753.627814ms 753.632254ms 753.908211ms 754.189267ms 754.194276ms 754.433114ms 754.450954ms 754.785061ms 754.79508ms 755.084237ms 755.451393ms 755.587001ms 755.626881ms 755.700551ms 755.974786ms 756.084815ms 756.091575ms 756.281733ms 756.343043ms 756.440232ms 756.58074ms 756.602609ms 756.738628ms 756.814158ms 757.081424ms 757.610658ms 757.890256ms 758.336111ms 758.805215ms 762.646521ms 763.203055ms 764.872785ms 766.414108ms 769.234916ms 788.314149ms 792.268154ms 797.564314ms 811.882361ms 838.733635ms]
Feb  9 09:30:40.916: INFO: 50 %ile: 747.546593ms
Feb  9 09:30:40.916: INFO: 90 %ile: 756.440232ms
Feb  9 09:30:40.916: INFO: 99 %ile: 811.882361ms
Feb  9 09:30:40.916: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:30:40.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2300" for this suite.

• [SLOW TEST:14.994 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":23,"skipped":424,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:30:40.944: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Feb  9 09:30:41.103: INFO: namespace kubectl-4059
Feb  9 09:30:41.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4059 create -f -'
Feb  9 09:30:41.801: INFO: stderr: ""
Feb  9 09:30:41.801: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb  9 09:30:42.809: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 09:30:42.809: INFO: Found 0 / 1
Feb  9 09:30:43.812: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 09:30:43.812: INFO: Found 0 / 1
Feb  9 09:30:44.813: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 09:30:44.813: INFO: Found 1 / 1
Feb  9 09:30:44.813: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  9 09:30:44.816: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 09:30:44.816: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  9 09:30:44.816: INFO: wait on agnhost-primary startup in kubectl-4059 
Feb  9 09:30:44.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4059 logs agnhost-primary-nt7hb agnhost-primary'
Feb  9 09:30:45.056: INFO: stderr: ""
Feb  9 09:30:45.056: INFO: stdout: "Paused\n"
STEP: exposing RC
Feb  9 09:30:45.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4059 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb  9 09:30:45.211: INFO: stderr: ""
Feb  9 09:30:45.211: INFO: stdout: "service/rm2 exposed\n"
Feb  9 09:30:45.215: INFO: Service rm2 in namespace kubectl-4059 found.
STEP: exposing service
Feb  9 09:30:47.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4059 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb  9 09:30:47.379: INFO: stderr: ""
Feb  9 09:30:47.379: INFO: stdout: "service/rm3 exposed\n"
Feb  9 09:30:47.387: INFO: Service rm3 in namespace kubectl-4059 found.
Feb  9 09:30:49.402: INFO: Get endpoints failed (interval 2s): endpoints "rm3" not found
Feb  9 09:30:51.401: INFO: Get endpoints failed (interval 2s): endpoints "rm3" not found
Feb  9 09:30:53.400: INFO: Get endpoints failed (interval 2s): endpoints "rm3" not found
Feb  9 09:30:55.403: INFO: Get endpoints failed (interval 2s): endpoints "rm3" not found
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:30:57.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4059" for this suite.

• [SLOW TEST:16.486 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":24,"skipped":478,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:30:57.431: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Feb  9 09:30:57.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 create -f -'
Feb  9 09:30:57.924: INFO: stderr: ""
Feb  9 09:30:57.924: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  9 09:30:57.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:30:58.064: INFO: stderr: ""
Feb  9 09:30:58.064: INFO: stdout: "update-demo-nautilus-4z4cp update-demo-nautilus-gqkmb "
Feb  9 09:30:58.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get pods update-demo-nautilus-4z4cp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  9 09:30:58.175: INFO: stderr: ""
Feb  9 09:30:58.175: INFO: stdout: ""
Feb  9 09:30:58.175: INFO: update-demo-nautilus-4z4cp is created but not running
Feb  9 09:31:03.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:31:03.305: INFO: stderr: ""
Feb  9 09:31:03.305: INFO: stdout: "update-demo-nautilus-4z4cp update-demo-nautilus-gqkmb "
Feb  9 09:31:03.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get pods update-demo-nautilus-4z4cp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  9 09:31:03.418: INFO: stderr: ""
Feb  9 09:31:03.418: INFO: stdout: "true"
Feb  9 09:31:03.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get pods update-demo-nautilus-4z4cp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  9 09:31:03.532: INFO: stderr: ""
Feb  9 09:31:03.532: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  9 09:31:03.532: INFO: validating pod update-demo-nautilus-4z4cp
Feb  9 09:31:03.540: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  9 09:31:03.540: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  9 09:31:03.540: INFO: update-demo-nautilus-4z4cp is verified up and running
Feb  9 09:31:03.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get pods update-demo-nautilus-gqkmb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  9 09:31:03.663: INFO: stderr: ""
Feb  9 09:31:03.663: INFO: stdout: "true"
Feb  9 09:31:03.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get pods update-demo-nautilus-gqkmb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  9 09:31:03.784: INFO: stderr: ""
Feb  9 09:31:03.784: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  9 09:31:03.784: INFO: validating pod update-demo-nautilus-gqkmb
Feb  9 09:31:03.792: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  9 09:31:03.792: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  9 09:31:03.792: INFO: update-demo-nautilus-gqkmb is verified up and running
STEP: using delete to clean up resources
Feb  9 09:31:03.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 delete --grace-period=0 --force -f -'
Feb  9 09:31:03.934: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  9 09:31:03.934: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb  9 09:31:03.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get rc,svc -l name=update-demo --no-headers'
Feb  9 09:31:04.082: INFO: stderr: "No resources found in kubectl-5897 namespace.\n"
Feb  9 09:31:04.082: INFO: stdout: ""
Feb  9 09:31:04.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  9 09:31:04.223: INFO: stderr: ""
Feb  9 09:31:04.223: INFO: stdout: "update-demo-nautilus-4z4cp\nupdate-demo-nautilus-gqkmb\n"
Feb  9 09:31:04.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get rc,svc -l name=update-demo --no-headers'
Feb  9 09:31:04.837: INFO: stderr: "No resources found in kubectl-5897 namespace.\n"
Feb  9 09:31:04.837: INFO: stdout: ""
Feb  9 09:31:04.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5897 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  9 09:31:04.958: INFO: stderr: ""
Feb  9 09:31:04.958: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:31:04.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5897" for this suite.

• [SLOW TEST:7.539 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":25,"skipped":482,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:31:04.971: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:31:11.168: INFO: Deleting pod "var-expansion-25a96d45-5340-4f01-8426-e60255128be8" in namespace "var-expansion-4445"
Feb  9 09:31:11.173: INFO: Wait up to 5m0s for pod "var-expansion-25a96d45-5340-4f01-8426-e60255128be8" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:31:51.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4445" for this suite.

• [SLOW TEST:46.243 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":26,"skipped":483,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:31:51.214: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4510
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:31:51.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4510" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":27,"skipped":488,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:31:51.393: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-90097a5c-0760-466e-807b-62475d5765a0 in namespace container-probe-5330
Feb  9 09:31:53.580: INFO: Started pod liveness-90097a5c-0760-466e-807b-62475d5765a0 in namespace container-probe-5330
STEP: checking the pod's current state and verifying that restartCount is present
Feb  9 09:31:53.584: INFO: Initial restart count of pod liveness-90097a5c-0760-466e-807b-62475d5765a0 is 0
Feb  9 09:32:13.722: INFO: Restart count of pod container-probe-5330/liveness-90097a5c-0760-466e-807b-62475d5765a0 is now 1 (20.137884326s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:32:13.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5330" for this suite.

• [SLOW TEST:22.358 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":28,"skipped":489,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:32:13.751: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:32:42.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2589" for this suite.

• [SLOW TEST:28.267 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":29,"skipped":490,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:32:42.019: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-53
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 09:32:42.816: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 09:32:44.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748459962, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748459962, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748459962, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748459962, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 09:32:47.860: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:32:47.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-53" for this suite.
STEP: Destroying namespace "webhook-53-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.980 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":30,"skipped":501,"failed":0}
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:32:47.999: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5403
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5403 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5403;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5403 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5403;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5403.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5403.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5403.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5403.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5403.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5403.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5403.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5403.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5403.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5403.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5403.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5403.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5403.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 41.34.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.34.41_udp@PTR;check="$$(dig +tcp +noall +answer +search 41.34.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.34.41_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5403 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5403;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5403 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5403;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5403.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5403.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5403.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5403.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5403.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5403.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5403.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5403.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5403.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5403.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5403.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5403.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5403.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 41.34.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.34.41_udp@PTR;check="$$(dig +tcp +noall +answer +search 41.34.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.34.41_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  9 09:33:00.429: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.433: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.437: INFO: Unable to read wheezy_udp@dns-test-service.dns-5403 from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.441: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5403 from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.445: INFO: Unable to read wheezy_udp@dns-test-service.dns-5403.svc from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.450: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5403.svc from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.454: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5403.svc from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.460: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5403.svc from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.473: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.480: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.494: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.499: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.503: INFO: Unable to read jessie_udp@dns-test-service.dns-5403 from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.507: INFO: Unable to read jessie_tcp@dns-test-service.dns-5403 from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.510: INFO: Unable to read jessie_udp@dns-test-service.dns-5403.svc from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.514: INFO: Unable to read jessie_tcp@dns-test-service.dns-5403.svc from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.517: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5403.svc from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.521: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5403.svc from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.533: INFO: Unable to read jessie_udp@PodARecord from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.536: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f: the server could not find the requested resource (get pods dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f)
Feb  9 09:33:00.542: INFO: Lookups using dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5403 wheezy_tcp@dns-test-service.dns-5403 wheezy_udp@dns-test-service.dns-5403.svc wheezy_tcp@dns-test-service.dns-5403.svc wheezy_udp@_http._tcp.dns-test-service.dns-5403.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5403.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5403 jessie_tcp@dns-test-service.dns-5403 jessie_udp@dns-test-service.dns-5403.svc jessie_tcp@dns-test-service.dns-5403.svc jessie_udp@_http._tcp.dns-test-service.dns-5403.svc jessie_tcp@_http._tcp.dns-test-service.dns-5403.svc jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb  9 09:33:05.662: INFO: DNS probes using dns-5403/dns-test-bc179eed-e3a2-42d1-89f5-c1a89e2aac5f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:33:05.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5403" for this suite.

• [SLOW TEST:17.793 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":31,"skipped":501,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:33:05.793: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-73943699-3ac3-4409-9347-4dc97f127140
STEP: Creating a pod to test consume configMaps
Feb  9 09:33:06.226: INFO: Waiting up to 5m0s for pod "pod-configmaps-b23dd88f-0f4a-47df-8255-9b260f898bd7" in namespace "configmap-8909" to be "Succeeded or Failed"
Feb  9 09:33:06.267: INFO: Pod "pod-configmaps-b23dd88f-0f4a-47df-8255-9b260f898bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 41.32954ms
Feb  9 09:33:08.284: INFO: Pod "pod-configmaps-b23dd88f-0f4a-47df-8255-9b260f898bd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.058187447s
STEP: Saw pod success
Feb  9 09:33:08.284: INFO: Pod "pod-configmaps-b23dd88f-0f4a-47df-8255-9b260f898bd7" satisfied condition "Succeeded or Failed"
Feb  9 09:33:08.292: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-configmaps-b23dd88f-0f4a-47df-8255-9b260f898bd7 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 09:33:08.371: INFO: Waiting for pod pod-configmaps-b23dd88f-0f4a-47df-8255-9b260f898bd7 to disappear
Feb  9 09:33:08.378: INFO: Pod pod-configmaps-b23dd88f-0f4a-47df-8255-9b260f898bd7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:33:08.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8909" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":32,"skipped":514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:33:08.388: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-9792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb  9 09:33:09.001: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb  9 09:33:11.024: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748459989, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748459989, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748459989, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748459989, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 09:33:14.045: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:33:14.056: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:33:15.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9792" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.972 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":33,"skipped":540,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:33:15.363: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Feb  9 09:33:15.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 create -f -'
Feb  9 09:33:16.009: INFO: stderr: ""
Feb  9 09:33:16.009: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  9 09:33:16.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:33:16.145: INFO: stderr: ""
Feb  9 09:33:16.145: INFO: stdout: "update-demo-nautilus-kn7z8 update-demo-nautilus-l24cc "
Feb  9 09:33:16.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-kn7z8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  9 09:33:16.277: INFO: stderr: ""
Feb  9 09:33:16.277: INFO: stdout: ""
Feb  9 09:33:16.277: INFO: update-demo-nautilus-kn7z8 is created but not running
Feb  9 09:33:21.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:33:21.417: INFO: stderr: ""
Feb  9 09:33:21.417: INFO: stdout: "update-demo-nautilus-kn7z8 update-demo-nautilus-l24cc "
Feb  9 09:33:21.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-kn7z8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  9 09:33:21.548: INFO: stderr: ""
Feb  9 09:33:21.548: INFO: stdout: "true"
Feb  9 09:33:21.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-kn7z8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  9 09:33:21.681: INFO: stderr: ""
Feb  9 09:33:21.681: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  9 09:33:21.681: INFO: validating pod update-demo-nautilus-kn7z8
Feb  9 09:33:21.689: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  9 09:33:21.689: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  9 09:33:21.689: INFO: update-demo-nautilus-kn7z8 is verified up and running
Feb  9 09:33:21.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-l24cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  9 09:33:21.799: INFO: stderr: ""
Feb  9 09:33:21.799: INFO: stdout: "true"
Feb  9 09:33:21.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-l24cc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  9 09:33:21.924: INFO: stderr: ""
Feb  9 09:33:21.924: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  9 09:33:21.924: INFO: validating pod update-demo-nautilus-l24cc
Feb  9 09:33:21.933: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  9 09:33:21.933: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  9 09:33:21.933: INFO: update-demo-nautilus-l24cc is verified up and running
STEP: scaling down the replication controller
Feb  9 09:33:21.938: INFO: scanned /root for discovery docs: <nil>
Feb  9 09:33:21.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb  9 09:33:23.120: INFO: stderr: ""
Feb  9 09:33:23.120: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  9 09:33:23.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:33:23.242: INFO: stderr: ""
Feb  9 09:33:23.242: INFO: stdout: "update-demo-nautilus-kn7z8 update-demo-nautilus-l24cc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  9 09:33:28.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:33:28.353: INFO: stderr: ""
Feb  9 09:33:28.353: INFO: stdout: "update-demo-nautilus-kn7z8 update-demo-nautilus-l24cc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  9 09:33:33.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:33:33.506: INFO: stderr: ""
Feb  9 09:33:33.506: INFO: stdout: "update-demo-nautilus-kn7z8 update-demo-nautilus-l24cc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  9 09:33:38.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:33:38.640: INFO: stderr: ""
Feb  9 09:33:38.640: INFO: stdout: "update-demo-nautilus-kn7z8 update-demo-nautilus-l24cc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  9 09:33:43.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:33:43.767: INFO: stderr: ""
Feb  9 09:33:43.767: INFO: stdout: "update-demo-nautilus-kn7z8 update-demo-nautilus-l24cc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  9 09:33:48.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:33:48.906: INFO: stderr: ""
Feb  9 09:33:48.906: INFO: stdout: "update-demo-nautilus-kn7z8 update-demo-nautilus-l24cc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  9 09:33:53.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:33:54.064: INFO: stderr: ""
Feb  9 09:33:54.064: INFO: stdout: "update-demo-nautilus-kn7z8 update-demo-nautilus-l24cc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  9 09:33:59.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:33:59.200: INFO: stderr: ""
Feb  9 09:33:59.200: INFO: stdout: "update-demo-nautilus-kn7z8 update-demo-nautilus-l24cc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb  9 09:34:04.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:34:04.337: INFO: stderr: ""
Feb  9 09:34:04.337: INFO: stdout: "update-demo-nautilus-l24cc "
Feb  9 09:34:04.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-l24cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  9 09:34:04.451: INFO: stderr: ""
Feb  9 09:34:04.451: INFO: stdout: "true"
Feb  9 09:34:04.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-l24cc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  9 09:34:04.575: INFO: stderr: ""
Feb  9 09:34:04.575: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  9 09:34:04.575: INFO: validating pod update-demo-nautilus-l24cc
Feb  9 09:34:04.585: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  9 09:34:04.585: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  9 09:34:04.585: INFO: update-demo-nautilus-l24cc is verified up and running
STEP: scaling up the replication controller
Feb  9 09:34:04.589: INFO: scanned /root for discovery docs: <nil>
Feb  9 09:34:04.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb  9 09:34:05.743: INFO: stderr: ""
Feb  9 09:34:05.743: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb  9 09:34:05.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:34:05.879: INFO: stderr: ""
Feb  9 09:34:05.879: INFO: stdout: "update-demo-nautilus-5jxx6 update-demo-nautilus-l24cc "
Feb  9 09:34:05.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-5jxx6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  9 09:34:05.997: INFO: stderr: ""
Feb  9 09:34:05.997: INFO: stdout: ""
Feb  9 09:34:05.997: INFO: update-demo-nautilus-5jxx6 is created but not running
Feb  9 09:34:10.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb  9 09:34:11.132: INFO: stderr: ""
Feb  9 09:34:11.132: INFO: stdout: "update-demo-nautilus-5jxx6 update-demo-nautilus-l24cc "
Feb  9 09:34:11.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-5jxx6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  9 09:34:11.235: INFO: stderr: ""
Feb  9 09:34:11.235: INFO: stdout: "true"
Feb  9 09:34:11.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-5jxx6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  9 09:34:11.343: INFO: stderr: ""
Feb  9 09:34:11.343: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  9 09:34:11.343: INFO: validating pod update-demo-nautilus-5jxx6
Feb  9 09:34:11.357: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  9 09:34:11.358: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  9 09:34:11.358: INFO: update-demo-nautilus-5jxx6 is verified up and running
Feb  9 09:34:11.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-l24cc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb  9 09:34:11.466: INFO: stderr: ""
Feb  9 09:34:11.467: INFO: stdout: "true"
Feb  9 09:34:11.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods update-demo-nautilus-l24cc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb  9 09:34:11.581: INFO: stderr: ""
Feb  9 09:34:11.581: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb  9 09:34:11.581: INFO: validating pod update-demo-nautilus-l24cc
Feb  9 09:34:11.586: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb  9 09:34:11.586: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb  9 09:34:11.586: INFO: update-demo-nautilus-l24cc is verified up and running
STEP: using delete to clean up resources
Feb  9 09:34:11.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 delete --grace-period=0 --force -f -'
Feb  9 09:34:11.728: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  9 09:34:11.728: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb  9 09:34:11.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get rc,svc -l name=update-demo --no-headers'
Feb  9 09:34:11.854: INFO: stderr: "No resources found in kubectl-4710 namespace.\n"
Feb  9 09:34:11.854: INFO: stdout: ""
Feb  9 09:34:11.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  9 09:34:11.974: INFO: stderr: ""
Feb  9 09:34:11.974: INFO: stdout: "update-demo-nautilus-5jxx6\nupdate-demo-nautilus-l24cc\n"
Feb  9 09:34:12.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get rc,svc -l name=update-demo --no-headers'
Feb  9 09:34:12.610: INFO: stderr: "No resources found in kubectl-4710 namespace.\n"
Feb  9 09:34:12.610: INFO: stdout: ""
Feb  9 09:34:12.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-4710 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  9 09:34:12.735: INFO: stderr: ""
Feb  9 09:34:12.735: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:34:12.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4710" for this suite.

• [SLOW TEST:57.387 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":34,"skipped":560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:34:12.751: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3218
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-b3656f67-832b-400a-b12d-a6f8e822e28f
STEP: Creating secret with name s-test-opt-upd-db99741e-4675-4214-b10f-f734fe652b01
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b3656f67-832b-400a-b12d-a6f8e822e28f
STEP: Updating secret s-test-opt-upd-db99741e-4675-4214-b10f-f734fe652b01
STEP: Creating secret with name s-test-opt-create-90e1f52e-6142-459e-83e8-22a41378d89f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:35:33.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3218" for this suite.

• [SLOW TEST:80.958 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":613,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:35:33.711: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 09:35:33.938: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fd30ed4-f301-496a-af1c-d69b13223c54" in namespace "projected-1172" to be "Succeeded or Failed"
Feb  9 09:35:33.948: INFO: Pod "downwardapi-volume-7fd30ed4-f301-496a-af1c-d69b13223c54": Phase="Pending", Reason="", readiness=false. Elapsed: 10.22315ms
Feb  9 09:35:35.961: INFO: Pod "downwardapi-volume-7fd30ed4-f301-496a-af1c-d69b13223c54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022955563s
Feb  9 09:35:37.966: INFO: Pod "downwardapi-volume-7fd30ed4-f301-496a-af1c-d69b13223c54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028187376s
STEP: Saw pod success
Feb  9 09:35:37.966: INFO: Pod "downwardapi-volume-7fd30ed4-f301-496a-af1c-d69b13223c54" satisfied condition "Succeeded or Failed"
Feb  9 09:35:37.970: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downwardapi-volume-7fd30ed4-f301-496a-af1c-d69b13223c54 container client-container: <nil>
STEP: delete the pod
Feb  9 09:35:38.045: INFO: Waiting for pod downwardapi-volume-7fd30ed4-f301-496a-af1c-d69b13223c54 to disappear
Feb  9 09:35:38.050: INFO: Pod downwardapi-volume-7fd30ed4-f301-496a-af1c-d69b13223c54 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:35:38.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1172" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":613,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:35:38.066: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:35:38.246: INFO: Create a RollingUpdate DaemonSet
Feb  9 09:35:38.254: INFO: Check that daemon pods launch on every node of the cluster
Feb  9 09:35:38.259: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:35:38.266: INFO: Number of nodes with available pods: 0
Feb  9 09:35:38.266: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 09:35:39.407: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:35:39.412: INFO: Number of nodes with available pods: 0
Feb  9 09:35:39.412: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 09:35:40.275: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:35:40.279: INFO: Number of nodes with available pods: 0
Feb  9 09:35:40.279: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 09:35:41.273: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:35:41.276: INFO: Number of nodes with available pods: 2
Feb  9 09:35:41.276: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 09:35:42.274: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:35:42.278: INFO: Number of nodes with available pods: 3
Feb  9 09:35:42.278: INFO: Number of running nodes: 3, number of available pods: 3
Feb  9 09:35:42.278: INFO: Update the DaemonSet to trigger a rollout
Feb  9 09:35:42.291: INFO: Updating DaemonSet daemon-set
Feb  9 09:36:25.322: INFO: Roll back the DaemonSet before rollout is complete
Feb  9 09:36:25.336: INFO: Updating DaemonSet daemon-set
Feb  9 09:36:25.336: INFO: Make sure DaemonSet rollback is complete
Feb  9 09:36:25.343: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:25.343: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:25.353: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:26.363: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:26.363: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:26.368: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:27.361: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:27.362: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:27.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:28.358: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:28.358: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:28.362: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:29.365: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:29.366: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:29.369: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:30.363: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:30.363: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:30.368: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:31.365: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:31.365: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:31.370: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:32.363: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:32.363: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:32.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:33.366: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:33.366: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:33.370: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:34.364: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:34.364: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:34.369: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:35.362: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:35.362: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:35.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:36.367: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:36.367: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:36.374: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:37.362: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:37.362: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:37.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:38.361: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:38.361: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:38.365: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:39.363: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:39.363: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:39.368: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:40.362: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:40.362: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:40.368: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:41.365: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:41.365: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:41.372: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:42.357: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:42.357: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:42.362: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:43.361: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:43.361: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:43.366: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:44.369: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:44.369: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:44.375: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:45.360: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:45.360: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:45.365: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:46.362: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:46.362: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:46.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:47.364: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:47.364: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:47.368: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:48.401: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:48.401: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:48.411: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:49.364: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:49.364: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:49.369: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:50.367: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:50.367: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:50.371: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:51.365: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:51.365: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:51.369: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:52.367: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:52.367: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:52.371: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:53.366: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:53.366: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:53.370: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:54.362: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:54.362: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:54.368: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:55.363: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:55.363: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:55.370: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:56.359: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:56.359: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:56.364: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:57.361: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:57.361: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:57.365: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:58.366: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:58.366: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:58.371: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:36:59.363: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:36:59.364: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:36:59.368: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:00.366: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:00.366: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:00.370: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:01.363: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:01.363: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:01.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:02.366: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:02.366: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:02.371: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:03.362: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:03.362: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:03.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:04.361: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:04.361: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:04.366: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:05.363: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:05.364: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:05.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:06.367: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:06.367: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:06.371: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:07.362: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:07.362: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:07.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:08.367: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:08.367: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:08.373: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:09.364: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:09.364: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:09.371: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:10.365: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:10.365: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:10.368: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:11.362: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:11.362: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:11.368: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:12.365: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:12.365: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:12.369: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:13.363: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:13.363: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:13.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:14.366: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:14.367: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:14.371: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:15.366: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:15.366: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:15.372: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:16.366: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:16.366: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:16.371: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:17.362: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:17.362: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:17.367: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:18.367: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:18.367: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:18.371: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:19.365: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:19.365: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:19.369: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:20.368: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:20.368: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:20.372: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:21.358: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:21.358: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:21.363: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:22.365: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:22.365: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:22.370: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:23.367: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:23.367: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:23.372: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:24.366: INFO: Wrong image for pod: daemon-set-j58zf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb  9 09:37:24.366: INFO: Pod daemon-set-j58zf is not available
Feb  9 09:37:24.371: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:37:25.364: INFO: Pod daemon-set-rgxws is not available
Feb  9 09:37:25.368: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6215, will wait for the garbage collector to delete the pods
Feb  9 09:37:25.443: INFO: Deleting DaemonSet.extensions daemon-set took: 12.461217ms
Feb  9 09:37:26.444: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000302356s
Feb  9 09:39:24.959: INFO: Number of nodes with available pods: 0
Feb  9 09:39:24.959: INFO: Number of running nodes: 0, number of available pods: 0
Feb  9 09:39:24.963: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15261"},"items":null}

Feb  9 09:39:24.966: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15261"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:39:24.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6215" for this suite.

• [SLOW TEST:226.938 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":37,"skipped":620,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:39:25.005: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9372
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9372
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9372
STEP: creating replication controller externalsvc in namespace services-9372
I0209 09:39:25.209092      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9372, replica count: 2
I0209 09:39:28.259630      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb  9 09:39:28.293: INFO: Creating new exec pod
Feb  9 09:39:32.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9372 exec execpodz9qv7 -- /bin/sh -x -c nslookup nodeport-service.services-9372.svc.cluster.local'
Feb  9 09:39:32.789: INFO: stderr: "+ nslookup nodeport-service.services-9372.svc.cluster.local\n"
Feb  9 09:39:32.789: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-9372.svc.cluster.local\tcanonical name = externalsvc.services-9372.svc.cluster.local.\nName:\texternalsvc.services-9372.svc.cluster.local\nAddress: 10.254.97.90\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9372, will wait for the garbage collector to delete the pods
Feb  9 09:39:32.856: INFO: Deleting ReplicationController externalsvc took: 11.17036ms
Feb  9 09:39:33.856: INFO: Terminating ReplicationController externalsvc pods took: 1.000250838s
Feb  9 09:40:24.987: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:40:25.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9372" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:60.020 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":38,"skipped":660,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:40:25.026: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-7be83a25-16f7-4321-aae3-f0caadab10b9
STEP: Creating a pod to test consume configMaps
Feb  9 09:40:25.216: INFO: Waiting up to 5m0s for pod "pod-configmaps-fbc1eedd-197a-4364-84a2-092e0a11f40f" in namespace "configmap-5201" to be "Succeeded or Failed"
Feb  9 09:40:25.224: INFO: Pod "pod-configmaps-fbc1eedd-197a-4364-84a2-092e0a11f40f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.649065ms
Feb  9 09:40:27.241: INFO: Pod "pod-configmaps-fbc1eedd-197a-4364-84a2-092e0a11f40f": Phase="Running", Reason="", readiness=true. Elapsed: 2.025065064s
Feb  9 09:40:29.254: INFO: Pod "pod-configmaps-fbc1eedd-197a-4364-84a2-092e0a11f40f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038092316s
STEP: Saw pod success
Feb  9 09:40:29.255: INFO: Pod "pod-configmaps-fbc1eedd-197a-4364-84a2-092e0a11f40f" satisfied condition "Succeeded or Failed"
Feb  9 09:40:29.258: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-configmaps-fbc1eedd-197a-4364-84a2-092e0a11f40f container agnhost-container: <nil>
STEP: delete the pod
Feb  9 09:40:29.344: INFO: Waiting for pod pod-configmaps-fbc1eedd-197a-4364-84a2-092e0a11f40f to disappear
Feb  9 09:40:29.348: INFO: Pod pod-configmaps-fbc1eedd-197a-4364-84a2-092e0a11f40f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:40:29.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5201" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":39,"skipped":661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:40:29.363: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Feb  9 09:40:29.516: INFO: created test-event-1
Feb  9 09:40:29.519: INFO: created test-event-2
Feb  9 09:40:29.523: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Feb  9 09:40:29.527: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Feb  9 09:40:29.537: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:40:29.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3846" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":40,"skipped":683,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:40:29.554: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb  9 09:40:29.718: INFO: Waiting up to 5m0s for pod "pod-fc6f4214-9725-41e4-9feb-809698117068" in namespace "emptydir-5768" to be "Succeeded or Failed"
Feb  9 09:40:29.727: INFO: Pod "pod-fc6f4214-9725-41e4-9feb-809698117068": Phase="Pending", Reason="", readiness=false. Elapsed: 9.253698ms
Feb  9 09:40:31.736: INFO: Pod "pod-fc6f4214-9725-41e4-9feb-809698117068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017639956s
Feb  9 09:40:33.755: INFO: Pod "pod-fc6f4214-9725-41e4-9feb-809698117068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037563521s
STEP: Saw pod success
Feb  9 09:40:33.756: INFO: Pod "pod-fc6f4214-9725-41e4-9feb-809698117068" satisfied condition "Succeeded or Failed"
Feb  9 09:40:33.758: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-fc6f4214-9725-41e4-9feb-809698117068 container test-container: <nil>
STEP: delete the pod
Feb  9 09:40:33.798: INFO: Waiting for pod pod-fc6f4214-9725-41e4-9feb-809698117068 to disappear
Feb  9 09:40:33.801: INFO: Pod pod-fc6f4214-9725-41e4-9feb-809698117068 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:40:33.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5768" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":41,"skipped":696,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:40:33.812: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1042
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb  9 09:40:40.058: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:40:40.063: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:40:42.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:40:42.074: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:40:44.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:40:44.079: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:40:46.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:40:46.080: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:40:48.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:40:48.081: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:40:50.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:40:50.073: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:40:52.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:40:52.082: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:40:54.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:40:54.078: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:40:56.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:40:56.080: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:40:58.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:40:58.079: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:00.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:00.079: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:02.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:02.078: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:04.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:04.078: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:06.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:06.078: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:08.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:08.079: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:10.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:10.082: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:12.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:12.080: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:14.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:14.077: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:16.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:16.078: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:18.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:18.073: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:20.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:20.079: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:22.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:22.077: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:24.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:24.082: INFO: Pod pod-with-prestop-http-hook still exists
Feb  9 09:41:26.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb  9 09:41:26.077: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:41:26.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1042" for this suite.

• [SLOW TEST:52.398 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":42,"skipped":721,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:41:26.211: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:41:28.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5269" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":43,"skipped":724,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:41:28.469: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-480a0316-cf88-47ea-b462-60368e5ff706
STEP: Creating a pod to test consume configMaps
Feb  9 09:41:28.650: INFO: Waiting up to 5m0s for pod "pod-configmaps-56df386e-5e7d-4932-8414-b1125551b720" in namespace "configmap-8423" to be "Succeeded or Failed"
Feb  9 09:41:28.656: INFO: Pod "pod-configmaps-56df386e-5e7d-4932-8414-b1125551b720": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162799ms
Feb  9 09:41:30.660: INFO: Pod "pod-configmaps-56df386e-5e7d-4932-8414-b1125551b720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010018562s
Feb  9 09:41:32.673: INFO: Pod "pod-configmaps-56df386e-5e7d-4932-8414-b1125551b720": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022785966s
STEP: Saw pod success
Feb  9 09:41:32.673: INFO: Pod "pod-configmaps-56df386e-5e7d-4932-8414-b1125551b720" satisfied condition "Succeeded or Failed"
Feb  9 09:41:32.676: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-configmaps-56df386e-5e7d-4932-8414-b1125551b720 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 09:41:32.705: INFO: Waiting for pod pod-configmaps-56df386e-5e7d-4932-8414-b1125551b720 to disappear
Feb  9 09:41:32.708: INFO: Pod pod-configmaps-56df386e-5e7d-4932-8414-b1125551b720 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:41:32.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8423" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":44,"skipped":728,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:41:32.718: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1237
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:42:32.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1237" for this suite.

• [SLOW TEST:60.193 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":783,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:42:32.912: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:42:33.087: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Pending, waiting for it to be Running (with Ready = true)
Feb  9 09:42:35.103: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = false)
Feb  9 09:42:37.102: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = false)
Feb  9 09:42:39.101: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = false)
Feb  9 09:42:41.105: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = false)
Feb  9 09:42:43.102: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = false)
Feb  9 09:42:45.096: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = false)
Feb  9 09:42:47.104: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = false)
Feb  9 09:42:49.102: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = false)
Feb  9 09:42:51.102: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = false)
Feb  9 09:42:53.098: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = false)
Feb  9 09:42:55.101: INFO: The status of Pod test-webserver-1d80c939-5e4f-4bcd-8c5f-bb8315232a69 is Running (Ready = true)
Feb  9 09:42:55.103: INFO: Container started at 2021-02-09 09:42:34 +0000 UTC, pod became ready at 2021-02-09 09:42:54 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:42:55.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3114" for this suite.

• [SLOW TEST:22.204 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":46,"skipped":798,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:42:55.117: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1245
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-e4c39a55-5412-470b-b73b-ec81038a0cb4
STEP: Creating a pod to test consume secrets
Feb  9 09:42:55.290: INFO: Waiting up to 5m0s for pod "pod-secrets-c1e1a0d4-ae36-45e1-8cbf-c157ce5cdc50" in namespace "secrets-1245" to be "Succeeded or Failed"
Feb  9 09:42:55.305: INFO: Pod "pod-secrets-c1e1a0d4-ae36-45e1-8cbf-c157ce5cdc50": Phase="Pending", Reason="", readiness=false. Elapsed: 14.056381ms
Feb  9 09:42:57.320: INFO: Pod "pod-secrets-c1e1a0d4-ae36-45e1-8cbf-c157ce5cdc50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029930635s
Feb  9 09:42:59.337: INFO: Pod "pod-secrets-c1e1a0d4-ae36-45e1-8cbf-c157ce5cdc50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046456961s
STEP: Saw pod success
Feb  9 09:42:59.337: INFO: Pod "pod-secrets-c1e1a0d4-ae36-45e1-8cbf-c157ce5cdc50" satisfied condition "Succeeded or Failed"
Feb  9 09:42:59.341: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-secrets-c1e1a0d4-ae36-45e1-8cbf-c157ce5cdc50 container secret-volume-test: <nil>
STEP: delete the pod
Feb  9 09:42:59.432: INFO: Waiting for pod pod-secrets-c1e1a0d4-ae36-45e1-8cbf-c157ce5cdc50 to disappear
Feb  9 09:42:59.438: INFO: Pod pod-secrets-c1e1a0d4-ae36-45e1-8cbf-c157ce5cdc50 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:42:59.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1245" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":47,"skipped":813,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:42:59.451: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4916
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb  9 09:43:07.664: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4916 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:43:07.665: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:43:07.907: INFO: Exec stderr: ""
Feb  9 09:43:07.907: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4916 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:43:07.907: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:43:08.093: INFO: Exec stderr: ""
Feb  9 09:43:08.093: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4916 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:43:08.093: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:43:08.263: INFO: Exec stderr: ""
Feb  9 09:43:08.263: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4916 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:43:08.263: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:43:08.437: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb  9 09:43:08.437: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4916 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:43:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:43:08.616: INFO: Exec stderr: ""
Feb  9 09:43:08.617: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4916 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:43:08.617: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:43:08.792: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb  9 09:43:08.792: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4916 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:43:08.792: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:43:08.973: INFO: Exec stderr: ""
Feb  9 09:43:08.973: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4916 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:43:08.973: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:43:09.151: INFO: Exec stderr: ""
Feb  9 09:43:09.151: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4916 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:43:09.151: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:43:09.333: INFO: Exec stderr: ""
Feb  9 09:43:09.333: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4916 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:43:09.333: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:43:09.522: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:43:09.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4916" for this suite.

• [SLOW TEST:10.092 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":48,"skipped":829,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:43:09.544: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Feb  9 09:43:09.768: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:43:09.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7657" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":49,"skipped":846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:43:09.801: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6632
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6632
STEP: Creating statefulset with conflicting port in namespace statefulset-6632
STEP: Waiting until pod test-pod will start running in namespace statefulset-6632
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6632
Feb  9 09:43:14.045: INFO: Observed stateful pod in namespace: statefulset-6632, name: ss-0, uid: 0789505e-c043-4746-8f1f-eef2fb4f5b2a, status phase: Pending. Waiting for statefulset controller to delete.
Feb  9 09:43:14.409: INFO: Observed stateful pod in namespace: statefulset-6632, name: ss-0, uid: 0789505e-c043-4746-8f1f-eef2fb4f5b2a, status phase: Failed. Waiting for statefulset controller to delete.
Feb  9 09:43:14.418: INFO: Observed stateful pod in namespace: statefulset-6632, name: ss-0, uid: 0789505e-c043-4746-8f1f-eef2fb4f5b2a, status phase: Failed. Waiting for statefulset controller to delete.
Feb  9 09:43:14.426: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6632
STEP: Removing pod with conflicting port in namespace statefulset-6632
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6632 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb  9 09:43:18.485: INFO: Deleting all statefulset in ns statefulset-6632
Feb  9 09:43:18.489: INFO: Scaling statefulset ss to 0
Feb  9 09:43:28.525: INFO: Waiting for statefulset status.replicas updated to 0
Feb  9 09:43:28.528: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:43:28.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6632" for this suite.

• [SLOW TEST:18.766 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":50,"skipped":869,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:43:28.571: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-148
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-1388480e-8d93-42ed-a7c1-0adde04d00e8
STEP: Creating a pod to test consume secrets
Feb  9 09:43:28.808: INFO: Waiting up to 5m0s for pod "pod-secrets-faf7b4ba-c347-4635-afcf-0ffe20454b7c" in namespace "secrets-148" to be "Succeeded or Failed"
Feb  9 09:43:28.816: INFO: Pod "pod-secrets-faf7b4ba-c347-4635-afcf-0ffe20454b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.229148ms
Feb  9 09:43:30.829: INFO: Pod "pod-secrets-faf7b4ba-c347-4635-afcf-0ffe20454b7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020761266s
STEP: Saw pod success
Feb  9 09:43:30.830: INFO: Pod "pod-secrets-faf7b4ba-c347-4635-afcf-0ffe20454b7c" satisfied condition "Succeeded or Failed"
Feb  9 09:43:30.832: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-secrets-faf7b4ba-c347-4635-afcf-0ffe20454b7c container secret-volume-test: <nil>
STEP: delete the pod
Feb  9 09:43:30.859: INFO: Waiting for pod pod-secrets-faf7b4ba-c347-4635-afcf-0ffe20454b7c to disappear
Feb  9 09:43:30.865: INFO: Pod pod-secrets-faf7b4ba-c347-4635-afcf-0ffe20454b7c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:43:30.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-148" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":921,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:43:30.875: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-efd39529-7b4b-4a9e-bce8-f9d1c6b90917
STEP: Creating a pod to test consume configMaps
Feb  9 09:43:31.070: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-67bbc9fc-aa5c-48bb-9c92-f4f9a8807d5d" in namespace "projected-1421" to be "Succeeded or Failed"
Feb  9 09:43:31.077: INFO: Pod "pod-projected-configmaps-67bbc9fc-aa5c-48bb-9c92-f4f9a8807d5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.672703ms
Feb  9 09:43:33.089: INFO: Pod "pod-projected-configmaps-67bbc9fc-aa5c-48bb-9c92-f4f9a8807d5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01931561s
Feb  9 09:43:35.101: INFO: Pod "pod-projected-configmaps-67bbc9fc-aa5c-48bb-9c92-f4f9a8807d5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031673939s
STEP: Saw pod success
Feb  9 09:43:35.102: INFO: Pod "pod-projected-configmaps-67bbc9fc-aa5c-48bb-9c92-f4f9a8807d5d" satisfied condition "Succeeded or Failed"
Feb  9 09:43:35.105: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-projected-configmaps-67bbc9fc-aa5c-48bb-9c92-f4f9a8807d5d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb  9 09:43:35.134: INFO: Waiting for pod pod-projected-configmaps-67bbc9fc-aa5c-48bb-9c92-f4f9a8807d5d to disappear
Feb  9 09:43:35.142: INFO: Pod pod-projected-configmaps-67bbc9fc-aa5c-48bb-9c92-f4f9a8807d5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:43:35.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1421" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":52,"skipped":925,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:43:35.155: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Feb  9 09:43:37.882: INFO: Successfully updated pod "annotationupdatedba6efa9-2a7a-4a19-9a8f-f3f3a696efb2"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:43:41.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4905" for this suite.

• [SLOW TEST:6.790 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":959,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:43:41.945: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7158
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7158
I0209 09:43:42.232173      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7158, replica count: 2
Feb  9 09:43:45.283: INFO: Creating new exec pod
I0209 09:43:45.283012      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 09:43:48.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-7158 exec execpodxrnbk -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb  9 09:43:49.038: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb  9 09:43:49.038: INFO: stdout: ""
Feb  9 09:43:49.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-7158 exec execpodxrnbk -- /bin/sh -x -c nc -zv -t -w 2 10.254.214.244 80'
Feb  9 09:43:49.350: INFO: stderr: "+ nc -zv -t -w 2 10.254.214.244 80\nConnection to 10.254.214.244 80 port [tcp/http] succeeded!\n"
Feb  9 09:43:49.350: INFO: stdout: ""
Feb  9 09:43:49.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-7158 exec execpodxrnbk -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.176 30178'
Feb  9 09:43:49.665: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.176 30178\nConnection to 10.0.0.176 30178 port [tcp/30178] succeeded!\n"
Feb  9 09:43:49.665: INFO: stdout: ""
Feb  9 09:43:49.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-7158 exec execpodxrnbk -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.248 30178'
Feb  9 09:43:49.995: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.248 30178\nConnection to 10.0.0.248 30178 port [tcp/30178] succeeded!\n"
Feb  9 09:43:49.995: INFO: stdout: ""
Feb  9 09:43:49.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-7158 exec execpodxrnbk -- /bin/sh -x -c nc -zv -t -w 2 10.6.0.143 30178'
Feb  9 09:43:50.339: INFO: stderr: "+ nc -zv -t -w 2 10.6.0.143 30178\nConnection to 10.6.0.143 30178 port [tcp/30178] succeeded!\n"
Feb  9 09:43:50.339: INFO: stdout: ""
Feb  9 09:43:50.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-7158 exec execpodxrnbk -- /bin/sh -x -c nc -zv -t -w 2 10.6.0.146 30178'
Feb  9 09:43:50.643: INFO: stderr: "+ nc -zv -t -w 2 10.6.0.146 30178\nConnection to 10.6.0.146 30178 port [tcp/30178] succeeded!\n"
Feb  9 09:43:50.643: INFO: stdout: ""
Feb  9 09:43:50.643: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:43:50.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7158" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:8.735 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":54,"skipped":959,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:43:50.680: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4744
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:43:51.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4744" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":55,"skipped":998,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:43:51.172: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1947
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 09:43:51.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da2a129d-562b-4082-bf5d-0c8418d5a73c" in namespace "downward-api-1947" to be "Succeeded or Failed"
Feb  9 09:43:51.357: INFO: Pod "downwardapi-volume-da2a129d-562b-4082-bf5d-0c8418d5a73c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.399386ms
Feb  9 09:43:53.376: INFO: Pod "downwardapi-volume-da2a129d-562b-4082-bf5d-0c8418d5a73c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024997325s
Feb  9 09:43:55.394: INFO: Pod "downwardapi-volume-da2a129d-562b-4082-bf5d-0c8418d5a73c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042012497s
STEP: Saw pod success
Feb  9 09:43:55.394: INFO: Pod "downwardapi-volume-da2a129d-562b-4082-bf5d-0c8418d5a73c" satisfied condition "Succeeded or Failed"
Feb  9 09:43:55.397: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-2 pod downwardapi-volume-da2a129d-562b-4082-bf5d-0c8418d5a73c container client-container: <nil>
STEP: delete the pod
Feb  9 09:43:55.486: INFO: Waiting for pod downwardapi-volume-da2a129d-562b-4082-bf5d-0c8418d5a73c to disappear
Feb  9 09:43:55.490: INFO: Pod downwardapi-volume-da2a129d-562b-4082-bf5d-0c8418d5a73c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:43:55.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1947" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":1007,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:43:55.502: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9759
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 09:43:55.678: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ceabe7c-d147-44df-8d3a-d04311bb678a" in namespace "projected-9759" to be "Succeeded or Failed"
Feb  9 09:43:55.681: INFO: Pod "downwardapi-volume-7ceabe7c-d147-44df-8d3a-d04311bb678a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.321217ms
Feb  9 09:43:57.690: INFO: Pod "downwardapi-volume-7ceabe7c-d147-44df-8d3a-d04311bb678a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012729635s
Feb  9 09:43:59.697: INFO: Pod "downwardapi-volume-7ceabe7c-d147-44df-8d3a-d04311bb678a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019666337s
STEP: Saw pod success
Feb  9 09:43:59.697: INFO: Pod "downwardapi-volume-7ceabe7c-d147-44df-8d3a-d04311bb678a" satisfied condition "Succeeded or Failed"
Feb  9 09:43:59.702: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-2 pod downwardapi-volume-7ceabe7c-d147-44df-8d3a-d04311bb678a container client-container: <nil>
STEP: delete the pod
Feb  9 09:43:59.725: INFO: Waiting for pod downwardapi-volume-7ceabe7c-d147-44df-8d3a-d04311bb678a to disappear
Feb  9 09:43:59.733: INFO: Pod downwardapi-volume-7ceabe7c-d147-44df-8d3a-d04311bb678a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:43:59.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9759" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":57,"skipped":1007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:43:59.746: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4658
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:43:59.929: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb  9 09:44:04.946: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  9 09:44:04.947: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb  9 09:44:04.974: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4658  588fbe18-aa4f-4e96-af12-888c7a70db74 17136 1 2021-02-09 09:44:04 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-02-09 09:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dc38b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb  9 09:44:04.985: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Feb  9 09:44:04.986: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb  9 09:44:04.986: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4658  ba9ffafa-43f5-4df5-9b03-bdea78b8ea16 17138 1 2021-02-09 09:43:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 588fbe18-aa4f-4e96-af12-888c7a70db74 0xc003f66d97 0xc003f66d98}] []  [{e2e.test Update apps/v1 2021-02-09 09:43:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-09 09:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"588fbe18-aa4f-4e96-af12-888c7a70db74\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f66e38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb  9 09:44:05.000: INFO: Pod "test-cleanup-controller-zpnm6" is available:
&Pod{ObjectMeta:{test-cleanup-controller-zpnm6 test-cleanup-controller- deployment-4658  0184a672-c31b-4095-8a83-525d7b6a6896 17120 0 2021-02-09 09:43:59 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.100.46.167/32 cni.projectcalico.org/podIPs:10.100.46.167/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller ba9ffafa-43f5-4df5-9b03-bdea78b8ea16 0xc002bc38b7 0xc002bc38b8}] []  [{kube-controller-manager Update v1 2021-02-09 09:43:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba9ffafa-43f5-4df5-9b03-bdea78b8ea16\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 09:44:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 09:44:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.46.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mdkm4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mdkm4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mdkm4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:43:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:44:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:44:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:43:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.100.46.167,StartTime:2021-02-09 09:43:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 09:44:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2654b759fbd36e08b2042146efcf1aca80c3f7566c22b1e6eb13bb52f1673a50,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.46.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:44:05.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4658" for this suite.

• [SLOW TEST:5.275 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":58,"skipped":1043,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:44:05.022: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8635
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-8635
STEP: creating replication controller nodeport-test in namespace services-8635
I0209 09:44:05.213665      25 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-8635, replica count: 2
I0209 09:44:08.267126      25 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 09:44:08.267: INFO: Creating new exec pod
Feb  9 09:44:11.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-8635 exec execpodh9hdh -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb  9 09:44:11.736: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb  9 09:44:11.736: INFO: stdout: ""
Feb  9 09:44:11.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-8635 exec execpodh9hdh -- /bin/sh -x -c nc -zv -t -w 2 10.254.182.19 80'
Feb  9 09:44:12.045: INFO: stderr: "+ nc -zv -t -w 2 10.254.182.19 80\nConnection to 10.254.182.19 80 port [tcp/http] succeeded!\n"
Feb  9 09:44:12.045: INFO: stdout: ""
Feb  9 09:44:12.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-8635 exec execpodh9hdh -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.176 30787'
Feb  9 09:44:12.369: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.176 30787\nConnection to 10.0.0.176 30787 port [tcp/30787] succeeded!\n"
Feb  9 09:44:12.369: INFO: stdout: ""
Feb  9 09:44:12.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-8635 exec execpodh9hdh -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.248 30787'
Feb  9 09:44:12.694: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.248 30787\nConnection to 10.0.0.248 30787 port [tcp/30787] succeeded!\n"
Feb  9 09:44:12.694: INFO: stdout: ""
Feb  9 09:44:12.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-8635 exec execpodh9hdh -- /bin/sh -x -c nc -zv -t -w 2 10.6.0.143 30787'
Feb  9 09:44:12.985: INFO: stderr: "+ nc -zv -t -w 2 10.6.0.143 30787\nConnection to 10.6.0.143 30787 port [tcp/30787] succeeded!\n"
Feb  9 09:44:12.985: INFO: stdout: ""
Feb  9 09:44:12.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-8635 exec execpodh9hdh -- /bin/sh -x -c nc -zv -t -w 2 10.6.0.146 30787'
Feb  9 09:44:13.262: INFO: stderr: "+ nc -zv -t -w 2 10.6.0.146 30787\nConnection to 10.6.0.146 30787 port [tcp/30787] succeeded!\n"
Feb  9 09:44:13.262: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:44:13.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8635" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:8.257 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":59,"skipped":1051,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:44:13.282: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5360
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Feb  9 09:44:13.444: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:44:17.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5360" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":60,"skipped":1063,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:44:17.240: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb  9 09:44:17.411: INFO: Waiting up to 5m0s for pod "pod-1021a244-bf03-41db-a0d0-3fe460bbc95b" in namespace "emptydir-6847" to be "Succeeded or Failed"
Feb  9 09:44:17.420: INFO: Pod "pod-1021a244-bf03-41db-a0d0-3fe460bbc95b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.592765ms
Feb  9 09:44:19.434: INFO: Pod "pod-1021a244-bf03-41db-a0d0-3fe460bbc95b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022895594s
STEP: Saw pod success
Feb  9 09:44:19.434: INFO: Pod "pod-1021a244-bf03-41db-a0d0-3fe460bbc95b" satisfied condition "Succeeded or Failed"
Feb  9 09:44:19.437: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-1021a244-bf03-41db-a0d0-3fe460bbc95b container test-container: <nil>
STEP: delete the pod
Feb  9 09:44:19.508: INFO: Waiting for pod pod-1021a244-bf03-41db-a0d0-3fe460bbc95b to disappear
Feb  9 09:44:19.512: INFO: Pod pod-1021a244-bf03-41db-a0d0-3fe460bbc95b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:44:19.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6847" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":61,"skipped":1074,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:44:19.522: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-f882ca8a-5f1f-49dd-8f96-6986b390ed61 in namespace container-probe-3258
Feb  9 09:44:23.710: INFO: Started pod test-webserver-f882ca8a-5f1f-49dd-8f96-6986b390ed61 in namespace container-probe-3258
STEP: checking the pod's current state and verifying that restartCount is present
Feb  9 09:44:23.713: INFO: Initial restart count of pod test-webserver-f882ca8a-5f1f-49dd-8f96-6986b390ed61 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:48:23.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3258" for this suite.

• [SLOW TEST:244.341 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":62,"skipped":1089,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:48:23.863: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Feb  9 09:48:24.023: INFO: Waiting up to 5m0s for pod "downward-api-f0e4fa65-fb7c-4c4c-b624-72c6f4cf63cb" in namespace "downward-api-8220" to be "Succeeded or Failed"
Feb  9 09:48:24.033: INFO: Pod "downward-api-f0e4fa65-fb7c-4c4c-b624-72c6f4cf63cb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.281558ms
Feb  9 09:48:26.046: INFO: Pod "downward-api-f0e4fa65-fb7c-4c4c-b624-72c6f4cf63cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02290424s
Feb  9 09:48:28.061: INFO: Pod "downward-api-f0e4fa65-fb7c-4c4c-b624-72c6f4cf63cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037943554s
STEP: Saw pod success
Feb  9 09:48:28.061: INFO: Pod "downward-api-f0e4fa65-fb7c-4c4c-b624-72c6f4cf63cb" satisfied condition "Succeeded or Failed"
Feb  9 09:48:28.065: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downward-api-f0e4fa65-fb7c-4c4c-b624-72c6f4cf63cb container dapi-container: <nil>
STEP: delete the pod
Feb  9 09:48:28.197: INFO: Waiting for pod downward-api-f0e4fa65-fb7c-4c4c-b624-72c6f4cf63cb to disappear
Feb  9 09:48:28.201: INFO: Pod downward-api-f0e4fa65-fb7c-4c4c-b624-72c6f4cf63cb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:48:28.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8220" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":1109,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:48:28.211: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 09:48:29.073: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 09:48:32.108: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:48:32.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-580" for this suite.
STEP: Destroying namespace "webhook-580-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":64,"skipped":1113,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:48:32.266: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb  9 09:48:32.443: INFO: Waiting up to 5m0s for pod "pod-fd68f99f-82a1-4753-b4ab-9b2522fb6558" in namespace "emptydir-4260" to be "Succeeded or Failed"
Feb  9 09:48:32.461: INFO: Pod "pod-fd68f99f-82a1-4753-b4ab-9b2522fb6558": Phase="Pending", Reason="", readiness=false. Elapsed: 17.526217ms
Feb  9 09:48:34.469: INFO: Pod "pod-fd68f99f-82a1-4753-b4ab-9b2522fb6558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026031073s
STEP: Saw pod success
Feb  9 09:48:34.469: INFO: Pod "pod-fd68f99f-82a1-4753-b4ab-9b2522fb6558" satisfied condition "Succeeded or Failed"
Feb  9 09:48:34.473: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-fd68f99f-82a1-4753-b4ab-9b2522fb6558 container test-container: <nil>
STEP: delete the pod
Feb  9 09:48:34.588: INFO: Waiting for pod pod-fd68f99f-82a1-4753-b4ab-9b2522fb6558 to disappear
Feb  9 09:48:34.593: INFO: Pod pod-fd68f99f-82a1-4753-b4ab-9b2522fb6558 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:48:34.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4260" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":1125,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:48:34.602: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 09:48:35.334: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 09:48:37.368: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748460915, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748460915, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748460915, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748460915, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 09:48:40.396: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:48:40.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1946" for this suite.
STEP: Destroying namespace "webhook-1946-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.961 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":66,"skipped":1129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:48:40.565: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-2839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:48:40.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2839" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":67,"skipped":1195,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:48:40.877: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Feb  9 09:48:41.036: INFO: PodSpec: initContainers in spec.initContainers
Feb  9 09:49:30.473: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-08ce0ed4-f599-481a-9306-57049581ed9b", GenerateName:"", Namespace:"init-container-4035", SelfLink:"", UID:"4c31aa60-7045-4075-b5a1-841b1caf18e2", ResourceVersion:"18584", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63748460921, loc:(*time.Location)(0x7962e20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"36383392"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.100.46.173/32", "cni.projectcalico.org/podIPs":"10.100.46.173/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002a2e5e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002a2e660)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002a2e680), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002a2e6a0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002a2e6c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002a2e6e0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-z6kd4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003d28ac0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z6kd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z6kd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z6kd4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0027dc818), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"v1-kube1-20-2-apco5j2qoq5i-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002a6f5e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027dc890)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027dc8b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0027dc8b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0027dc8bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0027bf5e0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748460921, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748460921, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748460921, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748460921, loc:(*time.Location)(0x7962e20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.5", PodIP:"10.100.46.173", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.46.173"}}, StartTime:(*v1.Time)(0xc002a2e700), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002a6f6c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002a6f730)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://14c871d30454cdf4162732ab7f478e4b80d36bcd900b1cd276121156c2dd6dcb", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002a2e7a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002a2e780), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0027dc91c)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:49:30.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4035" for this suite.

• [SLOW TEST:49.619 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":68,"skipped":1196,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:49:30.496: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Feb  9 09:49:30.675: INFO: Waiting up to 5m0s for pod "var-expansion-7882e1a1-3686-4cb0-a1a0-bb7000771854" in namespace "var-expansion-5920" to be "Succeeded or Failed"
Feb  9 09:49:30.682: INFO: Pod "var-expansion-7882e1a1-3686-4cb0-a1a0-bb7000771854": Phase="Pending", Reason="", readiness=false. Elapsed: 7.343259ms
Feb  9 09:49:32.693: INFO: Pod "var-expansion-7882e1a1-3686-4cb0-a1a0-bb7000771854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018568497s
STEP: Saw pod success
Feb  9 09:49:32.693: INFO: Pod "var-expansion-7882e1a1-3686-4cb0-a1a0-bb7000771854" satisfied condition "Succeeded or Failed"
Feb  9 09:49:32.695: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod var-expansion-7882e1a1-3686-4cb0-a1a0-bb7000771854 container dapi-container: <nil>
STEP: delete the pod
Feb  9 09:49:32.725: INFO: Waiting for pod var-expansion-7882e1a1-3686-4cb0-a1a0-bb7000771854 to disappear
Feb  9 09:49:32.728: INFO: Pod var-expansion-7882e1a1-3686-4cb0-a1a0-bb7000771854 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:49:32.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5920" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":1197,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:49:32.742: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:49:32.908: INFO: Creating deployment "webserver-deployment"
Feb  9 09:49:32.915: INFO: Waiting for observed generation 1
Feb  9 09:49:35.002: INFO: Waiting for all required pods to come up
Feb  9 09:49:35.010: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb  9 09:49:37.031: INFO: Waiting for deployment "webserver-deployment" to complete
Feb  9 09:49:37.039: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb  9 09:49:37.048: INFO: Updating deployment webserver-deployment
Feb  9 09:49:37.048: INFO: Waiting for observed generation 2
Feb  9 09:49:39.064: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb  9 09:49:39.068: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb  9 09:49:39.071: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb  9 09:49:39.080: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb  9 09:49:39.080: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb  9 09:49:39.084: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb  9 09:49:39.092: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb  9 09:49:39.092: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb  9 09:49:39.100: INFO: Updating deployment webserver-deployment
Feb  9 09:49:39.100: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb  9 09:49:39.107: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb  9 09:49:39.110: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb  9 09:49:41.146: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2576  9b0c23a8-0850-4a25-a020-9fc9a812f2d3 18995 3 2021-02-09 09:49:32 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-02-09 09:49:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6cbf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-02-09 09:49:39 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-02-09 09:49:39 +0000 UTC,LastTransitionTime:2021-02-09 09:49:32 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb  9 09:49:41.152: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-2576  ed1d7322-0607-4a19-8428-d3e980164ad5 18988 3 2021-02-09 09:49:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9b0c23a8-0850-4a25-a020-9fc9a812f2d3 0xc003f6cf97 0xc003f6cf98}] []  [{kube-controller-manager Update apps/v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b0c23a8-0850-4a25-a020-9fc9a812f2d3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6d018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  9 09:49:41.152: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb  9 09:49:41.153: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-2576  0a25ee2b-52b4-421c-a662-f431a97c19e5 18989 3 2021-02-09 09:49:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9b0c23a8-0850-4a25-a020-9fc9a812f2d3 0xc003f6d077 0xc003f6d078}] []  [{kube-controller-manager Update apps/v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9b0c23a8-0850-4a25-a020-9fc9a812f2d3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f6d0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb  9 09:49:41.167: INFO: Pod "webserver-deployment-795d758f88-2b4kn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2b4kn webserver-deployment-795d758f88- deployment-2576  ed269aaa-f4bf-46ea-bbb2-85bfd2d728a9 19070 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.239.176/32 cni.projectcalico.org/podIPs:10.100.239.176/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc003f6d567 0xc003f6d568}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.167: INFO: Pod "webserver-deployment-795d758f88-2qww6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2qww6 webserver-deployment-795d758f88- deployment-2576  8d00fdde-3480-4521-ae51-5f6fa412f582 18987 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc003f6d710 0xc003f6d711}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.168: INFO: Pod "webserver-deployment-795d758f88-2z4r8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2z4r8 webserver-deployment-795d758f88- deployment-2576  7cbad872-1080-42a7-94dd-82de34cc42ca 18999 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc003f6d8a0 0xc003f6d8a1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.168: INFO: Pod "webserver-deployment-795d758f88-4v867" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4v867 webserver-deployment-795d758f88- deployment-2576  ab7ff714-32be-4bc6-af78-a46f27bff1a8 18893 0 2021-02-09 09:49:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.239.170/32 cni.projectcalico.org/podIPs:10.100.239.170/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc003f6da30 0xc003f6da31}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2021-02-09 09:49:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.168: INFO: Pod "webserver-deployment-795d758f88-b9s2t" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-b9s2t webserver-deployment-795d758f88- deployment-2576  4b6ddd03-a61f-46e6-8b79-702a300a4a05 18954 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc003f6dbf0 0xc003f6dbf1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.169: INFO: Pod "webserver-deployment-795d758f88-d6qr2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-d6qr2 webserver-deployment-795d758f88- deployment-2576  1fa1f228-4201-452e-aeeb-d596ad8a646e 19040 0 2021-02-09 09:49:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.80.20/32 cni.projectcalico.org/podIPs:10.100.80.20/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc003f6dda0 0xc003f6dda1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:,StartTime:2021-02-09 09:49:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.169: INFO: Pod "webserver-deployment-795d758f88-frgms" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-frgms webserver-deployment-795d758f88- deployment-2576  1cad4506-eae1-4895-a2c2-d0f7afa3db08 19008 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc003f6df60 0xc003f6df61}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.169: INFO: Pod "webserver-deployment-795d758f88-hwnh8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hwnh8 webserver-deployment-795d758f88- deployment-2576  a4d22dfb-60a6-47f9-a5b1-ea0de35d3ee7 19029 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.46.178/32 cni.projectcalico.org/podIPs:10.100.46.178/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc000054320 0xc000054321}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.170: INFO: Pod "webserver-deployment-795d758f88-qmzgs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qmzgs webserver-deployment-795d758f88- deployment-2576  ad9913a9-37b8-4450-b7a6-260fc446d3e8 18886 0 2021-02-09 09:49:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.80.21/32 cni.projectcalico.org/podIPs:10.100.80.21/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc000054a10 0xc000054a11}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:,StartTime:2021-02-09 09:49:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.170: INFO: Pod "webserver-deployment-795d758f88-rlm5w" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rlm5w webserver-deployment-795d758f88- deployment-2576  f8b3a4e3-c32d-440f-bc69-eb1a970eeef7 19043 0 2021-02-09 09:49:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.46.177/32 cni.projectcalico.org/podIPs:10.100.46.177/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc000055950 0xc000055951}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2021-02-09 09:49:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.170: INFO: Pod "webserver-deployment-795d758f88-wq2hl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wq2hl webserver-deployment-795d758f88- deployment-2576  427b2ff1-03c5-4238-b3fa-cf13f3f82f2b 18903 0 2021-02-09 09:49:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.239.171/32 cni.projectcalico.org/podIPs:10.100.239.171/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc0025e4280 0xc0025e4281}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2021-02-09 09:49:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.171: INFO: Pod "webserver-deployment-795d758f88-z5chc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-z5chc webserver-deployment-795d758f88- deployment-2576  9c4cb269-16c6-4014-a90b-7a00ff10019f 19050 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.100.239.173/32 cni.projectcalico.org/podIPs:10.100.239.173/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc0025e45f0 0xc0025e45f1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.171: INFO: Pod "webserver-deployment-795d758f88-zlx4j" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zlx4j webserver-deployment-795d758f88- deployment-2576  a1cc3c23-5074-4f8a-baaf-ef796647e91d 19000 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 ed1d7322-0607-4a19-8428-d3e980164ad5 0xc0025e49c0 0xc0025e49c1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed1d7322-0607-4a19-8428-d3e980164ad5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.171: INFO: Pod "webserver-deployment-dd94f59b7-5tpmc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5tpmc webserver-deployment-dd94f59b7- deployment-2576  d2153614-b8be-40e9-8e2a-6c3af5dd27aa 18759 0 2021-02-09 09:49:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.46.176/32 cni.projectcalico.org/podIPs:10.100.46.176/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc0025e4cf0 0xc0025e4cf1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.46.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.100.46.176,StartTime:2021-02-09 09:49:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 09:49:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://256b9d2cd32b3b3e9dfe3565acb49872ea5aea81a23b6f5a8b6455b03e3b7da9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.46.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.172: INFO: Pod "webserver-deployment-dd94f59b7-95rqr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-95rqr webserver-deployment-dd94f59b7- deployment-2576  fb9d7ccb-37f7-4038-9d71-53645adc8459 19010 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc0025e5080 0xc0025e5081}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.172: INFO: Pod "webserver-deployment-dd94f59b7-9ls7j" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9ls7j webserver-deployment-dd94f59b7- deployment-2576  fdfce170-159c-4faf-bf8c-c17c7d6c7f28 18751 0 2021-02-09 09:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.46.175/32 cni.projectcalico.org/podIPs:10.100.46.175/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc0025e5530 0xc0025e5531}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.46.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.100.46.175,StartTime:2021-02-09 09:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 09:49:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://942eb5d1783d4bc16e80237afa448b4d22e6abbc607b704ed181685532a5d425,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.46.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.172: INFO: Pod "webserver-deployment-dd94f59b7-bsd8p" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bsd8p webserver-deployment-dd94f59b7- deployment-2576  8fc9df97-388e-470b-856e-c975e33377fa 19022 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc0025e5920 0xc0025e5921}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.173: INFO: Pod "webserver-deployment-dd94f59b7-bt5nk" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bt5nk webserver-deployment-dd94f59b7- deployment-2576  6260e1a7-9d0a-438f-aba8-7ae3d8a4e301 18755 0 2021-02-09 09:49:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.46.174/32 cni.projectcalico.org/podIPs:10.100.46.174/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc0025e5c00 0xc0025e5c01}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.46.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.100.46.174,StartTime:2021-02-09 09:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 09:49:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d8596425e6375890a959c3820628fe461aebf9ed853ecf5af28fda5da7b46fb4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.46.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.173: INFO: Pod "webserver-deployment-dd94f59b7-ffkbt" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ffkbt webserver-deployment-dd94f59b7- deployment-2576  8295ee62-c910-450f-a05c-6b22f600f58a 19063 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.239.175/32 cni.projectcalico.org/podIPs:10.100.239.175/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc0025e5ea0 0xc0025e5ea1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.174: INFO: Pod "webserver-deployment-dd94f59b7-g76r9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-g76r9 webserver-deployment-dd94f59b7- deployment-2576  835c0a62-4352-4f2d-aae1-cd826c044c34 18768 0 2021-02-09 09:49:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.239.166/32 cni.projectcalico.org/podIPs:10.100.239.166/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc003416120 0xc003416121}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 09:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.239.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:10.100.239.166,StartTime:2021-02-09 09:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 09:49:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://cc6b9c6e34e3fc4d9164d14a64d5c4445ed3e178a0f2428fd88091cd4d7513e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.239.166,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.174: INFO: Pod "webserver-deployment-dd94f59b7-jz5s5" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jz5s5 webserver-deployment-dd94f59b7- deployment-2576  57b94479-80fc-47d2-bb70-9f9caaaea6d7 18796 0 2021-02-09 09:49:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.80.19/32 cni.projectcalico.org/podIPs:10.100.80.19/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc0034162f0 0xc0034162f1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 09:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.80.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:10.100.80.19,StartTime:2021-02-09 09:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 09:49:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a9a75b411fd823923603bde9aae2a5c3011983c4f547351d28e92201b7b59571,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.80.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.176: INFO: Pod "webserver-deployment-dd94f59b7-kwf64" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kwf64 webserver-deployment-dd94f59b7- deployment-2576  3716ed18-1597-4d15-b822-9aaa0282743e 18771 0 2021-02-09 09:49:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.239.167/32 cni.projectcalico.org/podIPs:10.100.239.167/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc0034164a0 0xc0034164a1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 09:49:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.239.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:10.100.239.167,StartTime:2021-02-09 09:49:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 09:49:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://52cd3e6c356e12424575385669896b7b9263ce43025f1a738c474c40dd6f5262,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.239.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.176: INFO: Pod "webserver-deployment-dd94f59b7-mx96c" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mx96c webserver-deployment-dd94f59b7- deployment-2576  34829d40-aaa7-4864-a10e-05da326b486e 19072 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.239.177/32 cni.projectcalico.org/podIPs:10.100.239.177/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc003416660 0xc003416661}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.176: INFO: Pod "webserver-deployment-dd94f59b7-n6wtc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n6wtc webserver-deployment-dd94f59b7- deployment-2576  dda7fbb7-1ed7-4893-a5b9-f2b8ce975c44 18996 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc0034167f0 0xc0034167f1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.177: INFO: Pod "webserver-deployment-dd94f59b7-n9r6z" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n9r6z webserver-deployment-dd94f59b7- deployment-2576  bc7338d3-780d-41e6-9502-49b7d4f94163 19056 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.239.174/32 cni.projectcalico.org/podIPs:10.100.239.174/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc003416960 0xc003416961}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.177: INFO: Pod "webserver-deployment-dd94f59b7-nlhcj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nlhcj webserver-deployment-dd94f59b7- deployment-2576  e90ecf14-fa43-4431-a5e3-ba77d774d099 19089 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.239.178/32 cni.projectcalico.org/podIPs:10.100.239.178/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc003416b00 0xc003416b01}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.177: INFO: Pod "webserver-deployment-dd94f59b7-qcrzz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qcrzz webserver-deployment-dd94f59b7- deployment-2576  7bba1f4c-dd37-4df6-a25f-d12167b10af9 18793 0 2021-02-09 09:49:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.80.18/32 cni.projectcalico.org/podIPs:10.100.80.18/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc003416cd0 0xc003416cd1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 09:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.80.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:10.100.80.18,StartTime:2021-02-09 09:49:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 09:49:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a523bda784061bc48c89206d017411a21912b82de3766df14b1eb930df0c815a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.80.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.178: INFO: Pod "webserver-deployment-dd94f59b7-rmsgb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rmsgb webserver-deployment-dd94f59b7- deployment-2576  38745f14-29fc-4600-81e9-d088b3c10a75 19035 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.239.172/32 cni.projectcalico.org/podIPs:10.100.239.172/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc003416e80 0xc003416e81}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.178: INFO: Pod "webserver-deployment-dd94f59b7-sjh5d" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sjh5d webserver-deployment-dd94f59b7- deployment-2576  239cebfe-b43d-4086-88b6-23287dc7dd24 18980 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc003417010 0xc003417011}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.178: INFO: Pod "webserver-deployment-dd94f59b7-t94t2" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-t94t2 webserver-deployment-dd94f59b7- deployment-2576  e475ba18-75c4-4b43-90fe-3193002e9fbc 18798 0 2021-02-09 09:49:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.80.17/32 cni.projectcalico.org/podIPs:10.100.80.17/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc0034171a0 0xc0034171a1}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 09:49:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 09:49:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.80.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:10.100.80.17,StartTime:2021-02-09 09:49:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 09:49:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f85b5ef276f1b06b5b6b948d29bf397638ef44d892628b3844ea161baf80ddff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.80.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.179: INFO: Pod "webserver-deployment-dd94f59b7-w5rzc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w5rzc webserver-deployment-dd94f59b7- deployment-2576  755d435f-c38c-40db-bc6e-527ce9f023b8 19051 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.100.46.179/32 cni.projectcalico.org/podIPs:10.100.46.179/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc003417370 0xc003417371}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-02-09 09:49:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.179: INFO: Pod "webserver-deployment-dd94f59b7-wd2f2" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wd2f2 webserver-deployment-dd94f59b7- deployment-2576  15321ae2-bfee-46d7-8c02-2a7ded285e95 18994 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc003417500 0xc003417501}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:49:41.179: INFO: Pod "webserver-deployment-dd94f59b7-wvlhn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wvlhn webserver-deployment-dd94f59b7- deployment-2576  3690d519-20a8-4e89-a015-b8cf1f354582 19001 0 2021-02-09 09:49:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 0a25ee2b-52b4-421c-a662-f431a97c19e5 0xc003417670 0xc003417671}] []  [{kube-controller-manager Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a25ee2b-52b4-421c-a662-f431a97c19e5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 09:49:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qcwpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qcwpv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qcwpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:,StartTime:2021-02-09 09:49:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:49:41.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2576" for this suite.

• [SLOW TEST:8.473 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":70,"skipped":1198,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:49:41.216: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5434
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb  9 09:49:45.436: INFO: &Pod{ObjectMeta:{send-events-7e7d4b60-a68f-475b-8850-ead65fcefb1e  events-5434  26aed710-2656-4d50-94cd-c7424cb10194 19219 0 2021-02-09 09:49:41 +0000 UTC <nil> <nil> map[name:foo time:385534706] map[cni.projectcalico.org/podIP:10.100.239.179/32 cni.projectcalico.org/podIPs:10.100.239.179/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-02-09 09:49:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 09:49:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 09:49:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.239.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6m9qt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6m9qt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6m9qt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 09:49:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:10.100.239.179,StartTime:2021-02-09 09:49:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 09:49:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://86cb153fb77bd0e273f575d44f310cc429630749e1ccc204bf502a1ec7961669,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.239.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb  9 09:49:47.525: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb  9 09:49:49.541: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:49:49.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5434" for this suite.

• [SLOW TEST:8.377 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":71,"skipped":1217,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:49:49.594: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9162
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:50:00.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9162" for this suite.

• [SLOW TEST:11.317 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":72,"skipped":1217,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:50:00.911: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5043
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0209 09:50:02.643817      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0209 09:50:02.644003      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0209 09:50:02.644042      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb  9 09:50:02.644: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:50:02.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5043" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":73,"skipped":1234,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:50:02.655: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb  9 09:50:08.928: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  9 09:50:08.932: INFO: Pod pod-with-poststart-http-hook still exists
Feb  9 09:50:10.933: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  9 09:50:10.947: INFO: Pod pod-with-poststart-http-hook still exists
Feb  9 09:50:12.933: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  9 09:50:12.950: INFO: Pod pod-with-poststart-http-hook still exists
Feb  9 09:50:14.933: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  9 09:50:14.940: INFO: Pod pod-with-poststart-http-hook still exists
Feb  9 09:50:16.932: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  9 09:50:16.948: INFO: Pod pod-with-poststart-http-hook still exists
Feb  9 09:50:18.933: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  9 09:50:18.961: INFO: Pod pod-with-poststart-http-hook still exists
Feb  9 09:50:20.933: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  9 09:50:20.947: INFO: Pod pod-with-poststart-http-hook still exists
Feb  9 09:50:22.933: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  9 09:50:22.951: INFO: Pod pod-with-poststart-http-hook still exists
Feb  9 09:50:24.932: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb  9 09:50:24.939: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:50:24.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5885" for this suite.

• [SLOW TEST:22.299 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1254,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:50:24.955: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Feb  9 09:50:25.160: INFO: observed Pod pod-test in namespace pods-1234 in phase Pending conditions []
Feb  9 09:50:25.163: INFO: observed Pod pod-test in namespace pods-1234 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 09:50:25 +0000 UTC  }]
Feb  9 09:50:25.176: INFO: observed Pod pod-test in namespace pods-1234 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 09:50:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 09:50:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 09:50:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 09:50:25 +0000 UTC  }]
Feb  9 09:50:26.066: INFO: observed Pod pod-test in namespace pods-1234 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 09:50:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 09:50:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 09:50:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 09:50:25 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Feb  9 09:50:27.556: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Feb  9 09:50:27.589: INFO: observed event type ADDED
Feb  9 09:50:27.589: INFO: observed event type MODIFIED
Feb  9 09:50:27.590: INFO: observed event type MODIFIED
Feb  9 09:50:27.590: INFO: observed event type MODIFIED
Feb  9 09:50:27.590: INFO: observed event type MODIFIED
Feb  9 09:50:27.590: INFO: observed event type MODIFIED
Feb  9 09:50:27.590: INFO: observed event type MODIFIED
Feb  9 09:50:27.591: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:50:27.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1234" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":75,"skipped":1268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:50:27.602: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Feb  9 09:50:27.759: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb  9 09:50:27.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 create -f -'
Feb  9 09:50:28.063: INFO: stderr: ""
Feb  9 09:50:28.063: INFO: stdout: "service/agnhost-replica created\n"
Feb  9 09:50:28.063: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb  9 09:50:28.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 create -f -'
Feb  9 09:50:28.407: INFO: stderr: ""
Feb  9 09:50:28.407: INFO: stdout: "service/agnhost-primary created\n"
Feb  9 09:50:28.407: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb  9 09:50:28.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 create -f -'
Feb  9 09:50:28.744: INFO: stderr: ""
Feb  9 09:50:28.744: INFO: stdout: "service/frontend created\n"
Feb  9 09:50:28.744: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb  9 09:50:28.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 create -f -'
Feb  9 09:50:29.054: INFO: stderr: ""
Feb  9 09:50:29.054: INFO: stdout: "deployment.apps/frontend created\n"
Feb  9 09:50:29.055: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb  9 09:50:29.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 create -f -'
Feb  9 09:50:29.401: INFO: stderr: ""
Feb  9 09:50:29.401: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb  9 09:50:29.402: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb  9 09:50:29.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 create -f -'
Feb  9 09:50:29.731: INFO: stderr: ""
Feb  9 09:50:29.731: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Feb  9 09:50:29.731: INFO: Waiting for all frontend pods to be Running.
Feb  9 09:50:34.782: INFO: Waiting for frontend to serve content.
Feb  9 09:50:34.801: INFO: Trying to add a new entry to the guestbook.
Feb  9 09:50:34.813: INFO: Verifying that added entry can be retrieved.
Feb  9 09:50:34.821: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Feb  9 09:50:39.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 delete --grace-period=0 --force -f -'
Feb  9 09:50:40.013: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  9 09:50:40.013: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Feb  9 09:50:40.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 delete --grace-period=0 --force -f -'
Feb  9 09:50:40.233: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  9 09:50:40.233: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb  9 09:50:40.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 delete --grace-period=0 --force -f -'
Feb  9 09:50:40.362: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  9 09:50:40.362: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb  9 09:50:40.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 delete --grace-period=0 --force -f -'
Feb  9 09:50:40.519: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  9 09:50:40.519: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb  9 09:50:40.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 delete --grace-period=0 --force -f -'
Feb  9 09:50:40.643: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  9 09:50:40.643: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Feb  9 09:50:40.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2171 delete --grace-period=0 --force -f -'
Feb  9 09:50:40.766: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  9 09:50:40.766: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:50:40.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2171" for this suite.

• [SLOW TEST:13.178 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":76,"skipped":1290,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:50:40.780: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 09:50:40.970: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43e5717f-4d54-4101-9c8a-ca6282286393" in namespace "downward-api-5382" to be "Succeeded or Failed"
Feb  9 09:50:40.983: INFO: Pod "downwardapi-volume-43e5717f-4d54-4101-9c8a-ca6282286393": Phase="Pending", Reason="", readiness=false. Elapsed: 12.260676ms
Feb  9 09:50:42.991: INFO: Pod "downwardapi-volume-43e5717f-4d54-4101-9c8a-ca6282286393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020396437s
Feb  9 09:50:44.995: INFO: Pod "downwardapi-volume-43e5717f-4d54-4101-9c8a-ca6282286393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024945627s
STEP: Saw pod success
Feb  9 09:50:44.995: INFO: Pod "downwardapi-volume-43e5717f-4d54-4101-9c8a-ca6282286393" satisfied condition "Succeeded or Failed"
Feb  9 09:50:44.999: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod downwardapi-volume-43e5717f-4d54-4101-9c8a-ca6282286393 container client-container: <nil>
STEP: delete the pod
Feb  9 09:50:45.029: INFO: Waiting for pod downwardapi-volume-43e5717f-4d54-4101-9c8a-ca6282286393 to disappear
Feb  9 09:50:45.037: INFO: Pod downwardapi-volume-43e5717f-4d54-4101-9c8a-ca6282286393 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:50:45.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5382" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1301,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:50:45.050: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Feb  9 09:50:45.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-641 cluster-info'
Feb  9 09:50:45.327: INFO: stderr: ""
Feb  9 09:50:45.327: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:50:45.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-641" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":78,"skipped":1309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:50:45.343: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb  9 09:50:45.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-1661 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Feb  9 09:50:45.670: INFO: stderr: ""
Feb  9 09:50:45.670: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Feb  9 09:50:45.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-1661 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Feb  9 09:50:46.117: INFO: stderr: ""
Feb  9 09:50:46.118: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Feb  9 09:50:46.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-1661 delete pods e2e-test-httpd-pod'
Feb  9 09:51:25.011: INFO: stderr: ""
Feb  9 09:51:25.012: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:51:25.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1661" for this suite.

• [SLOW TEST:39.686 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":79,"skipped":1333,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:51:25.029: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-479
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb  9 09:51:25.196: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:51:29.176: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:51:45.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-479" for this suite.

• [SLOW TEST:20.529 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":80,"skipped":1338,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:51:45.560: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Feb  9 09:51:48.362: INFO: Successfully updated pod "labelsupdate7bad2022-0311-42fe-aef3-4d40454600ed"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:51:50.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4302" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":81,"skipped":1340,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:51:50.419: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9785
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-00fa61d8-fe79-41ef-b039-d4618ff5649b
STEP: Creating a pod to test consume secrets
Feb  9 09:51:50.607: INFO: Waiting up to 5m0s for pod "pod-secrets-629ee81b-16f6-45a0-ba1e-8661dccc35f1" in namespace "secrets-9785" to be "Succeeded or Failed"
Feb  9 09:51:50.613: INFO: Pod "pod-secrets-629ee81b-16f6-45a0-ba1e-8661dccc35f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.800725ms
Feb  9 09:51:52.627: INFO: Pod "pod-secrets-629ee81b-16f6-45a0-ba1e-8661dccc35f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02007199s
STEP: Saw pod success
Feb  9 09:51:52.627: INFO: Pod "pod-secrets-629ee81b-16f6-45a0-ba1e-8661dccc35f1" satisfied condition "Succeeded or Failed"
Feb  9 09:51:52.630: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-secrets-629ee81b-16f6-45a0-ba1e-8661dccc35f1 container secret-volume-test: <nil>
STEP: delete the pod
Feb  9 09:51:52.660: INFO: Waiting for pod pod-secrets-629ee81b-16f6-45a0-ba1e-8661dccc35f1 to disappear
Feb  9 09:51:52.664: INFO: Pod pod-secrets-629ee81b-16f6-45a0-ba1e-8661dccc35f1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:51:52.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9785" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":82,"skipped":1347,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:51:52.677: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 09:51:53.508: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 09:51:55.526: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748461113, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748461113, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748461113, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748461113, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 09:51:58.559: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:51:58.570: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2447-crds.webhook.example.com via the AdmissionRegistration API
Feb  9 09:51:59.111: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:51:59.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2260" for this suite.
STEP: Destroying namespace "webhook-2260-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.259 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":83,"skipped":1358,"failed":0}
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:51:59.936: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-863
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-863.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-863.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-863.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-863.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-863.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-863.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  9 09:52:10.219: INFO: Unable to read wheezy_udp@PodARecord from pod dns-863/dns-test-2a6a7b4b-6717-47fb-b10e-8922a9eae8e5: the server could not find the requested resource (get pods dns-test-2a6a7b4b-6717-47fb-b10e-8922a9eae8e5)
Feb  9 09:52:10.223: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-863/dns-test-2a6a7b4b-6717-47fb-b10e-8922a9eae8e5: the server could not find the requested resource (get pods dns-test-2a6a7b4b-6717-47fb-b10e-8922a9eae8e5)
Feb  9 09:52:10.236: INFO: Unable to read jessie_udp@PodARecord from pod dns-863/dns-test-2a6a7b4b-6717-47fb-b10e-8922a9eae8e5: the server could not find the requested resource (get pods dns-test-2a6a7b4b-6717-47fb-b10e-8922a9eae8e5)
Feb  9 09:52:10.241: INFO: Unable to read jessie_tcp@PodARecord from pod dns-863/dns-test-2a6a7b4b-6717-47fb-b10e-8922a9eae8e5: the server could not find the requested resource (get pods dns-test-2a6a7b4b-6717-47fb-b10e-8922a9eae8e5)
Feb  9 09:52:10.241: INFO: Lookups using dns-863/dns-test-2a6a7b4b-6717-47fb-b10e-8922a9eae8e5 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb  9 09:52:15.289: INFO: DNS probes using dns-863/dns-test-2a6a7b4b-6717-47fb-b10e-8922a9eae8e5 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:52:15.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-863" for this suite.

• [SLOW TEST:15.400 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":84,"skipped":1358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:52:15.338: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb  9 09:52:17.549: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:52:17.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1213" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1386,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:52:17.579: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 09:52:17.746: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a87f8fb3-8338-4b98-a1db-c36552d6cfc7" in namespace "projected-5627" to be "Succeeded or Failed"
Feb  9 09:52:17.756: INFO: Pod "downwardapi-volume-a87f8fb3-8338-4b98-a1db-c36552d6cfc7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.818301ms
Feb  9 09:52:19.773: INFO: Pod "downwardapi-volume-a87f8fb3-8338-4b98-a1db-c36552d6cfc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026443259s
Feb  9 09:52:21.788: INFO: Pod "downwardapi-volume-a87f8fb3-8338-4b98-a1db-c36552d6cfc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041543754s
STEP: Saw pod success
Feb  9 09:52:21.788: INFO: Pod "downwardapi-volume-a87f8fb3-8338-4b98-a1db-c36552d6cfc7" satisfied condition "Succeeded or Failed"
Feb  9 09:52:21.790: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downwardapi-volume-a87f8fb3-8338-4b98-a1db-c36552d6cfc7 container client-container: <nil>
STEP: delete the pod
Feb  9 09:52:21.821: INFO: Waiting for pod downwardapi-volume-a87f8fb3-8338-4b98-a1db-c36552d6cfc7 to disappear
Feb  9 09:52:21.826: INFO: Pod downwardapi-volume-a87f8fb3-8338-4b98-a1db-c36552d6cfc7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:52:21.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5627" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":86,"skipped":1397,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:52:21.835: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb  9 09:52:22.023: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:22.026: INFO: Number of nodes with available pods: 0
Feb  9 09:52:22.026: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 09:52:23.151: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:23.157: INFO: Number of nodes with available pods: 0
Feb  9 09:52:23.158: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 09:52:24.036: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:24.041: INFO: Number of nodes with available pods: 0
Feb  9 09:52:24.041: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 09:52:25.038: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:25.041: INFO: Number of nodes with available pods: 3
Feb  9 09:52:25.041: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb  9 09:52:25.063: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:25.066: INFO: Number of nodes with available pods: 2
Feb  9 09:52:25.066: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:26.076: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:26.080: INFO: Number of nodes with available pods: 2
Feb  9 09:52:26.080: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:27.076: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:27.081: INFO: Number of nodes with available pods: 2
Feb  9 09:52:27.082: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:28.080: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:28.084: INFO: Number of nodes with available pods: 2
Feb  9 09:52:28.084: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:29.078: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:29.081: INFO: Number of nodes with available pods: 2
Feb  9 09:52:29.081: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:30.089: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:30.092: INFO: Number of nodes with available pods: 2
Feb  9 09:52:30.093: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:31.077: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:31.081: INFO: Number of nodes with available pods: 2
Feb  9 09:52:31.081: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:32.080: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:32.084: INFO: Number of nodes with available pods: 2
Feb  9 09:52:32.084: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:33.074: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:33.080: INFO: Number of nodes with available pods: 2
Feb  9 09:52:33.080: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:34.077: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:34.082: INFO: Number of nodes with available pods: 2
Feb  9 09:52:34.082: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:35.075: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:35.078: INFO: Number of nodes with available pods: 2
Feb  9 09:52:35.078: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:36.080: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:36.085: INFO: Number of nodes with available pods: 2
Feb  9 09:52:36.085: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:37.073: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:37.078: INFO: Number of nodes with available pods: 2
Feb  9 09:52:37.078: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:38.081: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:38.087: INFO: Number of nodes with available pods: 2
Feb  9 09:52:38.087: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:39.084: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:39.093: INFO: Number of nodes with available pods: 2
Feb  9 09:52:39.093: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:40.078: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:40.081: INFO: Number of nodes with available pods: 2
Feb  9 09:52:40.082: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:41.079: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:41.082: INFO: Number of nodes with available pods: 2
Feb  9 09:52:41.082: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:42.078: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:42.082: INFO: Number of nodes with available pods: 2
Feb  9 09:52:42.082: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:43.077: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:43.081: INFO: Number of nodes with available pods: 2
Feb  9 09:52:43.081: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:44.080: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:44.084: INFO: Number of nodes with available pods: 2
Feb  9 09:52:44.084: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:45.078: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:45.081: INFO: Number of nodes with available pods: 2
Feb  9 09:52:45.081: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:46.078: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:46.082: INFO: Number of nodes with available pods: 2
Feb  9 09:52:46.082: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:47.082: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:47.087: INFO: Number of nodes with available pods: 2
Feb  9 09:52:47.087: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:48.080: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:48.086: INFO: Number of nodes with available pods: 2
Feb  9 09:52:48.086: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:49.081: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:49.085: INFO: Number of nodes with available pods: 2
Feb  9 09:52:49.085: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:50.079: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:50.083: INFO: Number of nodes with available pods: 2
Feb  9 09:52:50.083: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:51.080: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:51.084: INFO: Number of nodes with available pods: 2
Feb  9 09:52:51.084: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:52.081: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:52.086: INFO: Number of nodes with available pods: 2
Feb  9 09:52:52.086: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:53.077: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:53.081: INFO: Number of nodes with available pods: 2
Feb  9 09:52:53.081: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:54.079: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:54.082: INFO: Number of nodes with available pods: 2
Feb  9 09:52:54.082: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:55.078: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:55.082: INFO: Number of nodes with available pods: 2
Feb  9 09:52:55.082: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:56.078: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:56.084: INFO: Number of nodes with available pods: 2
Feb  9 09:52:56.084: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:57.076: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:57.080: INFO: Number of nodes with available pods: 2
Feb  9 09:52:57.081: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:58.080: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:58.083: INFO: Number of nodes with available pods: 2
Feb  9 09:52:58.083: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:52:59.080: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:52:59.083: INFO: Number of nodes with available pods: 2
Feb  9 09:52:59.083: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:53:00.079: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:53:00.083: INFO: Number of nodes with available pods: 2
Feb  9 09:53:00.083: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:53:01.076: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:53:01.081: INFO: Number of nodes with available pods: 2
Feb  9 09:53:01.081: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:53:02.078: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:53:02.083: INFO: Number of nodes with available pods: 2
Feb  9 09:53:02.083: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:53:03.077: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:53:03.080: INFO: Number of nodes with available pods: 2
Feb  9 09:53:03.080: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:53:04.078: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:53:04.082: INFO: Number of nodes with available pods: 2
Feb  9 09:53:04.082: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:53:05.077: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:53:05.083: INFO: Number of nodes with available pods: 2
Feb  9 09:53:05.083: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:53:06.077: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:53:06.082: INFO: Number of nodes with available pods: 2
Feb  9 09:53:06.082: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 09:53:07.082: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 09:53:07.085: INFO: Number of nodes with available pods: 3
Feb  9 09:53:07.085: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1848, will wait for the garbage collector to delete the pods
Feb  9 09:53:07.160: INFO: Deleting DaemonSet.extensions daemon-set took: 15.137593ms
Feb  9 09:53:08.160: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000160825s
Feb  9 09:54:03.676: INFO: Number of nodes with available pods: 0
Feb  9 09:54:03.677: INFO: Number of running nodes: 0, number of available pods: 0
Feb  9 09:54:03.681: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21273"},"items":null}

Feb  9 09:54:03.683: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21273"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:54:03.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1848" for this suite.

• [SLOW TEST:101.883 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":87,"skipped":1402,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:54:03.720: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8598
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8457db4c-bcad-4f10-85fe-de7c17379b47
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-8457db4c-bcad-4f10-85fe-de7c17379b47
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:54:08.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8598" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":88,"skipped":1408,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:54:08.146: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2814
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:54:08.299: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:54:08.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2814" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":89,"skipped":1415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:54:08.869: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Feb  9 09:54:09.038: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6673  47cac4dd-4bbf-4266-896e-38ed301222a5 21364 0 2021-02-09 09:54:09 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-02-09 09:54:09 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7shlj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7shlj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7shlj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb  9 09:54:09.042: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb  9 09:54:11.055: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb  9 09:54:13.058: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Feb  9 09:54:13.058: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6673 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:54:13.058: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Verifying customized DNS server is configured on pod...
Feb  9 09:54:13.316: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6673 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 09:54:13.316: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 09:54:13.490: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:54:13.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6673" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":90,"skipped":1459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:54:13.524: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3317
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb  9 09:54:13.693: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  9 09:55:13.755: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:55:13.760: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-8964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Feb  9 09:55:15.961: INFO: found a healthy node: v1-kube1-20-2-apco5j2qoq5i-node-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:55:26.069: INFO: pods created so far: [1 1 1]
Feb  9 09:55:26.069: INFO: length of pods created so far: 3
Feb  9 09:56:08.091: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:56:15.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8964" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:56:15.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3317" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:121.686 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":91,"skipped":1488,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:56:15.211: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb  9 09:56:19.551: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:19.557: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:21.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:21.570: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:23.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:23.570: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:25.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:25.575: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:27.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:27.566: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:29.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:29.575: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:31.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:31.573: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:33.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:33.570: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:35.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:35.574: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:37.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:37.568: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:39.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:39.573: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:41.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:41.577: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:43.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:43.568: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:45.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:45.572: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:47.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:47.567: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:49.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:49.572: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:51.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:51.573: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:53.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:53.569: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:55.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:55.575: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:57.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:57.567: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:56:59.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:56:59.573: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:01.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:01.576: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:03.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:03.577: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:05.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:05.566: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:07.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:07.566: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:09.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:09.575: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:11.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:11.573: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:13.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:13.570: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:15.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:15.574: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:17.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:17.564: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:19.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:19.574: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:21.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:21.573: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:23.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:23.569: INFO: Pod pod-with-poststart-exec-hook still exists
Feb  9 09:57:25.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb  9 09:57:25.572: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:57:25.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6068" for this suite.

• [SLOW TEST:70.372 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":92,"skipped":1500,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:57:25.584: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7505
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Feb  9 09:57:25.741: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:57:49.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7505" for this suite.

• [SLOW TEST:24.225 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":93,"skipped":1501,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:57:49.809: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-288bc951-4230-45b5-b4e4-deb9b49a91a1
STEP: Creating a pod to test consume configMaps
Feb  9 09:57:49.990: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2b9fb2b-53fe-4908-a7b9-c9e397051d85" in namespace "projected-2369" to be "Succeeded or Failed"
Feb  9 09:57:50.007: INFO: Pod "pod-projected-configmaps-f2b9fb2b-53fe-4908-a7b9-c9e397051d85": Phase="Pending", Reason="", readiness=false. Elapsed: 16.843356ms
Feb  9 09:57:52.023: INFO: Pod "pod-projected-configmaps-f2b9fb2b-53fe-4908-a7b9-c9e397051d85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032243012s
Feb  9 09:57:54.034: INFO: Pod "pod-projected-configmaps-f2b9fb2b-53fe-4908-a7b9-c9e397051d85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043176808s
STEP: Saw pod success
Feb  9 09:57:54.034: INFO: Pod "pod-projected-configmaps-f2b9fb2b-53fe-4908-a7b9-c9e397051d85" satisfied condition "Succeeded or Failed"
Feb  9 09:57:54.038: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-projected-configmaps-f2b9fb2b-53fe-4908-a7b9-c9e397051d85 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 09:57:54.128: INFO: Waiting for pod pod-projected-configmaps-f2b9fb2b-53fe-4908-a7b9-c9e397051d85 to disappear
Feb  9 09:57:54.132: INFO: Pod pod-projected-configmaps-f2b9fb2b-53fe-4908-a7b9-c9e397051d85 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:57:54.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2369" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":94,"skipped":1511,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:57:54.145: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-85f3d858-cce3-47e7-a716-2adb2b05d8ec
STEP: Creating a pod to test consume secrets
Feb  9 09:57:54.322: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-92852d9c-df1a-4005-baf2-495eb05ed353" in namespace "projected-447" to be "Succeeded or Failed"
Feb  9 09:57:54.327: INFO: Pod "pod-projected-secrets-92852d9c-df1a-4005-baf2-495eb05ed353": Phase="Pending", Reason="", readiness=false. Elapsed: 5.409971ms
Feb  9 09:57:56.341: INFO: Pod "pod-projected-secrets-92852d9c-df1a-4005-baf2-495eb05ed353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019063735s
Feb  9 09:57:58.347: INFO: Pod "pod-projected-secrets-92852d9c-df1a-4005-baf2-495eb05ed353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025081055s
STEP: Saw pod success
Feb  9 09:57:58.347: INFO: Pod "pod-projected-secrets-92852d9c-df1a-4005-baf2-495eb05ed353" satisfied condition "Succeeded or Failed"
Feb  9 09:57:58.351: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-projected-secrets-92852d9c-df1a-4005-baf2-495eb05ed353 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  9 09:57:58.378: INFO: Waiting for pod pod-projected-secrets-92852d9c-df1a-4005-baf2-495eb05ed353 to disappear
Feb  9 09:57:58.381: INFO: Pod pod-projected-secrets-92852d9c-df1a-4005-baf2-495eb05ed353 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:57:58.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-447" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1524,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:57:58.398: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6390
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Feb  9 09:57:58.552: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:58:20.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6390" for this suite.

• [SLOW TEST:22.073 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":96,"skipped":1524,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:58:20.476: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6646
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:58:36.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6646" for this suite.

• [SLOW TEST:16.373 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":97,"skipped":1531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:58:36.850: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-2466
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:58:37.003: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Creating first CR 
Feb  9 09:58:37.584: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-09T09:58:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-09T09:58:37Z]] name:name1 resourceVersion:22610 uid:ad156e83-f581-4b55-a1a0-cce00a4d72ac] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb  9 09:58:47.612: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-09T09:58:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-09T09:58:47Z]] name:name2 resourceVersion:22655 uid:8261cdbd-4468-4e32-849d-13817c236f5b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb  9 09:58:57.634: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-09T09:58:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-09T09:58:57Z]] name:name1 resourceVersion:22684 uid:ad156e83-f581-4b55-a1a0-cce00a4d72ac] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb  9 09:59:07.662: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-09T09:58:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-09T09:59:07Z]] name:name2 resourceVersion:22713 uid:8261cdbd-4468-4e32-849d-13817c236f5b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb  9 09:59:17.679: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-09T09:58:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-09T09:58:57Z]] name:name1 resourceVersion:22741 uid:ad156e83-f581-4b55-a1a0-cce00a4d72ac] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb  9 09:59:27.708: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-02-09T09:58:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-02-09T09:59:07Z]] name:name2 resourceVersion:22769 uid:8261cdbd-4468-4e32-849d-13817c236f5b] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:59:38.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2466" for this suite.

• [SLOW TEST:61.495 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":98,"skipped":1553,"failed":0}
SS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:59:38.345: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-7339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Feb  9 09:59:38.539: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Feb  9 09:59:38.551: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb  9 09:59:38.551: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Feb  9 09:59:38.569: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb  9 09:59:38.570: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Feb  9 09:59:38.591: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb  9 09:59:38.591: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Feb  9 09:59:45.668: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:59:45.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7339" for this suite.

• [SLOW TEST:7.349 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":99,"skipped":1555,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:59:45.700: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9424
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 09:59:46.458: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 09:59:48.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748461586, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748461586, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748461586, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748461586, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 09:59:51.503: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:59:51.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9424" for this suite.
STEP: Destroying namespace "webhook-9424-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.995 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":100,"skipped":1594,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:59:51.695: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Feb  9 09:59:51.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-1488 create -f -'
Feb  9 09:59:52.925: INFO: stderr: ""
Feb  9 09:59:52.925: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Feb  9 09:59:52.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-1488 diff -f -'
Feb  9 09:59:53.407: INFO: rc: 1
Feb  9 09:59:53.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-1488 delete -f -'
Feb  9 09:59:53.523: INFO: stderr: ""
Feb  9 09:59:53.523: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:59:53.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1488" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":101,"skipped":1603,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:59:53.539: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 09:59:53.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2790 create -f -'
Feb  9 09:59:53.979: INFO: stderr: ""
Feb  9 09:59:53.979: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb  9 09:59:53.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2790 create -f -'
Feb  9 09:59:54.277: INFO: stderr: ""
Feb  9 09:59:54.277: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb  9 09:59:55.288: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 09:59:55.288: INFO: Found 0 / 1
Feb  9 09:59:56.285: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 09:59:56.285: INFO: Found 1 / 1
Feb  9 09:59:56.285: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb  9 09:59:56.288: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 09:59:56.288: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  9 09:59:56.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2790 describe pod agnhost-primary-jwphq'
Feb  9 09:59:56.466: INFO: stderr: ""
Feb  9 09:59:56.466: INFO: stdout: "Name:         agnhost-primary-jwphq\nNamespace:    kubectl-2790\nPriority:     0\nNode:         v1-kube1-20-2-apco5j2qoq5i-node-0/10.0.0.248\nStart Time:   Tue, 09 Feb 2021 09:59:53 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.100.239.190/32\n              cni.projectcalico.org/podIPs: 10.100.239.190/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.100.239.190\nIPs:\n  IP:           10.100.239.190\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://7f29aedd8f0c833d66c38cca6f6b6f8d7059ad03bb5bd414c9a1844844eacff9\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 09 Feb 2021 09:59:55 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rs47d (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rs47d:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rs47d\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2790/agnhost-primary-jwphq to v1-kube1-20-2-apco5j2qoq5i-node-0\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Feb  9 09:59:56.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2790 describe rc agnhost-primary'
Feb  9 09:59:56.734: INFO: stderr: ""
Feb  9 09:59:56.734: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2790\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-jwphq\n"
Feb  9 09:59:56.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2790 describe service agnhost-primary'
Feb  9 09:59:56.889: INFO: stderr: ""
Feb  9 09:59:56.889: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2790\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.254.54.168\nIPs:               10.254.54.168\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.239.190:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb  9 09:59:56.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2790 describe node v1-kube1-20-2-apco5j2qoq5i-master-0'
Feb  9 09:59:57.103: INFO: stderr: ""
Feb  9 09:59:57.103: INFO: stdout: "Name:               v1-kube1-20-2-apco5j2qoq5i-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=007e1639-6d22-4a11-96f1-36ea7b5acec4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=Ioni\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=v1-kube1-20-2-apco5j2qoq5i-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=007e1639-6d22-4a11-96f1-36ea7b5acec4\n                    topology.kubernetes.io/region=Ioni\n                    topology.kubernetes.io/zone=nova\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.220/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 09 Feb 2021 08:40:21 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  v1-kube1-20-2-apco5j2qoq5i-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 09 Feb 2021 09:59:48 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 09 Feb 2021 08:41:20 +0000   Tue, 09 Feb 2021 08:41:20 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 09 Feb 2021 09:58:37 +0000   Tue, 09 Feb 2021 08:40:14 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 09 Feb 2021 09:58:37 +0000   Tue, 09 Feb 2021 08:40:14 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 09 Feb 2021 09:58:37 +0000   Tue, 09 Feb 2021 08:40:14 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 09 Feb 2021 09:58:37 +0000   Tue, 09 Feb 2021 08:41:01 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.220\n  ExternalIP:  10.6.0.165\n  Hostname:    v1-kube1-20-2-apco5j2qoq5i-master-0\nCapacity:\n  cpu:                4\n  ephemeral-storage:  20435948Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8141740Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  18833769646\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8039340Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 55b2732382ea40758dcfc7f5b6d7b54b\n  System UUID:                55b27323-82ea-4075-8dcf-c7f5b6d7b54b\n  Boot ID:                    99b5e836-2fd6-43fd-8fdb-e1285ed09491\n  Kernel Version:             5.10.12-200.fc33.x86_64\n  OS Image:                   Fedora CoreOS 33.20210117.3.2\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.13\n  Kubelet Version:            v1.20.2\n  Kube-Proxy Version:         v1.20.2\nPodCIDR:                      10.100.0.0/24\nPodCIDRs:                     10.100.0.0/24\nProviderID:                   openstack:///55b27323-82ea-4075-8dcf-c7f5b6d7b54b\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-6f66b5488b-djfxt                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 calico-node-gdvpq                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 coredns-57999d5467-5np8j                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     79m\n  kube-system                 coredns-57999d5467-wdb8r                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     79m\n  kube-system                 csi-cinder-controllerplugin-0                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 dashboard-metrics-scraper-7b59f7d4df-x7tl2                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 k8s-keystone-auth-bmbxd                                    200m (5%)     0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kubernetes-dashboard-7fb447bf79-zm2vf                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 magnum-auto-healer-wmsq7                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 openstack-cloud-controller-manager-hs2w2                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-lmzws    0 (0%)        0 (0%)      0 (0%)           0 (0%)         52m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (16%)  0 (0%)\n  memory             140Mi (1%)  340Mi (4%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Feb  9 09:59:57.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2790 describe namespace kubectl-2790'
Feb  9 09:59:57.254: INFO: stderr: ""
Feb  9 09:59:57.254: INFO: stdout: "Name:         kubectl-2790\nLabels:       e2e-framework=kubectl\n              e2e-run=7d186960-2624-4d69-aed6-1f0d1a2b9b5a\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:59:57.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2790" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":102,"skipped":1610,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:59:57.269: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9402
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:59:57.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9402" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":103,"skipped":1619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:59:57.473: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2093
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-a955577b-f899-4418-8777-542e960adb8c
STEP: Creating a pod to test consume secrets
Feb  9 09:59:57.665: INFO: Waiting up to 5m0s for pod "pod-secrets-ebaa41d5-3e65-43e8-adf2-327ff9336e81" in namespace "secrets-2093" to be "Succeeded or Failed"
Feb  9 09:59:57.669: INFO: Pod "pod-secrets-ebaa41d5-3e65-43e8-adf2-327ff9336e81": Phase="Pending", Reason="", readiness=false. Elapsed: 3.651229ms
Feb  9 09:59:59.677: INFO: Pod "pod-secrets-ebaa41d5-3e65-43e8-adf2-327ff9336e81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01219261s
STEP: Saw pod success
Feb  9 09:59:59.678: INFO: Pod "pod-secrets-ebaa41d5-3e65-43e8-adf2-327ff9336e81" satisfied condition "Succeeded or Failed"
Feb  9 09:59:59.682: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-secrets-ebaa41d5-3e65-43e8-adf2-327ff9336e81 container secret-volume-test: <nil>
STEP: delete the pod
Feb  9 09:59:59.924: INFO: Waiting for pod pod-secrets-ebaa41d5-3e65-43e8-adf2-327ff9336e81 to disappear
Feb  9 09:59:59.928: INFO: Pod pod-secrets-ebaa41d5-3e65-43e8-adf2-327ff9336e81 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 09:59:59.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2093" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":104,"skipped":1753,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 09:59:59.939: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1793.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1793.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1793.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1793.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  9 10:00:04.155: INFO: DNS probes using dns-test-3f23629f-0f7e-45a7-9301-11c2b9857cb7 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1793.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1793.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1793.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1793.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  9 10:00:06.231: INFO: File wheezy_udp@dns-test-service-3.dns-1793.svc.cluster.local from pod  dns-1793/dns-test-cd402e58-68c0-4702-88d0-22ba60898a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  9 10:00:06.235: INFO: File jessie_udp@dns-test-service-3.dns-1793.svc.cluster.local from pod  dns-1793/dns-test-cd402e58-68c0-4702-88d0-22ba60898a08 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb  9 10:00:06.235: INFO: Lookups using dns-1793/dns-test-cd402e58-68c0-4702-88d0-22ba60898a08 failed for: [wheezy_udp@dns-test-service-3.dns-1793.svc.cluster.local jessie_udp@dns-test-service-3.dns-1793.svc.cluster.local]

Feb  9 10:00:11.245: INFO: DNS probes using dns-test-cd402e58-68c0-4702-88d0-22ba60898a08 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1793.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1793.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1793.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1793.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  9 10:00:15.348: INFO: DNS probes using dns-test-56a02b89-774c-4880-88cf-2faa4c03867c succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:00:15.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1793" for this suite.

• [SLOW TEST:15.453 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":105,"skipped":1764,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:00:15.392: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5845
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:00:41.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5845" for this suite.

• [SLOW TEST:25.652 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":106,"skipped":1783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:00:41.045: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4870, will wait for the garbage collector to delete the pods
Feb  9 10:00:45.290: INFO: Deleting Job.batch foo took: 13.80707ms
Feb  9 10:00:46.291: INFO: Terminating Job.batch foo pods took: 1.000323673s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:02:03.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4870" for this suite.

• [SLOW TEST:82.685 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":107,"skipped":1808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:02:03.731: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:02:03.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-356" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":108,"skipped":1842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:02:03.921: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:02:04.687: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:02:07.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:02:07.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6696" for this suite.
STEP: Destroying namespace "webhook-6696-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":109,"skipped":1866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:02:07.939: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9751
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-g8fx
STEP: Creating a pod to test atomic-volume-subpath
Feb  9 10:02:08.266: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-g8fx" in namespace "subpath-9751" to be "Succeeded or Failed"
Feb  9 10:02:08.272: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.476448ms
Feb  9 10:02:10.286: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 2.019929135s
Feb  9 10:02:12.295: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 4.029483314s
Feb  9 10:02:14.310: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 6.043750643s
Feb  9 10:02:16.324: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 8.057718053s
Feb  9 10:02:18.337: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 10.070723855s
Feb  9 10:02:20.349: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 12.082729318s
Feb  9 10:02:22.363: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 14.097329171s
Feb  9 10:02:24.377: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 16.111104255s
Feb  9 10:02:26.392: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 18.125874268s
Feb  9 10:02:28.402: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 20.135995011s
Feb  9 10:02:30.414: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Running", Reason="", readiness=true. Elapsed: 22.148129271s
Feb  9 10:02:32.427: INFO: Pod "pod-subpath-test-configmap-g8fx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.16150793s
STEP: Saw pod success
Feb  9 10:02:32.428: INFO: Pod "pod-subpath-test-configmap-g8fx" satisfied condition "Succeeded or Failed"
Feb  9 10:02:32.431: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-2 pod pod-subpath-test-configmap-g8fx container test-container-subpath-configmap-g8fx: <nil>
STEP: delete the pod
Feb  9 10:02:32.514: INFO: Waiting for pod pod-subpath-test-configmap-g8fx to disappear
Feb  9 10:02:32.519: INFO: Pod pod-subpath-test-configmap-g8fx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-g8fx
Feb  9 10:02:32.519: INFO: Deleting pod "pod-subpath-test-configmap-g8fx" in namespace "subpath-9751"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:02:32.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9751" for this suite.

• [SLOW TEST:24.593 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":110,"skipped":1904,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:02:32.533: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8720
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  9 10:02:32.688: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb  9 10:02:32.743: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  9 10:02:34.757: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:02:36.756: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:02:38.752: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:02:40.756: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:02:42.766: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:02:44.748: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:02:46.757: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:02:48.758: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:02:50.753: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb  9 10:02:50.764: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb  9 10:02:50.770: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb  9 10:02:54.827: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb  9 10:02:54.827: INFO: Going to poll 10.100.239.132 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb  9 10:02:54.830: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.239.132 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8720 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 10:02:54.830: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 10:02:56.081: INFO: Found all 1 expected endpoints: [netserver-0]
Feb  9 10:02:56.082: INFO: Going to poll 10.100.46.155 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb  9 10:02:56.094: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.46.155 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8720 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 10:02:56.094: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 10:02:57.281: INFO: Found all 1 expected endpoints: [netserver-1]
Feb  9 10:02:57.281: INFO: Going to poll 10.100.80.37 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb  9 10:02:57.293: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.80.37 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8720 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 10:02:57.293: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 10:02:58.487: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:02:58.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8720" for this suite.

• [SLOW TEST:25.974 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":111,"skipped":1912,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:02:58.507: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb  9 10:02:58.673: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3922  6d4d887c-d228-4073-96c1-1e5a20c070c9 24183 0 2021-02-09 10:02:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-09 10:02:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 10:02:58.674: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3922  6d4d887c-d228-4073-96c1-1e5a20c070c9 24183 0 2021-02-09 10:02:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-09 10:02:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb  9 10:03:08.694: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3922  6d4d887c-d228-4073-96c1-1e5a20c070c9 24262 0 2021-02-09 10:02:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-09 10:03:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 10:03:08.694: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3922  6d4d887c-d228-4073-96c1-1e5a20c070c9 24262 0 2021-02-09 10:02:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-09 10:03:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb  9 10:03:18.715: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3922  6d4d887c-d228-4073-96c1-1e5a20c070c9 24290 0 2021-02-09 10:02:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-09 10:03:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 10:03:18.716: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3922  6d4d887c-d228-4073-96c1-1e5a20c070c9 24290 0 2021-02-09 10:02:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-09 10:03:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb  9 10:03:28.758: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3922  6d4d887c-d228-4073-96c1-1e5a20c070c9 24317 0 2021-02-09 10:02:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-09 10:03:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 10:03:28.758: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3922  6d4d887c-d228-4073-96c1-1e5a20c070c9 24317 0 2021-02-09 10:02:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-02-09 10:03:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb  9 10:03:38.781: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3922  e905722f-d2a1-4a8f-aefe-49c53d4269c2 24345 0 2021-02-09 10:03:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-09 10:03:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 10:03:38.781: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3922  e905722f-d2a1-4a8f-aefe-49c53d4269c2 24345 0 2021-02-09 10:03:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-09 10:03:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb  9 10:03:48.794: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3922  e905722f-d2a1-4a8f-aefe-49c53d4269c2 24374 0 2021-02-09 10:03:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-09 10:03:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 10:03:48.794: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3922  e905722f-d2a1-4a8f-aefe-49c53d4269c2 24374 0 2021-02-09 10:03:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-02-09 10:03:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:03:58.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3922" for this suite.

• [SLOW TEST:60.332 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":112,"skipped":1913,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:03:58.840: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb  9 10:04:03.562: INFO: Successfully updated pod "adopt-release-7s8wx"
STEP: Checking that the Job readopts the Pod
Feb  9 10:04:03.562: INFO: Waiting up to 15m0s for pod "adopt-release-7s8wx" in namespace "job-357" to be "adopted"
Feb  9 10:04:03.568: INFO: Pod "adopt-release-7s8wx": Phase="Running", Reason="", readiness=true. Elapsed: 6.545548ms
Feb  9 10:04:05.580: INFO: Pod "adopt-release-7s8wx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018408613s
Feb  9 10:04:05.580: INFO: Pod "adopt-release-7s8wx" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb  9 10:04:06.106: INFO: Successfully updated pod "adopt-release-7s8wx"
STEP: Checking that the Job releases the Pod
Feb  9 10:04:06.106: INFO: Waiting up to 15m0s for pod "adopt-release-7s8wx" in namespace "job-357" to be "released"
Feb  9 10:04:06.113: INFO: Pod "adopt-release-7s8wx": Phase="Running", Reason="", readiness=true. Elapsed: 7.611606ms
Feb  9 10:04:08.122: INFO: Pod "adopt-release-7s8wx": Phase="Running", Reason="", readiness=true. Elapsed: 2.015911249s
Feb  9 10:04:08.122: INFO: Pod "adopt-release-7s8wx" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:04:08.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-357" for this suite.

• [SLOW TEST:9.295 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":113,"skipped":1935,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:04:08.137: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1954
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 10:04:08.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8595f653-b862-45dd-b72e-96ba4cdce5d0" in namespace "downward-api-1954" to be "Succeeded or Failed"
Feb  9 10:04:08.314: INFO: Pod "downwardapi-volume-8595f653-b862-45dd-b72e-96ba4cdce5d0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.418388ms
Feb  9 10:04:10.324: INFO: Pod "downwardapi-volume-8595f653-b862-45dd-b72e-96ba4cdce5d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016510192s
Feb  9 10:04:12.334: INFO: Pod "downwardapi-volume-8595f653-b862-45dd-b72e-96ba4cdce5d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026650445s
STEP: Saw pod success
Feb  9 10:04:12.334: INFO: Pod "downwardapi-volume-8595f653-b862-45dd-b72e-96ba4cdce5d0" satisfied condition "Succeeded or Failed"
Feb  9 10:04:12.337: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-2 pod downwardapi-volume-8595f653-b862-45dd-b72e-96ba4cdce5d0 container client-container: <nil>
STEP: delete the pod
Feb  9 10:04:12.412: INFO: Waiting for pod downwardapi-volume-8595f653-b862-45dd-b72e-96ba4cdce5d0 to disappear
Feb  9 10:04:12.416: INFO: Pod downwardapi-volume-8595f653-b862-45dd-b72e-96ba4cdce5d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:04:12.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1954" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":1948,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:04:12.430: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:04:20.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5251" for this suite.

• [SLOW TEST:8.444 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":115,"skipped":1989,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:04:20.875: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:04:24.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3771" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":116,"skipped":1998,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:04:24.095: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb  9 10:04:24.504: INFO: Pod name wrapped-volume-race-3de1935d-0bc9-4642-bedf-8c599a98661e: Found 0 pods out of 5
Feb  9 10:04:29.536: INFO: Pod name wrapped-volume-race-3de1935d-0bc9-4642-bedf-8c599a98661e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3de1935d-0bc9-4642-bedf-8c599a98661e in namespace emptydir-wrapper-5635, will wait for the garbage collector to delete the pods
Feb  9 10:04:49.666: INFO: Deleting ReplicationController wrapped-volume-race-3de1935d-0bc9-4642-bedf-8c599a98661e took: 11.986906ms
Feb  9 10:04:49.766: INFO: Terminating ReplicationController wrapped-volume-race-3de1935d-0bc9-4642-bedf-8c599a98661e pods took: 100.16781ms
STEP: Creating RC which spawns configmap-volume pods
Feb  9 10:05:50.516: INFO: Pod name wrapped-volume-race-f54bf4a2-1734-4521-9f5e-7dfe0dfa0f62: Found 0 pods out of 5
Feb  9 10:05:55.534: INFO: Pod name wrapped-volume-race-f54bf4a2-1734-4521-9f5e-7dfe0dfa0f62: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f54bf4a2-1734-4521-9f5e-7dfe0dfa0f62 in namespace emptydir-wrapper-5635, will wait for the garbage collector to delete the pods
Feb  9 10:06:07.634: INFO: Deleting ReplicationController wrapped-volume-race-f54bf4a2-1734-4521-9f5e-7dfe0dfa0f62 took: 9.749982ms
Feb  9 10:06:08.635: INFO: Terminating ReplicationController wrapped-volume-race-f54bf4a2-1734-4521-9f5e-7dfe0dfa0f62 pods took: 1.000780435s
STEP: Creating RC which spawns configmap-volume pods
Feb  9 10:07:03.774: INFO: Pod name wrapped-volume-race-4a04a0cb-0186-4dc2-92ac-bed0e55dbdee: Found 0 pods out of 5
Feb  9 10:07:08.792: INFO: Pod name wrapped-volume-race-4a04a0cb-0186-4dc2-92ac-bed0e55dbdee: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4a04a0cb-0186-4dc2-92ac-bed0e55dbdee in namespace emptydir-wrapper-5635, will wait for the garbage collector to delete the pods
Feb  9 10:07:20.922: INFO: Deleting ReplicationController wrapped-volume-race-4a04a0cb-0186-4dc2-92ac-bed0e55dbdee took: 15.28631ms
Feb  9 10:07:21.923: INFO: Terminating ReplicationController wrapped-volume-race-4a04a0cb-0186-4dc2-92ac-bed0e55dbdee pods took: 1.000423207s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:08:04.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5635" for this suite.

• [SLOW TEST:219.980 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":117,"skipped":1999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:08:04.080: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb  9 10:08:04.294: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 10:08:04.299: INFO: Number of nodes with available pods: 0
Feb  9 10:08:04.299: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 10:08:05.307: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 10:08:05.311: INFO: Number of nodes with available pods: 0
Feb  9 10:08:05.311: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 10:08:06.312: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 10:08:06.317: INFO: Number of nodes with available pods: 1
Feb  9 10:08:06.317: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 10:08:07.312: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 10:08:07.316: INFO: Number of nodes with available pods: 3
Feb  9 10:08:07.316: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb  9 10:08:07.342: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 10:08:07.347: INFO: Number of nodes with available pods: 3
Feb  9 10:08:07.347: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5696, will wait for the garbage collector to delete the pods
Feb  9 10:08:08.431: INFO: Deleting DaemonSet.extensions daemon-set took: 6.708175ms
Feb  9 10:08:09.431: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000290609s
Feb  9 10:08:50.240: INFO: Number of nodes with available pods: 0
Feb  9 10:08:50.241: INFO: Number of running nodes: 0, number of available pods: 0
Feb  9 10:08:50.246: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26672"},"items":null}

Feb  9 10:08:50.250: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26672"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:08:50.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5696" for this suite.

• [SLOW TEST:46.200 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":118,"skipped":2030,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:08:50.287: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Feb  9 10:08:50.489: INFO: Waiting up to 5m0s for pod "client-containers-b7804d96-6768-404a-9746-331f72ea1169" in namespace "containers-609" to be "Succeeded or Failed"
Feb  9 10:08:50.500: INFO: Pod "client-containers-b7804d96-6768-404a-9746-331f72ea1169": Phase="Pending", Reason="", readiness=false. Elapsed: 9.87694ms
Feb  9 10:08:52.507: INFO: Pod "client-containers-b7804d96-6768-404a-9746-331f72ea1169": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017107716s
STEP: Saw pod success
Feb  9 10:08:52.507: INFO: Pod "client-containers-b7804d96-6768-404a-9746-331f72ea1169" satisfied condition "Succeeded or Failed"
Feb  9 10:08:52.511: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod client-containers-b7804d96-6768-404a-9746-331f72ea1169 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 10:08:52.598: INFO: Waiting for pod client-containers-b7804d96-6768-404a-9746-331f72ea1169 to disappear
Feb  9 10:08:52.603: INFO: Pod client-containers-b7804d96-6768-404a-9746-331f72ea1169 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:08:52.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-609" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":2039,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:08:52.614: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3253
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:08:52.772: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:08:54.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3253" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":2051,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:08:55.004: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:08:57.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2805" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":121,"skipped":2073,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:08:57.313: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 10:08:57.529: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbfac2ae-0413-4cc1-8963-bb8a0662f8dd" in namespace "projected-1400" to be "Succeeded or Failed"
Feb  9 10:08:57.545: INFO: Pod "downwardapi-volume-cbfac2ae-0413-4cc1-8963-bb8a0662f8dd": Phase="Pending", Reason="", readiness=false. Elapsed: 15.409348ms
Feb  9 10:08:59.563: INFO: Pod "downwardapi-volume-cbfac2ae-0413-4cc1-8963-bb8a0662f8dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033611949s
Feb  9 10:09:01.577: INFO: Pod "downwardapi-volume-cbfac2ae-0413-4cc1-8963-bb8a0662f8dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047586857s
STEP: Saw pod success
Feb  9 10:09:01.577: INFO: Pod "downwardapi-volume-cbfac2ae-0413-4cc1-8963-bb8a0662f8dd" satisfied condition "Succeeded or Failed"
Feb  9 10:09:01.580: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downwardapi-volume-cbfac2ae-0413-4cc1-8963-bb8a0662f8dd container client-container: <nil>
STEP: delete the pod
Feb  9 10:09:01.603: INFO: Waiting for pod downwardapi-volume-cbfac2ae-0413-4cc1-8963-bb8a0662f8dd to disappear
Feb  9 10:09:01.611: INFO: Pod downwardapi-volume-cbfac2ae-0413-4cc1-8963-bb8a0662f8dd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:09:01.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1400" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2084,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:09:01.632: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:09:05.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6327" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":123,"skipped":2093,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:09:05.833: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6427
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Feb  9 10:09:06.048: INFO: Waiting up to 5m0s for pod "test-pod-4e3423fa-7c2a-4dfe-8efa-9b73179062d6" in namespace "svcaccounts-6427" to be "Succeeded or Failed"
Feb  9 10:09:06.054: INFO: Pod "test-pod-4e3423fa-7c2a-4dfe-8efa-9b73179062d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.978033ms
Feb  9 10:09:08.066: INFO: Pod "test-pod-4e3423fa-7c2a-4dfe-8efa-9b73179062d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017826367s
Feb  9 10:09:10.081: INFO: Pod "test-pod-4e3423fa-7c2a-4dfe-8efa-9b73179062d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033435598s
STEP: Saw pod success
Feb  9 10:09:10.081: INFO: Pod "test-pod-4e3423fa-7c2a-4dfe-8efa-9b73179062d6" satisfied condition "Succeeded or Failed"
Feb  9 10:09:10.085: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod test-pod-4e3423fa-7c2a-4dfe-8efa-9b73179062d6 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 10:09:10.115: INFO: Waiting for pod test-pod-4e3423fa-7c2a-4dfe-8efa-9b73179062d6 to disappear
Feb  9 10:09:10.119: INFO: Pod test-pod-4e3423fa-7c2a-4dfe-8efa-9b73179062d6 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:09:10.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6427" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":124,"skipped":2095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:09:10.130: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 10:09:10.298: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8b64ebd6-3e39-4965-be25-946f4684871a" in namespace "projected-8524" to be "Succeeded or Failed"
Feb  9 10:09:10.306: INFO: Pod "downwardapi-volume-8b64ebd6-3e39-4965-be25-946f4684871a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106949ms
Feb  9 10:09:12.319: INFO: Pod "downwardapi-volume-8b64ebd6-3e39-4965-be25-946f4684871a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021298766s
STEP: Saw pod success
Feb  9 10:09:12.319: INFO: Pod "downwardapi-volume-8b64ebd6-3e39-4965-be25-946f4684871a" satisfied condition "Succeeded or Failed"
Feb  9 10:09:12.322: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod downwardapi-volume-8b64ebd6-3e39-4965-be25-946f4684871a container client-container: <nil>
STEP: delete the pod
Feb  9 10:09:12.352: INFO: Waiting for pod downwardapi-volume-8b64ebd6-3e39-4965-be25-946f4684871a to disappear
Feb  9 10:09:12.357: INFO: Pod downwardapi-volume-8b64ebd6-3e39-4965-be25-946f4684871a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:09:12.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8524" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":125,"skipped":2121,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:09:12.367: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2970
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb  9 10:09:12.594: INFO: Waiting up to 5m0s for pod "pod-75439036-742d-4996-8543-9de5ae7b592f" in namespace "emptydir-2970" to be "Succeeded or Failed"
Feb  9 10:09:12.608: INFO: Pod "pod-75439036-742d-4996-8543-9de5ae7b592f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.097042ms
Feb  9 10:09:14.623: INFO: Pod "pod-75439036-742d-4996-8543-9de5ae7b592f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028515786s
STEP: Saw pod success
Feb  9 10:09:14.623: INFO: Pod "pod-75439036-742d-4996-8543-9de5ae7b592f" satisfied condition "Succeeded or Failed"
Feb  9 10:09:14.626: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-2 pod pod-75439036-742d-4996-8543-9de5ae7b592f container test-container: <nil>
STEP: delete the pod
Feb  9 10:09:14.711: INFO: Waiting for pod pod-75439036-742d-4996-8543-9de5ae7b592f to disappear
Feb  9 10:09:14.848: INFO: Pod pod-75439036-742d-4996-8543-9de5ae7b592f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:09:14.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2970" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":126,"skipped":2121,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:09:14.865: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6925
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb  9 10:09:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb  9 10:09:31.227: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 10:09:35.184: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:09:52.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6925" for this suite.

• [SLOW TEST:37.383 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":127,"skipped":2137,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:09:52.250: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7910
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7910.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7910.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7910.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7910.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7910.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7910.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7910.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7910.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7910.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7910.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7910.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 223.154.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.154.223_udp@PTR;check="$$(dig +tcp +noall +answer +search 223.154.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.154.223_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7910.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7910.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7910.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7910.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7910.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7910.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7910.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7910.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7910.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7910.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7910.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 223.154.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.154.223_udp@PTR;check="$$(dig +tcp +noall +answer +search 223.154.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.154.223_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  9 10:09:56.508: INFO: Unable to read wheezy_udp@dns-test-service.dns-7910.svc.cluster.local from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.513: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7910.svc.cluster.local from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.518: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.522: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.533: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.537: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.550: INFO: Unable to read jessie_udp@dns-test-service.dns-7910.svc.cluster.local from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.554: INFO: Unable to read jessie_tcp@dns-test-service.dns-7910.svc.cluster.local from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.557: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.561: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.572: INFO: Unable to read jessie_udp@PodARecord from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.576: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862: the server could not find the requested resource (get pods dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862)
Feb  9 10:09:56.583: INFO: Lookups using dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862 failed for: [wheezy_udp@dns-test-service.dns-7910.svc.cluster.local wheezy_tcp@dns-test-service.dns-7910.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-7910.svc.cluster.local jessie_tcp@dns-test-service.dns-7910.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7910.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb  9 10:10:01.656: INFO: DNS probes using dns-7910/dns-test-8708a592-9f69-4e84-9368-fbd9bdc5a862 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:10:01.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7910" for this suite.

• [SLOW TEST:9.655 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":128,"skipped":2146,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:10:01.908: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:10:02.491: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 10:10:04.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462202, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462202, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462202, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462202, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:10:07.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:10:19.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6435" for this suite.
STEP: Destroying namespace "webhook-6435-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.903 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":129,"skipped":2152,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:10:19.811: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5200
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:10:24.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5200" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":130,"skipped":2156,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:10:24.098: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Feb  9 10:10:28.836: INFO: Successfully updated pod "labelsupdate46ad6d4e-aedd-410c-a4a5-01280539189e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:10:30.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8747" for this suite.

• [SLOW TEST:6.790 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:10:30.888: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:10:31.374: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 10:10:33.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462231, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462231, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462231, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462231, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:10:36.410: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:10:36.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8060" for this suite.
STEP: Destroying namespace "webhook-8060-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":132,"skipped":2186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:10:36.571: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Feb  9 10:10:36.859: INFO: Waiting up to 5m0s for pod "downward-api-88df4ac0-8b9c-43fa-82e4-d0f405a45b8c" in namespace "downward-api-9523" to be "Succeeded or Failed"
Feb  9 10:10:36.869: INFO: Pod "downward-api-88df4ac0-8b9c-43fa-82e4-d0f405a45b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.328504ms
Feb  9 10:10:38.881: INFO: Pod "downward-api-88df4ac0-8b9c-43fa-82e4-d0f405a45b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022201967s
Feb  9 10:10:40.897: INFO: Pod "downward-api-88df4ac0-8b9c-43fa-82e4-d0f405a45b8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038780896s
STEP: Saw pod success
Feb  9 10:10:40.897: INFO: Pod "downward-api-88df4ac0-8b9c-43fa-82e4-d0f405a45b8c" satisfied condition "Succeeded or Failed"
Feb  9 10:10:40.903: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-2 pod downward-api-88df4ac0-8b9c-43fa-82e4-d0f405a45b8c container dapi-container: <nil>
STEP: delete the pod
Feb  9 10:10:40.930: INFO: Waiting for pod downward-api-88df4ac0-8b9c-43fa-82e4-d0f405a45b8c to disappear
Feb  9 10:10:40.939: INFO: Pod downward-api-88df4ac0-8b9c-43fa-82e4-d0f405a45b8c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:10:40.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9523" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":133,"skipped":2265,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:10:40.951: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:10:41.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3176" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2266,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:10:41.157: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb  9 10:10:41.317: INFO: Waiting up to 5m0s for pod "pod-577a7123-03fb-4728-85b1-7f485caa31d1" in namespace "emptydir-7220" to be "Succeeded or Failed"
Feb  9 10:10:41.321: INFO: Pod "pod-577a7123-03fb-4728-85b1-7f485caa31d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.793818ms
Feb  9 10:10:43.329: INFO: Pod "pod-577a7123-03fb-4728-85b1-7f485caa31d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011506736s
Feb  9 10:10:45.343: INFO: Pod "pod-577a7123-03fb-4728-85b1-7f485caa31d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025267087s
STEP: Saw pod success
Feb  9 10:10:45.343: INFO: Pod "pod-577a7123-03fb-4728-85b1-7f485caa31d1" satisfied condition "Succeeded or Failed"
Feb  9 10:10:45.346: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-577a7123-03fb-4728-85b1-7f485caa31d1 container test-container: <nil>
STEP: delete the pod
Feb  9 10:10:45.369: INFO: Waiting for pod pod-577a7123-03fb-4728-85b1-7f485caa31d1 to disappear
Feb  9 10:10:45.375: INFO: Pod pod-577a7123-03fb-4728-85b1-7f485caa31d1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:10:45.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7220" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":135,"skipped":2276,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:10:45.386: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:10:45.559: INFO: Waiting up to 5m0s for pod "busybox-user-65534-f45e48bc-5767-4623-ada2-d6f1151c27cd" in namespace "security-context-test-5117" to be "Succeeded or Failed"
Feb  9 10:10:45.564: INFO: Pod "busybox-user-65534-f45e48bc-5767-4623-ada2-d6f1151c27cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.875606ms
Feb  9 10:10:47.579: INFO: Pod "busybox-user-65534-f45e48bc-5767-4623-ada2-d6f1151c27cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019763363s
Feb  9 10:10:49.595: INFO: Pod "busybox-user-65534-f45e48bc-5767-4623-ada2-d6f1151c27cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03573893s
Feb  9 10:10:49.595: INFO: Pod "busybox-user-65534-f45e48bc-5767-4623-ada2-d6f1151c27cd" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:10:49.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5117" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:10:49.609: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Feb  9 10:10:49.777: INFO: created test-pod-1
Feb  9 10:10:49.795: INFO: created test-pod-2
Feb  9 10:10:49.805: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:10:49.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2339" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":137,"skipped":2312,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:10:49.865: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-j7nm
STEP: Creating a pod to test atomic-volume-subpath
Feb  9 10:10:50.054: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j7nm" in namespace "subpath-1008" to be "Succeeded or Failed"
Feb  9 10:10:50.061: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.439227ms
Feb  9 10:10:52.079: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 2.024506621s
Feb  9 10:10:54.090: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 4.035574022s
Feb  9 10:10:56.105: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 6.051314661s
Feb  9 10:10:58.116: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 8.061558301s
Feb  9 10:11:00.128: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 10.074126096s
Feb  9 10:11:02.140: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 12.085638593s
Feb  9 10:11:04.151: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 14.097039491s
Feb  9 10:11:06.167: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 16.113299684s
Feb  9 10:11:08.181: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 18.12671657s
Feb  9 10:11:10.193: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 20.138759101s
Feb  9 10:11:12.206: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Running", Reason="", readiness=true. Elapsed: 22.152087839s
Feb  9 10:11:14.213: INFO: Pod "pod-subpath-test-configmap-j7nm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.158729853s
STEP: Saw pod success
Feb  9 10:11:14.213: INFO: Pod "pod-subpath-test-configmap-j7nm" satisfied condition "Succeeded or Failed"
Feb  9 10:11:14.217: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-subpath-test-configmap-j7nm container test-container-subpath-configmap-j7nm: <nil>
STEP: delete the pod
Feb  9 10:11:14.245: INFO: Waiting for pod pod-subpath-test-configmap-j7nm to disappear
Feb  9 10:11:14.252: INFO: Pod pod-subpath-test-configmap-j7nm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j7nm
Feb  9 10:11:14.252: INFO: Deleting pod "pod-subpath-test-configmap-j7nm" in namespace "subpath-1008"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:11:14.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1008" for this suite.

• [SLOW TEST:24.402 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":138,"skipped":2319,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:11:14.268: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-7438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Feb  9 10:11:14.420: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  9 10:12:14.480: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:12:14.486: INFO: Starting informer...
STEP: Starting pods...
Feb  9 10:12:14.716: INFO: Pod1 is running on v1-kube1-20-2-apco5j2qoq5i-node-1. Tainting Node
Feb  9 10:12:16.963: INFO: Pod2 is running on v1-kube1-20-2-apco5j2qoq5i-node-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb  9 10:13:03.601: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb  9 10:13:03.617: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:13:03.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7438" for this suite.

• [SLOW TEST:109.408 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":139,"skipped":2328,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:13:03.676: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8524
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb  9 10:13:05.897: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:13:05.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8524" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":140,"skipped":2337,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:13:05.921: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2879
Feb  9 10:13:08.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-2879 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb  9 10:13:08.814: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb  9 10:13:08.814: INFO: stdout: "iptables"
Feb  9 10:13:08.814: INFO: proxyMode: iptables
Feb  9 10:13:08.826: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb  9 10:13:08.833: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-2879
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2879
I0209 10:13:08.856799      25 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2879, replica count: 3
I0209 10:13:11.907888      25 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 10:13:11.923: INFO: Creating new exec pod
Feb  9 10:13:14.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-2879 exec execpod-affinitypx5qb -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Feb  9 10:13:15.334: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb  9 10:13:15.335: INFO: stdout: ""
Feb  9 10:13:15.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-2879 exec execpod-affinitypx5qb -- /bin/sh -x -c nc -zv -t -w 2 10.254.101.48 80'
Feb  9 10:13:15.645: INFO: stderr: "+ nc -zv -t -w 2 10.254.101.48 80\nConnection to 10.254.101.48 80 port [tcp/http] succeeded!\n"
Feb  9 10:13:15.645: INFO: stdout: ""
Feb  9 10:13:15.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-2879 exec execpod-affinitypx5qb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.101.48:80/ ; done'
Feb  9 10:13:16.056: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n"
Feb  9 10:13:16.056: INFO: stdout: "\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln\naffinity-clusterip-timeout-ph7ln"
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Received response from host: affinity-clusterip-timeout-ph7ln
Feb  9 10:13:16.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-2879 exec execpod-affinitypx5qb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.101.48:80/'
Feb  9 10:13:16.398: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n"
Feb  9 10:13:16.398: INFO: stdout: "affinity-clusterip-timeout-ph7ln"
Feb  9 10:13:36.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-2879 exec execpod-affinitypx5qb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.101.48:80/'
Feb  9 10:13:36.717: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.101.48:80/\n"
Feb  9 10:13:36.717: INFO: stdout: "affinity-clusterip-timeout-cvzn4"
Feb  9 10:13:36.718: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2879, will wait for the garbage collector to delete the pods
Feb  9 10:13:36.812: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 9.168458ms
Feb  9 10:13:37.812: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 1.00020867s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:14:13.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2879" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:67.852 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":141,"skipped":2350,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:14:13.773: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-19fbedd0-1a6a-462b-a7f2-5014656a0b09
STEP: Creating a pod to test consume secrets
Feb  9 10:14:14.214: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7505e450-8aa6-4dce-a870-74333ca23358" in namespace "projected-6588" to be "Succeeded or Failed"
Feb  9 10:14:14.227: INFO: Pod "pod-projected-secrets-7505e450-8aa6-4dce-a870-74333ca23358": Phase="Pending", Reason="", readiness=false. Elapsed: 12.628008ms
Feb  9 10:14:16.247: INFO: Pod "pod-projected-secrets-7505e450-8aa6-4dce-a870-74333ca23358": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032771368s
Feb  9 10:14:18.261: INFO: Pod "pod-projected-secrets-7505e450-8aa6-4dce-a870-74333ca23358": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04730928s
STEP: Saw pod success
Feb  9 10:14:18.261: INFO: Pod "pod-projected-secrets-7505e450-8aa6-4dce-a870-74333ca23358" satisfied condition "Succeeded or Failed"
Feb  9 10:14:18.264: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-projected-secrets-7505e450-8aa6-4dce-a870-74333ca23358 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  9 10:14:18.355: INFO: Waiting for pod pod-projected-secrets-7505e450-8aa6-4dce-a870-74333ca23358 to disappear
Feb  9 10:14:18.362: INFO: Pod pod-projected-secrets-7505e450-8aa6-4dce-a870-74333ca23358 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:14:18.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6588" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":142,"skipped":2361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:14:18.386: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1905
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:14:18.540: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb  9 10:14:22.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-1905 --namespace=crd-publish-openapi-1905 create -f -'
Feb  9 10:14:23.267: INFO: stderr: ""
Feb  9 10:14:23.267: INFO: stdout: "e2e-test-crd-publish-openapi-2295-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb  9 10:14:23.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-1905 --namespace=crd-publish-openapi-1905 delete e2e-test-crd-publish-openapi-2295-crds test-cr'
Feb  9 10:14:23.418: INFO: stderr: ""
Feb  9 10:14:23.418: INFO: stdout: "e2e-test-crd-publish-openapi-2295-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb  9 10:14:23.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-1905 --namespace=crd-publish-openapi-1905 apply -f -'
Feb  9 10:14:23.707: INFO: stderr: ""
Feb  9 10:14:23.707: INFO: stdout: "e2e-test-crd-publish-openapi-2295-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb  9 10:14:23.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-1905 --namespace=crd-publish-openapi-1905 delete e2e-test-crd-publish-openapi-2295-crds test-cr'
Feb  9 10:14:23.825: INFO: stderr: ""
Feb  9 10:14:23.825: INFO: stdout: "e2e-test-crd-publish-openapi-2295-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb  9 10:14:23.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-1905 explain e2e-test-crd-publish-openapi-2295-crds'
Feb  9 10:14:24.113: INFO: stderr: ""
Feb  9 10:14:24.113: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2295-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:14:28.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1905" for this suite.

• [SLOW TEST:9.723 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":143,"skipped":2441,"failed":0}
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:14:28.110: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4042
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-4042
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4042 to expose endpoints map[]
Feb  9 10:14:28.324: INFO: successfully validated that service multi-endpoint-test in namespace services-4042 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4042
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4042 to expose endpoints map[pod1:[100]]
Feb  9 10:14:31.388: INFO: successfully validated that service multi-endpoint-test in namespace services-4042 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4042
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4042 to expose endpoints map[pod1:[100] pod2:[101]]
Feb  9 10:14:33.428: INFO: successfully validated that service multi-endpoint-test in namespace services-4042 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-4042
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4042 to expose endpoints map[pod2:[101]]
Feb  9 10:14:33.465: INFO: successfully validated that service multi-endpoint-test in namespace services-4042 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4042
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4042 to expose endpoints map[]
Feb  9 10:14:33.615: INFO: successfully validated that service multi-endpoint-test in namespace services-4042 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:14:33.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4042" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.560 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":144,"skipped":2443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:14:33.672: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:14:33.891: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:14:37.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6873" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2465,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:14:37.995: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5167
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-5167
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5167
Feb  9 10:14:38.175: INFO: Found 0 stateful pods, waiting for 1
Feb  9 10:14:48.196: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb  9 10:14:48.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  9 10:14:48.508: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  9 10:14:48.508: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  9 10:14:48.508: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  9 10:14:48.514: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb  9 10:14:58.538: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  9 10:14:58.538: INFO: Waiting for statefulset status.replicas updated to 0
Feb  9 10:14:58.568: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:14:58.568: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:14:58.568: INFO: 
Feb  9 10:14:58.568: INFO: StatefulSet ss has not reached scale 3, at 1
Feb  9 10:14:59.581: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99626616s
Feb  9 10:15:00.590: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983913984s
Feb  9 10:15:01.597: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97419891s
Feb  9 10:15:02.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.967349473s
Feb  9 10:15:03.619: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957472811s
Feb  9 10:15:04.627: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.945376493s
Feb  9 10:15:05.638: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.93810312s
Feb  9 10:15:06.646: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.926665196s
Feb  9 10:15:07.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 918.924509ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5167
Feb  9 10:15:08.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:15:08.983: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb  9 10:15:08.983: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  9 10:15:08.983: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  9 10:15:08.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:15:09.310: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb  9 10:15:09.310: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  9 10:15:09.310: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  9 10:15:09.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:15:09.641: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb  9 10:15:09.641: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb  9 10:15:09.641: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb  9 10:15:09.649: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb  9 10:15:19.669: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 10:15:19.669: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 10:15:19.669: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb  9 10:15:19.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  9 10:15:19.975: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  9 10:15:19.975: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  9 10:15:19.975: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  9 10:15:19.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  9 10:15:20.304: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  9 10:15:20.304: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  9 10:15:20.304: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  9 10:15:20.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb  9 10:15:20.612: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb  9 10:15:20.612: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb  9 10:15:20.612: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb  9 10:15:20.612: INFO: Waiting for statefulset status.replicas updated to 0
Feb  9 10:15:20.621: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb  9 10:15:30.640: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb  9 10:15:30.640: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb  9 10:15:30.640: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb  9 10:15:30.655: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:15:30.655: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:15:30.655: INFO: ss-1  v1-kube1-20-2-apco5j2qoq5i-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:30.655: INFO: ss-2  v1-kube1-20-2-apco5j2qoq5i-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:30.655: INFO: 
Feb  9 10:15:30.655: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  9 10:15:31.732: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:15:31.732: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:15:31.732: INFO: ss-1  v1-kube1-20-2-apco5j2qoq5i-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:31.733: INFO: ss-2  v1-kube1-20-2-apco5j2qoq5i-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:31.733: INFO: 
Feb  9 10:15:31.733: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  9 10:15:32.744: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:15:32.744: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:15:32.744: INFO: ss-1  v1-kube1-20-2-apco5j2qoq5i-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:32.744: INFO: ss-2  v1-kube1-20-2-apco5j2qoq5i-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:32.744: INFO: 
Feb  9 10:15:32.744: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  9 10:15:33.754: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:15:33.754: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:15:33.754: INFO: ss-1  v1-kube1-20-2-apco5j2qoq5i-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:33.755: INFO: ss-2  v1-kube1-20-2-apco5j2qoq5i-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:33.755: INFO: 
Feb  9 10:15:33.755: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  9 10:15:34.763: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:15:34.763: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:15:34.764: INFO: ss-1  v1-kube1-20-2-apco5j2qoq5i-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:34.764: INFO: ss-2  v1-kube1-20-2-apco5j2qoq5i-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:34.764: INFO: 
Feb  9 10:15:34.764: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  9 10:15:35.775: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:15:35.775: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:15:35.775: INFO: ss-1  v1-kube1-20-2-apco5j2qoq5i-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:35.776: INFO: ss-2  v1-kube1-20-2-apco5j2qoq5i-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:35.776: INFO: 
Feb  9 10:15:35.776: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  9 10:15:36.787: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:15:36.787: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:15:36.787: INFO: ss-1  v1-kube1-20-2-apco5j2qoq5i-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:36.787: INFO: ss-2  v1-kube1-20-2-apco5j2qoq5i-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:36.787: INFO: 
Feb  9 10:15:36.787: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  9 10:15:37.797: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:15:37.798: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:15:37.798: INFO: ss-1  v1-kube1-20-2-apco5j2qoq5i-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:37.798: INFO: ss-2  v1-kube1-20-2-apco5j2qoq5i-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:37.798: INFO: 
Feb  9 10:15:37.798: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  9 10:15:38.813: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:15:38.813: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:15:38.813: INFO: ss-1  v1-kube1-20-2-apco5j2qoq5i-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:38.814: INFO: ss-2  v1-kube1-20-2-apco5j2qoq5i-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:38.814: INFO: 
Feb  9 10:15:38.814: INFO: StatefulSet ss has not reached scale 0, at 3
Feb  9 10:15:39.824: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Feb  9 10:15:39.824: INFO: ss-0  v1-kube1-20-2-apco5j2qoq5i-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:38 +0000 UTC  }]
Feb  9 10:15:39.824: INFO: ss-1  v1-kube1-20-2-apco5j2qoq5i-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:39.825: INFO: ss-2  v1-kube1-20-2-apco5j2qoq5i-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:15:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-02-09 10:14:58 +0000 UTC  }]
Feb  9 10:15:39.825: INFO: 
Feb  9 10:15:39.825: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5167
Feb  9 10:15:40.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:15:41.048: INFO: rc: 1
Feb  9 10:15:41.048: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb  9 10:15:51.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:15:51.276: INFO: rc: 1
Feb  9 10:15:51.276: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb  9 10:16:01.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:16:01.510: INFO: rc: 1
Feb  9 10:16:01.510: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb  9 10:16:11.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:16:11.738: INFO: rc: 1
Feb  9 10:16:11.738: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb  9 10:16:21.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:16:21.956: INFO: rc: 1
Feb  9 10:16:21.956: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb  9 10:16:31.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:16:32.099: INFO: rc: 1
Feb  9 10:16:32.099: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:16:42.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:16:42.231: INFO: rc: 1
Feb  9 10:16:42.231: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:16:52.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:16:52.368: INFO: rc: 1
Feb  9 10:16:52.368: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:17:02.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:17:02.508: INFO: rc: 1
Feb  9 10:17:02.509: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:17:12.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:17:12.648: INFO: rc: 1
Feb  9 10:17:12.648: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:17:22.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:17:22.774: INFO: rc: 1
Feb  9 10:17:22.774: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:17:32.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:17:32.935: INFO: rc: 1
Feb  9 10:17:32.935: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:17:42.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:17:43.068: INFO: rc: 1
Feb  9 10:17:43.068: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:17:53.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:17:53.180: INFO: rc: 1
Feb  9 10:17:53.180: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:18:03.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:18:03.319: INFO: rc: 1
Feb  9 10:18:03.319: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:18:13.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:18:13.443: INFO: rc: 1
Feb  9 10:18:13.443: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:18:23.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:18:23.562: INFO: rc: 1
Feb  9 10:18:23.562: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:18:33.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:18:33.724: INFO: rc: 1
Feb  9 10:18:33.725: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:18:43.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:18:43.854: INFO: rc: 1
Feb  9 10:18:43.854: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:18:53.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:18:53.976: INFO: rc: 1
Feb  9 10:18:53.976: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:19:03.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:19:04.103: INFO: rc: 1
Feb  9 10:19:04.103: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:19:14.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:19:14.246: INFO: rc: 1
Feb  9 10:19:14.247: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:19:24.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:19:24.377: INFO: rc: 1
Feb  9 10:19:24.378: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:19:34.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:19:34.605: INFO: rc: 1
Feb  9 10:19:34.605: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:19:44.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:19:44.746: INFO: rc: 1
Feb  9 10:19:44.747: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:19:54.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:19:54.885: INFO: rc: 1
Feb  9 10:19:54.885: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:20:04.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:20:05.006: INFO: rc: 1
Feb  9 10:20:05.006: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:20:15.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:20:15.136: INFO: rc: 1
Feb  9 10:20:15.136: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:20:25.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:20:25.265: INFO: rc: 1
Feb  9 10:20:25.265: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:20:35.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:20:35.417: INFO: rc: 1
Feb  9 10:20:35.417: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb  9 10:20:45.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=statefulset-5167 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb  9 10:20:45.574: INFO: rc: 1
Feb  9 10:20:45.574: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Feb  9 10:20:45.574: INFO: Scaling statefulset ss to 0
Feb  9 10:20:45.591: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb  9 10:20:45.595: INFO: Deleting all statefulset in ns statefulset-5167
Feb  9 10:20:45.599: INFO: Scaling statefulset ss to 0
Feb  9 10:20:45.610: INFO: Waiting for statefulset status.replicas updated to 0
Feb  9 10:20:45.613: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:20:45.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5167" for this suite.

• [SLOW TEST:367.650 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":146,"skipped":2468,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:20:45.645: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Feb  9 10:20:45.798: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-1436 proxy --unix-socket=/tmp/kubectl-proxy-unix816346665/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:20:45.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1436" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":147,"skipped":2484,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:20:45.899: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5817
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb  9 10:20:46.063: INFO: Waiting up to 5m0s for pod "pod-5ab4e844-92cb-4cea-b919-7c200934c0de" in namespace "emptydir-5817" to be "Succeeded or Failed"
Feb  9 10:20:46.092: INFO: Pod "pod-5ab4e844-92cb-4cea-b919-7c200934c0de": Phase="Pending", Reason="", readiness=false. Elapsed: 29.156653ms
Feb  9 10:20:48.098: INFO: Pod "pod-5ab4e844-92cb-4cea-b919-7c200934c0de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034595058s
STEP: Saw pod success
Feb  9 10:20:48.098: INFO: Pod "pod-5ab4e844-92cb-4cea-b919-7c200934c0de" satisfied condition "Succeeded or Failed"
Feb  9 10:20:48.101: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-5ab4e844-92cb-4cea-b919-7c200934c0de container test-container: <nil>
STEP: delete the pod
Feb  9 10:20:48.233: INFO: Waiting for pod pod-5ab4e844-92cb-4cea-b919-7c200934c0de to disappear
Feb  9 10:20:48.243: INFO: Pod pod-5ab4e844-92cb-4cea-b919-7c200934c0de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:20:48.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5817" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":148,"skipped":2499,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:20:48.254: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1945
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0209 10:20:58.565529      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0209 10:20:58.565699      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0209 10:20:58.565831      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb  9 10:20:58.566: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb  9 10:20:58.566: INFO: Deleting pod "simpletest-rc-to-be-deleted-42bqh" in namespace "gc-1945"
Feb  9 10:20:58.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jgkz" in namespace "gc-1945"
Feb  9 10:20:58.599: INFO: Deleting pod "simpletest-rc-to-be-deleted-65h7q" in namespace "gc-1945"
Feb  9 10:20:58.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vrwl" in namespace "gc-1945"
Feb  9 10:20:58.628: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gkxb" in namespace "gc-1945"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:20:58.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1945" for this suite.

• [SLOW TEST:10.532 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":149,"skipped":2508,"failed":0}
SSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:20:58.787: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-1026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Feb  9 10:20:58.994: INFO: created test-podtemplate-1
Feb  9 10:20:59.004: INFO: created test-podtemplate-2
Feb  9 10:20:59.008: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Feb  9 10:20:59.013: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Feb  9 10:20:59.029: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:20:59.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1026" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":150,"skipped":2513,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:20:59.046: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9603
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb  9 10:21:02.351: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:21:02.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9603" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":151,"skipped":2515,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:21:02.376: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3780
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:21:02.524: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb  9 10:21:06.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 --namespace=crd-publish-openapi-3780 create -f -'
Feb  9 10:21:07.270: INFO: stderr: ""
Feb  9 10:21:07.270: INFO: stdout: "e2e-test-crd-publish-openapi-7525-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb  9 10:21:07.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 --namespace=crd-publish-openapi-3780 delete e2e-test-crd-publish-openapi-7525-crds test-foo'
Feb  9 10:21:07.419: INFO: stderr: ""
Feb  9 10:21:07.419: INFO: stdout: "e2e-test-crd-publish-openapi-7525-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb  9 10:21:07.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 --namespace=crd-publish-openapi-3780 apply -f -'
Feb  9 10:21:07.733: INFO: stderr: ""
Feb  9 10:21:07.733: INFO: stdout: "e2e-test-crd-publish-openapi-7525-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb  9 10:21:07.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 --namespace=crd-publish-openapi-3780 delete e2e-test-crd-publish-openapi-7525-crds test-foo'
Feb  9 10:21:07.852: INFO: stderr: ""
Feb  9 10:21:07.852: INFO: stdout: "e2e-test-crd-publish-openapi-7525-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb  9 10:21:07.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 --namespace=crd-publish-openapi-3780 create -f -'
Feb  9 10:21:08.105: INFO: rc: 1
Feb  9 10:21:08.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 --namespace=crd-publish-openapi-3780 apply -f -'
Feb  9 10:21:08.361: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb  9 10:21:08.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 --namespace=crd-publish-openapi-3780 create -f -'
Feb  9 10:21:08.659: INFO: rc: 1
Feb  9 10:21:08.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 --namespace=crd-publish-openapi-3780 apply -f -'
Feb  9 10:21:08.931: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb  9 10:21:08.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 explain e2e-test-crd-publish-openapi-7525-crds'
Feb  9 10:21:09.231: INFO: stderr: ""
Feb  9 10:21:09.231: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7525-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb  9 10:21:09.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 explain e2e-test-crd-publish-openapi-7525-crds.metadata'
Feb  9 10:21:09.526: INFO: stderr: ""
Feb  9 10:21:09.526: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7525-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb  9 10:21:09.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 explain e2e-test-crd-publish-openapi-7525-crds.spec'
Feb  9 10:21:09.820: INFO: stderr: ""
Feb  9 10:21:09.820: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7525-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb  9 10:21:09.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 explain e2e-test-crd-publish-openapi-7525-crds.spec.bars'
Feb  9 10:21:10.118: INFO: stderr: ""
Feb  9 10:21:10.118: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7525-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb  9 10:21:10.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-3780 explain e2e-test-crd-publish-openapi-7525-crds.spec.bars2'
Feb  9 10:21:10.406: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:21:14.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3780" for this suite.

• [SLOW TEST:12.480 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":152,"skipped":2571,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:21:14.862: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Feb  9 10:21:15.032: INFO: Waiting up to 5m0s for pod "downward-api-61bac36e-952c-408e-be53-dca73ef2b078" in namespace "downward-api-3287" to be "Succeeded or Failed"
Feb  9 10:21:15.037: INFO: Pod "downward-api-61bac36e-952c-408e-be53-dca73ef2b078": Phase="Pending", Reason="", readiness=false. Elapsed: 5.35389ms
Feb  9 10:21:17.051: INFO: Pod "downward-api-61bac36e-952c-408e-be53-dca73ef2b078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018585607s
STEP: Saw pod success
Feb  9 10:21:17.051: INFO: Pod "downward-api-61bac36e-952c-408e-be53-dca73ef2b078" satisfied condition "Succeeded or Failed"
Feb  9 10:21:17.054: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downward-api-61bac36e-952c-408e-be53-dca73ef2b078 container dapi-container: <nil>
STEP: delete the pod
Feb  9 10:21:17.086: INFO: Waiting for pod downward-api-61bac36e-952c-408e-be53-dca73ef2b078 to disappear
Feb  9 10:21:17.090: INFO: Pod downward-api-61bac36e-952c-408e-be53-dca73ef2b078 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:21:17.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3287" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2585,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:21:17.107: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1469
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:21:17.652: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb  9 10:21:19.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462877, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462877, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462877, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748462877, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:21:22.695: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:21:22.706: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:21:23.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1469" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.816 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":154,"skipped":2619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:21:23.924: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb  9 10:21:24.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5988 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Feb  9 10:21:24.359: INFO: stderr: ""
Feb  9 10:21:24.359: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb  9 10:21:29.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5988 get pod e2e-test-httpd-pod -o json'
Feb  9 10:21:29.532: INFO: stderr: ""
Feb  9 10:21:29.532: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.100.239.153/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.239.153/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-02-09T10:21:24Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-02-09T10:21:24Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-02-09T10:21:25Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.100.239.153\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-02-09T10:21:26Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5988\",\n        \"resourceVersion\": \"31027\",\n        \"uid\": \"f8466173-a97a-49cb-8710-2b96b592c4a4\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-q77ff\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"v1-kube1-20-2-apco5j2qoq5i-node-0\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-q77ff\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-q77ff\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-09T10:21:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-09T10:21:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-09T10:21:26Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-02-09T10:21:24Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://4a025bd1bd0a90e6305a5bec38c5eda60d4751490532ac8dba1a38752d900cb0\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-02-09T10:21:25Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.248\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.239.153\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.239.153\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-02-09T10:21:24Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb  9 10:21:29.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5988 replace -f -'
Feb  9 10:21:29.941: INFO: stderr: ""
Feb  9 10:21:29.941: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Feb  9 10:21:29.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5988 delete pods e2e-test-httpd-pod'
Feb  9 10:22:24.912: INFO: stderr: ""
Feb  9 10:22:24.913: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:22:24.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5988" for this suite.

• [SLOW TEST:61.013 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":155,"skipped":2642,"failed":0}
SSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:22:24.937: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-8620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:22:25.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8620" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":156,"skipped":2647,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:22:25.159: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:22:26.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:22:29.492: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:22:29.502: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:22:30.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9437" for this suite.
STEP: Destroying namespace "webhook-9437-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.709 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":157,"skipped":2660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:22:30.870: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:22:31.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4119" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":158,"skipped":2700,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:22:31.232: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:22:31.419: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-254afdd4-75dd-43e9-a356-ffc7176b435d" in namespace "security-context-test-333" to be "Succeeded or Failed"
Feb  9 10:22:31.437: INFO: Pod "alpine-nnp-false-254afdd4-75dd-43e9-a356-ffc7176b435d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.606871ms
Feb  9 10:22:33.450: INFO: Pod "alpine-nnp-false-254afdd4-75dd-43e9-a356-ffc7176b435d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030909082s
Feb  9 10:22:35.456: INFO: Pod "alpine-nnp-false-254afdd4-75dd-43e9-a356-ffc7176b435d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03719126s
Feb  9 10:22:37.465: INFO: Pod "alpine-nnp-false-254afdd4-75dd-43e9-a356-ffc7176b435d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046254078s
Feb  9 10:22:37.465: INFO: Pod "alpine-nnp-false-254afdd4-75dd-43e9-a356-ffc7176b435d" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:22:37.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-333" for this suite.

• [SLOW TEST:6.322 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2704,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:22:37.553: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9111
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9111
STEP: creating service affinity-nodeport-transition in namespace services-9111
STEP: creating replication controller affinity-nodeport-transition in namespace services-9111
I0209 10:22:37.743101      25 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-9111, replica count: 3
I0209 10:22:40.793610      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 10:22:40.820: INFO: Creating new exec pod
Feb  9 10:22:45.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9111 exec execpod-affinity89m5d -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Feb  9 10:22:46.240: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb  9 10:22:46.240: INFO: stdout: ""
Feb  9 10:22:46.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9111 exec execpod-affinity89m5d -- /bin/sh -x -c nc -zv -t -w 2 10.254.184.34 80'
Feb  9 10:22:46.583: INFO: stderr: "+ nc -zv -t -w 2 10.254.184.34 80\nConnection to 10.254.184.34 80 port [tcp/http] succeeded!\n"
Feb  9 10:22:46.583: INFO: stdout: ""
Feb  9 10:22:46.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9111 exec execpod-affinity89m5d -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.5 31514'
Feb  9 10:22:46.880: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.5 31514\nConnection to 10.0.0.5 31514 port [tcp/31514] succeeded!\n"
Feb  9 10:22:46.880: INFO: stdout: ""
Feb  9 10:22:46.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9111 exec execpod-affinity89m5d -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.176 31514'
Feb  9 10:22:47.201: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.176 31514\nConnection to 10.0.0.176 31514 port [tcp/31514] succeeded!\n"
Feb  9 10:22:47.201: INFO: stdout: ""
Feb  9 10:22:47.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9111 exec execpod-affinity89m5d -- /bin/sh -x -c nc -zv -t -w 2 10.6.0.92 31514'
Feb  9 10:22:47.516: INFO: stderr: "+ nc -zv -t -w 2 10.6.0.92 31514\nConnection to 10.6.0.92 31514 port [tcp/31514] succeeded!\n"
Feb  9 10:22:47.516: INFO: stdout: ""
Feb  9 10:22:47.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9111 exec execpod-affinity89m5d -- /bin/sh -x -c nc -zv -t -w 2 10.6.0.143 31514'
Feb  9 10:22:47.833: INFO: stderr: "+ nc -zv -t -w 2 10.6.0.143 31514\nConnection to 10.6.0.143 31514 port [tcp/31514] succeeded!\n"
Feb  9 10:22:47.833: INFO: stdout: ""
Feb  9 10:22:47.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9111 exec execpod-affinity89m5d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.248:31514/ ; done'
Feb  9 10:22:48.364: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n"
Feb  9 10:22:48.364: INFO: stdout: "\naffinity-nodeport-transition-928xt\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-dlwk7\naffinity-nodeport-transition-dlwk7\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-928xt\naffinity-nodeport-transition-928xt\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-928xt\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk"
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-928xt
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-dlwk7
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-dlwk7
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-928xt
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-928xt
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-928xt
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.364: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9111 exec execpod-affinity89m5d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.248:31514/ ; done'
Feb  9 10:22:48.962: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:31514/\n"
Feb  9 10:22:48.962: INFO: stdout: "\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk\naffinity-nodeport-transition-zdzjk"
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Received response from host: affinity-nodeport-transition-zdzjk
Feb  9 10:22:48.962: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9111, will wait for the garbage collector to delete the pods
Feb  9 10:22:49.052: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.184371ms
Feb  9 10:22:49.152: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.289785ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:23:34.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9111" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:57.435 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":160,"skipped":2710,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:23:34.990: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-ad12d0cb-3330-43f9-8edb-c698a3b45575
STEP: Creating a pod to test consume configMaps
Feb  9 10:23:35.182: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-82a4447a-a322-4093-82e9-0b1c9a83f0da" in namespace "projected-3824" to be "Succeeded or Failed"
Feb  9 10:23:35.202: INFO: Pod "pod-projected-configmaps-82a4447a-a322-4093-82e9-0b1c9a83f0da": Phase="Pending", Reason="", readiness=false. Elapsed: 20.229022ms
Feb  9 10:23:37.212: INFO: Pod "pod-projected-configmaps-82a4447a-a322-4093-82e9-0b1c9a83f0da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029740107s
STEP: Saw pod success
Feb  9 10:23:37.212: INFO: Pod "pod-projected-configmaps-82a4447a-a322-4093-82e9-0b1c9a83f0da" satisfied condition "Succeeded or Failed"
Feb  9 10:23:37.215: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-projected-configmaps-82a4447a-a322-4093-82e9-0b1c9a83f0da container agnhost-container: <nil>
STEP: delete the pod
Feb  9 10:23:37.313: INFO: Waiting for pod pod-projected-configmaps-82a4447a-a322-4093-82e9-0b1c9a83f0da to disappear
Feb  9 10:23:37.317: INFO: Pod pod-projected-configmaps-82a4447a-a322-4093-82e9-0b1c9a83f0da no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:23:37.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3824" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":2717,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:23:37.333: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Feb  9 10:23:37.497: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  9 10:23:37.505: INFO: Waiting for terminating namespaces to be deleted...
Feb  9 10:23:37.512: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-0 before test
Feb  9 10:23:37.526: INFO: calico-node-p8hnn from kube-system started at 2021-02-09 08:44:14 +0000 UTC (1 container statuses recorded)
Feb  9 10:23:37.526: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 10:23:37.526: INFO: csi-cinder-nodeplugin-sx9zs from kube-system started at 2021-02-09 08:44:34 +0000 UTC (2 container statuses recorded)
Feb  9 10:23:37.526: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 10:23:37.526: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 10:23:37.526: INFO: kube-dns-autoscaler-f57cd985f-9pfc8 from kube-system started at 2021-02-09 08:44:35 +0000 UTC (1 container statuses recorded)
Feb  9 10:23:37.526: INFO: 	Container autoscaler ready: true, restart count 0
Feb  9 10:23:37.526: INFO: magnum-metrics-server-7ccb6f57c7-2bhgn from kube-system started at 2021-02-09 08:44:36 +0000 UTC (1 container statuses recorded)
Feb  9 10:23:37.526: INFO: 	Container metrics-server ready: true, restart count 0
Feb  9 10:23:37.526: INFO: npd-xtrnn from kube-system started at 2021-02-09 08:44:34 +0000 UTC (1 container statuses recorded)
Feb  9 10:23:37.526: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 10:23:37.526: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-g88r4 from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:23:37.526: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Feb  9 10:23:37.526: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  9 10:23:37.526: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-1 before test
Feb  9 10:23:37.535: INFO: calico-node-4xppr from kube-system started at 2021-02-09 08:54:53 +0000 UTC (1 container statuses recorded)
Feb  9 10:23:37.535: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 10:23:37.535: INFO: csi-cinder-nodeplugin-zltdx from kube-system started at 2021-02-09 10:13:03 +0000 UTC (2 container statuses recorded)
Feb  9 10:23:37.535: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 10:23:37.535: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 10:23:37.535: INFO: npd-hhqz5 from kube-system started at 2021-02-09 08:55:13 +0000 UTC (1 container statuses recorded)
Feb  9 10:23:37.535: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 10:23:37.535: INFO: sonobuoy from sonobuoy started at 2021-02-09 09:07:16 +0000 UTC (1 container statuses recorded)
Feb  9 10:23:37.535: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  9 10:23:37.535: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-6vf7q from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:23:37.535: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Feb  9 10:23:37.535: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  9 10:23:37.535: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-2 before test
Feb  9 10:23:37.543: INFO: calico-node-6klgd from kube-system started at 2021-02-09 08:55:40 +0000 UTC (1 container statuses recorded)
Feb  9 10:23:37.543: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 10:23:37.543: INFO: csi-cinder-nodeplugin-b6x2m from kube-system started at 2021-02-09 08:56:00 +0000 UTC (2 container statuses recorded)
Feb  9 10:23:37.543: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 10:23:37.543: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 10:23:37.543: INFO: npd-9f7sk from kube-system started at 2021-02-09 08:56:00 +0000 UTC (1 container statuses recorded)
Feb  9 10:23:37.543: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 10:23:37.543: INFO: sonobuoy-e2e-job-cba60cb6bac9483e from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:23:37.543: INFO: 	Container e2e ready: true, restart count 0
Feb  9 10:23:37.543: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  9 10:23:37.543: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-kqw79 from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:23:37.543: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Feb  9 10:23:37.543: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node v1-kube1-20-2-apco5j2qoq5i-node-0
STEP: verifying the node has the label node v1-kube1-20-2-apco5j2qoq5i-node-1
STEP: verifying the node has the label node v1-kube1-20-2-apco5j2qoq5i-node-2
Feb  9 10:23:37.640: INFO: Pod calico-node-4xppr requesting resource cpu=250m on Node v1-kube1-20-2-apco5j2qoq5i-node-1
Feb  9 10:23:37.640: INFO: Pod calico-node-6klgd requesting resource cpu=250m on Node v1-kube1-20-2-apco5j2qoq5i-node-2
Feb  9 10:23:37.640: INFO: Pod calico-node-p8hnn requesting resource cpu=250m on Node v1-kube1-20-2-apco5j2qoq5i-node-0
Feb  9 10:23:37.640: INFO: Pod csi-cinder-nodeplugin-b6x2m requesting resource cpu=0m on Node v1-kube1-20-2-apco5j2qoq5i-node-2
Feb  9 10:23:37.640: INFO: Pod csi-cinder-nodeplugin-sx9zs requesting resource cpu=0m on Node v1-kube1-20-2-apco5j2qoq5i-node-0
Feb  9 10:23:37.640: INFO: Pod csi-cinder-nodeplugin-zltdx requesting resource cpu=0m on Node v1-kube1-20-2-apco5j2qoq5i-node-1
Feb  9 10:23:37.640: INFO: Pod kube-dns-autoscaler-f57cd985f-9pfc8 requesting resource cpu=20m on Node v1-kube1-20-2-apco5j2qoq5i-node-0
Feb  9 10:23:37.640: INFO: Pod magnum-metrics-server-7ccb6f57c7-2bhgn requesting resource cpu=0m on Node v1-kube1-20-2-apco5j2qoq5i-node-0
Feb  9 10:23:37.641: INFO: Pod npd-9f7sk requesting resource cpu=20m on Node v1-kube1-20-2-apco5j2qoq5i-node-2
Feb  9 10:23:37.641: INFO: Pod npd-hhqz5 requesting resource cpu=20m on Node v1-kube1-20-2-apco5j2qoq5i-node-1
Feb  9 10:23:37.641: INFO: Pod npd-xtrnn requesting resource cpu=20m on Node v1-kube1-20-2-apco5j2qoq5i-node-0
Feb  9 10:23:37.641: INFO: Pod sonobuoy requesting resource cpu=0m on Node v1-kube1-20-2-apco5j2qoq5i-node-1
Feb  9 10:23:37.641: INFO: Pod sonobuoy-e2e-job-cba60cb6bac9483e requesting resource cpu=0m on Node v1-kube1-20-2-apco5j2qoq5i-node-2
Feb  9 10:23:37.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-6vf7q requesting resource cpu=0m on Node v1-kube1-20-2-apco5j2qoq5i-node-1
Feb  9 10:23:37.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-g88r4 requesting resource cpu=0m on Node v1-kube1-20-2-apco5j2qoq5i-node-0
Feb  9 10:23:37.641: INFO: Pod sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-kqw79 requesting resource cpu=0m on Node v1-kube1-20-2-apco5j2qoq5i-node-2
STEP: Starting Pods to consume most of the cluster CPU.
Feb  9 10:23:37.641: INFO: Creating a pod which consumes cpu=2597m on Node v1-kube1-20-2-apco5j2qoq5i-node-0
Feb  9 10:23:37.657: INFO: Creating a pod which consumes cpu=2611m on Node v1-kube1-20-2-apco5j2qoq5i-node-1
Feb  9 10:23:37.671: INFO: Creating a pod which consumes cpu=2611m on Node v1-kube1-20-2-apco5j2qoq5i-node-2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-34b8e6e3-9b0a-4086-9d47-7f53d524dff8.16620d4796190ba3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7438/filler-pod-34b8e6e3-9b0a-4086-9d47-7f53d524dff8 to v1-kube1-20-2-apco5j2qoq5i-node-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-34b8e6e3-9b0a-4086-9d47-7f53d524dff8.16620d47ce66eefe], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-34b8e6e3-9b0a-4086-9d47-7f53d524dff8.16620d47d4ea7f74], Reason = [Created], Message = [Created container filler-pod-34b8e6e3-9b0a-4086-9d47-7f53d524dff8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-34b8e6e3-9b0a-4086-9d47-7f53d524dff8.16620d47e1891828], Reason = [Started], Message = [Started container filler-pod-34b8e6e3-9b0a-4086-9d47-7f53d524dff8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-505433d1-61cd-4ea4-8ca2-30c196170add.16620d4797d18522], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7438/filler-pod-505433d1-61cd-4ea4-8ca2-30c196170add to v1-kube1-20-2-apco5j2qoq5i-node-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-505433d1-61cd-4ea4-8ca2-30c196170add.16620d47d6a0cff8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-505433d1-61cd-4ea4-8ca2-30c196170add.16620d47e27dbbcb], Reason = [Created], Message = [Created container filler-pod-505433d1-61cd-4ea4-8ca2-30c196170add]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-505433d1-61cd-4ea4-8ca2-30c196170add.16620d47f0ae6dc6], Reason = [Started], Message = [Started container filler-pod-505433d1-61cd-4ea4-8ca2-30c196170add]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8316ce0d-1622-4d4f-a415-c3cc48eb65dd.16620d4796d89ca9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7438/filler-pod-8316ce0d-1622-4d4f-a415-c3cc48eb65dd to v1-kube1-20-2-apco5j2qoq5i-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8316ce0d-1622-4d4f-a415-c3cc48eb65dd.16620d47d040150a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8316ce0d-1622-4d4f-a415-c3cc48eb65dd.16620d47d27c76d6], Reason = [Created], Message = [Created container filler-pod-8316ce0d-1622-4d4f-a415-c3cc48eb65dd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8316ce0d-1622-4d4f-a415-c3cc48eb65dd.16620d47e30330d9], Reason = [Started], Message = [Started container filler-pod-8316ce0d-1622-4d4f-a415-c3cc48eb65dd]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16620d4889a79037], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16620d488a354e4d], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node v1-kube1-20-2-apco5j2qoq5i-node-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node v1-kube1-20-2-apco5j2qoq5i-node-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node v1-kube1-20-2-apco5j2qoq5i-node-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:23:42.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7438" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:5.522 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":162,"skipped":2730,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:23:42.857: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8192
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:24:00.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8192" for this suite.

• [SLOW TEST:17.260 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":163,"skipped":2743,"failed":0}
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:24:00.119: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Feb  9 10:24:00.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-7807 api-versions'
Feb  9 10:24:00.408: INFO: stderr: ""
Feb  9 10:24:00.408: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:24:00.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7807" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":164,"skipped":2743,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:24:00.426: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6120
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:24:00.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6120" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":165,"skipped":2774,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:24:00.614: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4184
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Feb  9 10:26:01.329: INFO: Successfully updated pod "var-expansion-27c1d888-e757-4e94-8184-06870069e36d"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Feb  9 10:26:03.348: INFO: Deleting pod "var-expansion-27c1d888-e757-4e94-8184-06870069e36d" in namespace "var-expansion-4184"
Feb  9 10:26:03.358: INFO: Wait up to 5m0s for pod "var-expansion-27c1d888-e757-4e94-8184-06870069e36d" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:27:15.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4184" for this suite.

• [SLOW TEST:194.780 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":166,"skipped":2809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:27:15.395: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9700
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-b8c0fe00-b19e-4db8-acc4-c0bd773a3011
STEP: Creating configMap with name cm-test-opt-upd-5ec298bc-1418-4ffd-9169-c92edc370fd2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b8c0fe00-b19e-4db8-acc4-c0bd773a3011
STEP: Updating configmap cm-test-opt-upd-5ec298bc-1418-4ffd-9169-c92edc370fd2
STEP: Creating configMap with name cm-test-opt-create-931fa059-59cf-4035-97b3-a33c156fbcca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:28:38.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9700" for this suite.

• [SLOW TEST:82.928 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":167,"skipped":2843,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:28:38.323: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6492
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 10:28:38.499: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39068872-90a0-4836-b693-433a1e944348" in namespace "downward-api-6492" to be "Succeeded or Failed"
Feb  9 10:28:38.507: INFO: Pod "downwardapi-volume-39068872-90a0-4836-b693-433a1e944348": Phase="Pending", Reason="", readiness=false. Elapsed: 7.625398ms
Feb  9 10:28:40.518: INFO: Pod "downwardapi-volume-39068872-90a0-4836-b693-433a1e944348": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019068071s
Feb  9 10:28:42.525: INFO: Pod "downwardapi-volume-39068872-90a0-4836-b693-433a1e944348": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026097855s
STEP: Saw pod success
Feb  9 10:28:42.525: INFO: Pod "downwardapi-volume-39068872-90a0-4836-b693-433a1e944348" satisfied condition "Succeeded or Failed"
Feb  9 10:28:42.529: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod downwardapi-volume-39068872-90a0-4836-b693-433a1e944348 container client-container: <nil>
STEP: delete the pod
Feb  9 10:28:42.614: INFO: Waiting for pod downwardapi-volume-39068872-90a0-4836-b693-433a1e944348 to disappear
Feb  9 10:28:42.618: INFO: Pod downwardapi-volume-39068872-90a0-4836-b693-433a1e944348 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:28:42.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6492" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":168,"skipped":2847,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:28:42.630: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2511
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Feb  9 10:28:42.801: INFO: Found 0 stateful pods, waiting for 3
Feb  9 10:28:52.817: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 10:28:52.818: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 10:28:52.818: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb  9 10:28:52.860: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb  9 10:29:02.902: INFO: Updating stateful set ss2
Feb  9 10:29:02.919: INFO: Waiting for Pod statefulset-2511/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Feb  9 10:29:12.993: INFO: Found 1 stateful pods, waiting for 3
Feb  9 10:29:23.002: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 10:29:23.002: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb  9 10:29:23.002: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb  9 10:29:23.029: INFO: Updating stateful set ss2
Feb  9 10:29:23.054: INFO: Waiting for Pod statefulset-2511/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 10:29:33.071: INFO: Waiting for Pod statefulset-2511/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 10:29:43.075: INFO: Waiting for Pod statefulset-2511/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 10:29:53.104: INFO: Updating stateful set ss2
Feb  9 10:29:53.114: INFO: Waiting for StatefulSet statefulset-2511/ss2 to complete update
Feb  9 10:29:53.114: INFO: Waiting for Pod statefulset-2511/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 10:30:03.133: INFO: Waiting for StatefulSet statefulset-2511/ss2 to complete update
Feb  9 10:30:03.133: INFO: Waiting for Pod statefulset-2511/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb  9 10:30:13.127: INFO: Waiting for StatefulSet statefulset-2511/ss2 to complete update
Feb  9 10:30:13.127: INFO: Waiting for Pod statefulset-2511/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb  9 10:30:23.134: INFO: Deleting all statefulset in ns statefulset-2511
Feb  9 10:30:23.138: INFO: Scaling statefulset ss2 to 0
Feb  9 10:32:23.170: INFO: Waiting for statefulset status.replicas updated to 0
Feb  9 10:32:23.176: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:32:23.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2511" for this suite.

• [SLOW TEST:220.579 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":169,"skipped":2872,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:32:23.210: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4261
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:32:23.370: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb  9 10:32:27.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-4261 --namespace=crd-publish-openapi-4261 create -f -'
Feb  9 10:32:28.488: INFO: stderr: ""
Feb  9 10:32:28.488: INFO: stdout: "e2e-test-crd-publish-openapi-2631-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb  9 10:32:28.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-4261 --namespace=crd-publish-openapi-4261 delete e2e-test-crd-publish-openapi-2631-crds test-cr'
Feb  9 10:32:28.633: INFO: stderr: ""
Feb  9 10:32:28.633: INFO: stdout: "e2e-test-crd-publish-openapi-2631-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb  9 10:32:28.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-4261 --namespace=crd-publish-openapi-4261 apply -f -'
Feb  9 10:32:28.939: INFO: stderr: ""
Feb  9 10:32:28.939: INFO: stdout: "e2e-test-crd-publish-openapi-2631-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb  9 10:32:28.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-4261 --namespace=crd-publish-openapi-4261 delete e2e-test-crd-publish-openapi-2631-crds test-cr'
Feb  9 10:32:29.070: INFO: stderr: ""
Feb  9 10:32:29.070: INFO: stdout: "e2e-test-crd-publish-openapi-2631-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb  9 10:32:29.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-4261 explain e2e-test-crd-publish-openapi-2631-crds'
Feb  9 10:32:29.389: INFO: stderr: ""
Feb  9 10:32:29.389: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2631-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:32:33.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4261" for this suite.

• [SLOW TEST:10.203 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":170,"skipped":2882,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:32:33.414: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:32:34.174: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 10:32:36.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748463554, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748463554, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748463554, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748463554, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:32:39.230: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb  9 10:32:39.260: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:32:39.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1620" for this suite.
STEP: Destroying namespace "webhook-1620-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.948 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":171,"skipped":2891,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:32:39.362: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Feb  9 10:32:39.592: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  9 10:32:39.613: INFO: Waiting for terminating namespaces to be deleted...
Feb  9 10:32:39.617: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-0 before test
Feb  9 10:32:39.626: INFO: calico-node-p8hnn from kube-system started at 2021-02-09 08:44:14 +0000 UTC (1 container statuses recorded)
Feb  9 10:32:39.626: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 10:32:39.627: INFO: csi-cinder-nodeplugin-sx9zs from kube-system started at 2021-02-09 08:44:34 +0000 UTC (2 container statuses recorded)
Feb  9 10:32:39.627: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 10:32:39.627: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 10:32:39.627: INFO: kube-dns-autoscaler-f57cd985f-9pfc8 from kube-system started at 2021-02-09 08:44:35 +0000 UTC (1 container statuses recorded)
Feb  9 10:32:39.627: INFO: 	Container autoscaler ready: true, restart count 0
Feb  9 10:32:39.627: INFO: magnum-metrics-server-7ccb6f57c7-2bhgn from kube-system started at 2021-02-09 08:44:36 +0000 UTC (1 container statuses recorded)
Feb  9 10:32:39.627: INFO: 	Container metrics-server ready: true, restart count 0
Feb  9 10:32:39.627: INFO: npd-xtrnn from kube-system started at 2021-02-09 08:44:34 +0000 UTC (1 container statuses recorded)
Feb  9 10:32:39.627: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 10:32:39.627: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-g88r4 from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:32:39.627: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Feb  9 10:32:39.627: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  9 10:32:39.628: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-1 before test
Feb  9 10:32:39.633: INFO: calico-node-4xppr from kube-system started at 2021-02-09 08:54:53 +0000 UTC (1 container statuses recorded)
Feb  9 10:32:39.634: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 10:32:39.634: INFO: csi-cinder-nodeplugin-zltdx from kube-system started at 2021-02-09 10:13:03 +0000 UTC (2 container statuses recorded)
Feb  9 10:32:39.634: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 10:32:39.634: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 10:32:39.634: INFO: npd-hhqz5 from kube-system started at 2021-02-09 08:55:13 +0000 UTC (1 container statuses recorded)
Feb  9 10:32:39.634: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 10:32:39.634: INFO: sonobuoy from sonobuoy started at 2021-02-09 09:07:16 +0000 UTC (1 container statuses recorded)
Feb  9 10:32:39.634: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  9 10:32:39.634: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-6vf7q from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:32:39.634: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Feb  9 10:32:39.634: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  9 10:32:39.634: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-2 before test
Feb  9 10:32:39.644: INFO: calico-node-6klgd from kube-system started at 2021-02-09 08:55:40 +0000 UTC (1 container statuses recorded)
Feb  9 10:32:39.644: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 10:32:39.644: INFO: csi-cinder-nodeplugin-b6x2m from kube-system started at 2021-02-09 08:56:00 +0000 UTC (2 container statuses recorded)
Feb  9 10:32:39.644: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 10:32:39.644: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 10:32:39.644: INFO: npd-9f7sk from kube-system started at 2021-02-09 08:56:00 +0000 UTC (1 container statuses recorded)
Feb  9 10:32:39.644: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 10:32:39.644: INFO: sonobuoy-e2e-job-cba60cb6bac9483e from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:32:39.644: INFO: 	Container e2e ready: true, restart count 0
Feb  9 10:32:39.644: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  9 10:32:39.644: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-kqw79 from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:32:39.644: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Feb  9 10:32:39.644: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b017a991-ffdd-4366-849f-c5c1e91d5ad2 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.5 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-b017a991-ffdd-4366-849f-c5c1e91d5ad2 off the node v1-kube1-20-2-apco5j2qoq5i-node-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b017a991-ffdd-4366-849f-c5c1e91d5ad2
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:37:45.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7784" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:306.453 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":172,"skipped":2895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:37:45.817: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Feb  9 10:37:45.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5597 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb  9 10:37:46.116: INFO: stderr: ""
Feb  9 10:37:46.116: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Feb  9 10:37:46.116: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb  9 10:37:46.117: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5597" to be "running and ready, or succeeded"
Feb  9 10:37:46.136: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 19.135783ms
Feb  9 10:37:48.144: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.027385132s
Feb  9 10:37:48.144: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb  9 10:37:48.144: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb  9 10:37:48.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5597 logs logs-generator logs-generator'
Feb  9 10:37:48.387: INFO: stderr: ""
Feb  9 10:37:48.387: INFO: stdout: "I0209 10:37:47.401113       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/w56 230\nI0209 10:37:47.601247       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/r52m 454\nI0209 10:37:47.801502       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/42q 521\nI0209 10:37:48.001250       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/v8br 261\nI0209 10:37:48.201239       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/7ds 455\n"
Feb  9 10:37:50.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5597 logs logs-generator logs-generator'
Feb  9 10:37:50.548: INFO: stderr: ""
Feb  9 10:37:50.548: INFO: stdout: "I0209 10:37:47.401113       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/w56 230\nI0209 10:37:47.601247       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/r52m 454\nI0209 10:37:47.801502       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/42q 521\nI0209 10:37:48.001250       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/v8br 261\nI0209 10:37:48.201239       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/7ds 455\nI0209 10:37:48.401246       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/nmn 350\nI0209 10:37:48.601222       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/5dq 291\nI0209 10:37:48.801273       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/xvk 379\nI0209 10:37:49.001344       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/ws8 205\nI0209 10:37:49.201499       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/9vql 252\nI0209 10:37:49.401464       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/xhxr 315\nI0209 10:37:49.601277       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/c5c 570\nI0209 10:37:49.801337       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/sxk 207\nI0209 10:37:50.001286       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/bgm 417\nI0209 10:37:50.201444       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/6qbx 425\nI0209 10:37:50.401316       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/fnv 473\n"
STEP: limiting log lines
Feb  9 10:37:50.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5597 logs logs-generator logs-generator --tail=1'
Feb  9 10:37:50.686: INFO: stderr: ""
Feb  9 10:37:50.686: INFO: stdout: "I0209 10:37:50.601405       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/4nk 273\n"
Feb  9 10:37:50.686: INFO: got output "I0209 10:37:50.601405       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/4nk 273\n"
STEP: limiting log bytes
Feb  9 10:37:50.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5597 logs logs-generator logs-generator --limit-bytes=1'
Feb  9 10:37:50.817: INFO: stderr: ""
Feb  9 10:37:50.817: INFO: stdout: "I"
Feb  9 10:37:50.817: INFO: got output "I"
STEP: exposing timestamps
Feb  9 10:37:50.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5597 logs logs-generator logs-generator --tail=1 --timestamps'
Feb  9 10:37:50.963: INFO: stderr: ""
Feb  9 10:37:50.963: INFO: stdout: "2021-02-09T10:37:50.801575764Z I0209 10:37:50.801300       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/p7c 593\n"
Feb  9 10:37:50.963: INFO: got output "2021-02-09T10:37:50.801575764Z I0209 10:37:50.801300       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/p7c 593\n"
STEP: restricting to a time range
Feb  9 10:37:53.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5597 logs logs-generator logs-generator --since=1s'
Feb  9 10:37:53.628: INFO: stderr: ""
Feb  9 10:37:53.628: INFO: stdout: "I0209 10:37:52.801357       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/8rmm 268\nI0209 10:37:53.001349       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/wv6 411\nI0209 10:37:53.201418       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/d7sn 291\nI0209 10:37:53.401305       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/bpmg 475\nI0209 10:37:53.601332       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/bsq7 474\n"
Feb  9 10:37:53.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5597 logs logs-generator logs-generator --since=24h'
Feb  9 10:37:53.770: INFO: stderr: ""
Feb  9 10:37:53.770: INFO: stdout: "I0209 10:37:47.401113       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/w56 230\nI0209 10:37:47.601247       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/r52m 454\nI0209 10:37:47.801502       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/42q 521\nI0209 10:37:48.001250       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/v8br 261\nI0209 10:37:48.201239       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/7ds 455\nI0209 10:37:48.401246       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/nmn 350\nI0209 10:37:48.601222       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/5dq 291\nI0209 10:37:48.801273       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/xvk 379\nI0209 10:37:49.001344       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/ws8 205\nI0209 10:37:49.201499       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/9vql 252\nI0209 10:37:49.401464       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/xhxr 315\nI0209 10:37:49.601277       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/c5c 570\nI0209 10:37:49.801337       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/sxk 207\nI0209 10:37:50.001286       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/bgm 417\nI0209 10:37:50.201444       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/6qbx 425\nI0209 10:37:50.401316       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/fnv 473\nI0209 10:37:50.601405       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/4nk 273\nI0209 10:37:50.801300       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/p7c 593\nI0209 10:37:51.001588       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/mhdg 564\nI0209 10:37:51.201169       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/sb8f 295\nI0209 10:37:51.401349       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/4rt 274\nI0209 10:37:51.601275       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/5cw 289\nI0209 10:37:51.801425       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/ttb 291\nI0209 10:37:52.001337       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/j8xc 493\nI0209 10:37:52.201367       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/qb2 376\nI0209 10:37:52.401350       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/bnz 583\nI0209 10:37:52.601337       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/mpc 204\nI0209 10:37:52.801357       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/8rmm 268\nI0209 10:37:53.001349       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/wv6 411\nI0209 10:37:53.201418       1 logs_generator.go:76] 29 GET /api/v1/namespaces/kube-system/pods/d7sn 291\nI0209 10:37:53.401305       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/bpmg 475\nI0209 10:37:53.601332       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/bsq7 474\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Feb  9 10:37:53.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5597 delete pod logs-generator'
Feb  9 10:38:34.899: INFO: stderr: ""
Feb  9 10:38:34.899: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:38:34.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5597" for this suite.

• [SLOW TEST:49.100 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":173,"skipped":2926,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:38:34.917: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1581
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:38:35.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1581" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":174,"skipped":2940,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:38:35.151: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb  9 10:38:35.334: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-251  680eabab-d591-467f-8c1a-778ffeb5a7fa 35250 0 2021-02-09 10:38:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-02-09 10:38:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 10:38:35.334: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-251  680eabab-d591-467f-8c1a-778ffeb5a7fa 35251 0 2021-02-09 10:38:35 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-02-09 10:38:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:38:35.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-251" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":175,"skipped":2948,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:38:35.346: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:38:37.533: INFO: Deleting pod "var-expansion-2d5cd565-12f4-4c12-bae6-2326cd1d0a68" in namespace "var-expansion-4605"
Feb  9 10:38:37.548: INFO: Wait up to 5m0s for pod "var-expansion-2d5cd565-12f4-4c12-bae6-2326cd1d0a68" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:39:15.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4605" for this suite.

• [SLOW TEST:40.232 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":176,"skipped":2953,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:39:15.581: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-8339/secret-test-4464d21a-b726-4e5e-a5cc-8f6c1812d11c
STEP: Creating a pod to test consume secrets
Feb  9 10:39:15.770: INFO: Waiting up to 5m0s for pod "pod-configmaps-56c78fbe-eda8-4789-82ad-66c11c86a32e" in namespace "secrets-8339" to be "Succeeded or Failed"
Feb  9 10:39:15.777: INFO: Pod "pod-configmaps-56c78fbe-eda8-4789-82ad-66c11c86a32e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.474571ms
Feb  9 10:39:17.791: INFO: Pod "pod-configmaps-56c78fbe-eda8-4789-82ad-66c11c86a32e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021082064s
STEP: Saw pod success
Feb  9 10:39:17.792: INFO: Pod "pod-configmaps-56c78fbe-eda8-4789-82ad-66c11c86a32e" satisfied condition "Succeeded or Failed"
Feb  9 10:39:17.795: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-configmaps-56c78fbe-eda8-4789-82ad-66c11c86a32e container env-test: <nil>
STEP: delete the pod
Feb  9 10:39:17.869: INFO: Waiting for pod pod-configmaps-56c78fbe-eda8-4789-82ad-66c11c86a32e to disappear
Feb  9 10:39:17.873: INFO: Pod pod-configmaps-56c78fbe-eda8-4789-82ad-66c11c86a32e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:39:17.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8339" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":177,"skipped":2957,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:39:17.883: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6380
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Feb  9 10:39:18.042: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:39:22.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6380" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":178,"skipped":2984,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:39:22.178: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1058
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-4563b560-e77b-4c51-aa9e-a5ca25cfb048 in namespace container-probe-1058
Feb  9 10:39:26.374: INFO: Started pod busybox-4563b560-e77b-4c51-aa9e-a5ca25cfb048 in namespace container-probe-1058
STEP: checking the pod's current state and verifying that restartCount is present
Feb  9 10:39:26.378: INFO: Initial restart count of pod busybox-4563b560-e77b-4c51-aa9e-a5ca25cfb048 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:43:28.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1058" for this suite.

• [SLOW TEST:245.882 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":2986,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:43:28.065: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:43:28.640: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 10:43:30.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464208, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464208, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464208, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464208, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:43:33.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb  9 10:43:35.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=webhook-1732 attach --namespace=webhook-1732 to-be-attached-pod -i -c=container1'
Feb  9 10:43:36.279: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:43:36.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1732" for this suite.
STEP: Destroying namespace "webhook-1732-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.284 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":180,"skipped":2988,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:43:36.349: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8159
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb  9 10:43:41.668: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:43:42.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8159" for this suite.

• [SLOW TEST:6.367 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":181,"skipped":2990,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:43:42.717: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7062
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:43:42.871: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:43:49.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7062" for this suite.

• [SLOW TEST:6.555 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":182,"skipped":2997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:43:49.276: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-a5caad96-8aa3-4bc6-bcc5-ec4cb6ac2f91 in namespace container-probe-9972
Feb  9 10:43:51.458: INFO: Started pod busybox-a5caad96-8aa3-4bc6-bcc5-ec4cb6ac2f91 in namespace container-probe-9972
STEP: checking the pod's current state and verifying that restartCount is present
Feb  9 10:43:51.461: INFO: Initial restart count of pod busybox-a5caad96-8aa3-4bc6-bcc5-ec4cb6ac2f91 is 0
Feb  9 10:44:43.814: INFO: Restart count of pod container-probe-9972/busybox-a5caad96-8aa3-4bc6-bcc5-ec4cb6ac2f91 is now 1 (52.353424151s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:44:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9972" for this suite.

• [SLOW TEST:54.565 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":183,"skipped":3029,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:44:43.843: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:44:44.023: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb  9 10:44:44.035: INFO: Number of nodes with available pods: 0
Feb  9 10:44:44.035: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb  9 10:44:44.059: INFO: Number of nodes with available pods: 0
Feb  9 10:44:44.059: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:45.067: INFO: Number of nodes with available pods: 0
Feb  9 10:44:45.067: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:46.068: INFO: Number of nodes with available pods: 0
Feb  9 10:44:46.068: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:47.069: INFO: Number of nodes with available pods: 1
Feb  9 10:44:47.069: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb  9 10:44:47.107: INFO: Number of nodes with available pods: 1
Feb  9 10:44:47.108: INFO: Number of running nodes: 0, number of available pods: 1
Feb  9 10:44:48.120: INFO: Number of nodes with available pods: 0
Feb  9 10:44:48.120: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb  9 10:44:48.141: INFO: Number of nodes with available pods: 0
Feb  9 10:44:48.141: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:49.147: INFO: Number of nodes with available pods: 0
Feb  9 10:44:49.147: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:50.150: INFO: Number of nodes with available pods: 0
Feb  9 10:44:50.150: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:51.152: INFO: Number of nodes with available pods: 0
Feb  9 10:44:51.152: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:52.154: INFO: Number of nodes with available pods: 0
Feb  9 10:44:52.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:53.153: INFO: Number of nodes with available pods: 0
Feb  9 10:44:53.153: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:54.150: INFO: Number of nodes with available pods: 0
Feb  9 10:44:54.150: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:55.154: INFO: Number of nodes with available pods: 0
Feb  9 10:44:55.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:56.150: INFO: Number of nodes with available pods: 0
Feb  9 10:44:56.150: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:57.154: INFO: Number of nodes with available pods: 0
Feb  9 10:44:57.155: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:58.152: INFO: Number of nodes with available pods: 0
Feb  9 10:44:58.152: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:44:59.154: INFO: Number of nodes with available pods: 0
Feb  9 10:44:59.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:00.152: INFO: Number of nodes with available pods: 0
Feb  9 10:45:00.152: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:01.150: INFO: Number of nodes with available pods: 0
Feb  9 10:45:01.150: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:02.149: INFO: Number of nodes with available pods: 0
Feb  9 10:45:02.149: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:03.149: INFO: Number of nodes with available pods: 0
Feb  9 10:45:03.149: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:04.154: INFO: Number of nodes with available pods: 0
Feb  9 10:45:04.155: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:05.154: INFO: Number of nodes with available pods: 0
Feb  9 10:45:05.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:06.153: INFO: Number of nodes with available pods: 0
Feb  9 10:45:06.153: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:07.152: INFO: Number of nodes with available pods: 0
Feb  9 10:45:07.152: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:08.150: INFO: Number of nodes with available pods: 0
Feb  9 10:45:08.150: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:09.152: INFO: Number of nodes with available pods: 0
Feb  9 10:45:09.152: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:10.151: INFO: Number of nodes with available pods: 0
Feb  9 10:45:10.151: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:11.154: INFO: Number of nodes with available pods: 0
Feb  9 10:45:11.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:12.151: INFO: Number of nodes with available pods: 0
Feb  9 10:45:12.151: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:13.152: INFO: Number of nodes with available pods: 0
Feb  9 10:45:13.152: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:14.153: INFO: Number of nodes with available pods: 0
Feb  9 10:45:14.153: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:15.155: INFO: Number of nodes with available pods: 0
Feb  9 10:45:15.155: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:16.146: INFO: Number of nodes with available pods: 0
Feb  9 10:45:16.146: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:17.154: INFO: Number of nodes with available pods: 0
Feb  9 10:45:17.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:18.149: INFO: Number of nodes with available pods: 0
Feb  9 10:45:18.149: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:19.152: INFO: Number of nodes with available pods: 0
Feb  9 10:45:19.152: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:20.151: INFO: Number of nodes with available pods: 0
Feb  9 10:45:20.151: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:21.154: INFO: Number of nodes with available pods: 0
Feb  9 10:45:21.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:22.154: INFO: Number of nodes with available pods: 0
Feb  9 10:45:22.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:23.154: INFO: Number of nodes with available pods: 0
Feb  9 10:45:23.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:24.151: INFO: Number of nodes with available pods: 0
Feb  9 10:45:24.151: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:25.151: INFO: Number of nodes with available pods: 0
Feb  9 10:45:25.152: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:26.152: INFO: Number of nodes with available pods: 0
Feb  9 10:45:26.152: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:27.148: INFO: Number of nodes with available pods: 0
Feb  9 10:45:27.150: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:28.147: INFO: Number of nodes with available pods: 0
Feb  9 10:45:28.147: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:29.154: INFO: Number of nodes with available pods: 0
Feb  9 10:45:29.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:30.157: INFO: Number of nodes with available pods: 0
Feb  9 10:45:30.157: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:31.155: INFO: Number of nodes with available pods: 0
Feb  9 10:45:31.156: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:32.148: INFO: Number of nodes with available pods: 0
Feb  9 10:45:32.148: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:33.149: INFO: Number of nodes with available pods: 0
Feb  9 10:45:33.149: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:34.153: INFO: Number of nodes with available pods: 0
Feb  9 10:45:34.153: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:35.156: INFO: Number of nodes with available pods: 0
Feb  9 10:45:35.156: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:36.151: INFO: Number of nodes with available pods: 0
Feb  9 10:45:36.151: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:37.154: INFO: Number of nodes with available pods: 0
Feb  9 10:45:37.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:38.150: INFO: Number of nodes with available pods: 0
Feb  9 10:45:38.150: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:39.154: INFO: Number of nodes with available pods: 0
Feb  9 10:45:39.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:40.151: INFO: Number of nodes with available pods: 0
Feb  9 10:45:40.151: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:41.155: INFO: Number of nodes with available pods: 0
Feb  9 10:45:41.155: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:42.150: INFO: Number of nodes with available pods: 0
Feb  9 10:45:42.150: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:43.152: INFO: Number of nodes with available pods: 0
Feb  9 10:45:43.154: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:44.151: INFO: Number of nodes with available pods: 0
Feb  9 10:45:44.151: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:45.155: INFO: Number of nodes with available pods: 0
Feb  9 10:45:45.155: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:46.153: INFO: Number of nodes with available pods: 0
Feb  9 10:45:46.153: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:47.155: INFO: Number of nodes with available pods: 0
Feb  9 10:45:47.155: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:48.149: INFO: Number of nodes with available pods: 0
Feb  9 10:45:48.149: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:49.152: INFO: Number of nodes with available pods: 0
Feb  9 10:45:49.152: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:50.150: INFO: Number of nodes with available pods: 0
Feb  9 10:45:50.150: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:51.145: INFO: Number of nodes with available pods: 0
Feb  9 10:45:51.146: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:52.153: INFO: Number of nodes with available pods: 0
Feb  9 10:45:52.153: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-2 is running more than one daemon pod
Feb  9 10:45:53.149: INFO: Number of nodes with available pods: 1
Feb  9 10:45:53.149: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7217, will wait for the garbage collector to delete the pods
Feb  9 10:45:53.219: INFO: Deleting DaemonSet.extensions daemon-set took: 8.453959ms
Feb  9 10:45:54.219: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000187468s
Feb  9 10:46:50.231: INFO: Number of nodes with available pods: 0
Feb  9 10:46:50.231: INFO: Number of running nodes: 0, number of available pods: 0
Feb  9 10:46:50.235: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37177"},"items":null}

Feb  9 10:46:50.238: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37177"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:46:50.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7217" for this suite.

• [SLOW TEST:126.434 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":184,"skipped":3047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:46:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-f4fc9a0b-19e4-4c31-8dcc-4d909ce49d51
STEP: Creating a pod to test consume secrets
Feb  9 10:46:50.455: INFO: Waiting up to 5m0s for pod "pod-secrets-262f67db-4274-4009-9bd8-a485a403facf" in namespace "secrets-5076" to be "Succeeded or Failed"
Feb  9 10:46:50.460: INFO: Pod "pod-secrets-262f67db-4274-4009-9bd8-a485a403facf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.815078ms
Feb  9 10:46:52.472: INFO: Pod "pod-secrets-262f67db-4274-4009-9bd8-a485a403facf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01751687s
STEP: Saw pod success
Feb  9 10:46:52.472: INFO: Pod "pod-secrets-262f67db-4274-4009-9bd8-a485a403facf" satisfied condition "Succeeded or Failed"
Feb  9 10:46:52.475: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-secrets-262f67db-4274-4009-9bd8-a485a403facf container secret-volume-test: <nil>
STEP: delete the pod
Feb  9 10:46:52.603: INFO: Waiting for pod pod-secrets-262f67db-4274-4009-9bd8-a485a403facf to disappear
Feb  9 10:46:52.607: INFO: Pod pod-secrets-262f67db-4274-4009-9bd8-a485a403facf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:46:52.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5076" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":185,"skipped":3082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:46:52.619: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-2e151aaf-05c6-4669-992b-e1806a15a6b4
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:46:52.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5258" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":186,"skipped":3109,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:46:52.800: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2323
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Feb  9 10:46:53.546: INFO: created pod pod-service-account-defaultsa
Feb  9 10:46:53.546: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb  9 10:46:53.561: INFO: created pod pod-service-account-mountsa
Feb  9 10:46:53.561: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb  9 10:46:53.574: INFO: created pod pod-service-account-nomountsa
Feb  9 10:46:53.574: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb  9 10:46:53.584: INFO: created pod pod-service-account-defaultsa-mountspec
Feb  9 10:46:53.585: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb  9 10:46:53.602: INFO: created pod pod-service-account-mountsa-mountspec
Feb  9 10:46:53.602: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb  9 10:46:53.614: INFO: created pod pod-service-account-nomountsa-mountspec
Feb  9 10:46:53.614: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb  9 10:46:53.623: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb  9 10:46:53.623: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb  9 10:46:53.635: INFO: created pod pod-service-account-mountsa-nomountspec
Feb  9 10:46:53.635: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb  9 10:46:53.645: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb  9 10:46:53.645: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:46:53.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2323" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":187,"skipped":3124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:46:53.664: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3758
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:46:54.669: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:46:57.693: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:46:57.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3758" for this suite.
STEP: Destroying namespace "webhook-3758-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":188,"skipped":3150,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:46:57.958: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Feb  9 10:47:00.759: INFO: Successfully updated pod "annotationupdatead7ad134-60fa-4bd4-90a4-9c4947036f06"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:47:02.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-885" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3161,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:47:02.805: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2231
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-9ce5ab57-8e35-4d76-8ce5-159fd2192982-8831
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:47:03.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2231" for this suite.
STEP: Destroying namespace "nspatchtest-9ce5ab57-8e35-4d76-8ce5-159fd2192982-8831" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":190,"skipped":3165,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:47:03.144: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4235
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-94eb22d8-067d-4a1b-80f3-8f0a7745d79f
STEP: Creating a pod to test consume configMaps
Feb  9 10:47:03.325: INFO: Waiting up to 5m0s for pod "pod-configmaps-53065c81-dad2-4fc8-bfc4-482401a930a4" in namespace "configmap-4235" to be "Succeeded or Failed"
Feb  9 10:47:03.332: INFO: Pod "pod-configmaps-53065c81-dad2-4fc8-bfc4-482401a930a4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.166363ms
Feb  9 10:47:05.347: INFO: Pod "pod-configmaps-53065c81-dad2-4fc8-bfc4-482401a930a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022010153s
STEP: Saw pod success
Feb  9 10:47:05.347: INFO: Pod "pod-configmaps-53065c81-dad2-4fc8-bfc4-482401a930a4" satisfied condition "Succeeded or Failed"
Feb  9 10:47:05.349: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-configmaps-53065c81-dad2-4fc8-bfc4-482401a930a4 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 10:47:05.428: INFO: Waiting for pod pod-configmaps-53065c81-dad2-4fc8-bfc4-482401a930a4 to disappear
Feb  9 10:47:05.444: INFO: Pod pod-configmaps-53065c81-dad2-4fc8-bfc4-482401a930a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:47:05.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4235" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":3186,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:47:05.495: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb  9 10:47:11.722: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:11.728: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:13.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:13.742: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:15.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:15.741: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:17.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:17.738: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:19.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:19.745: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:21.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:21.741: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:23.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:23.736: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:25.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:25.745: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:27.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:27.745: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:29.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:29.739: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:31.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:31.736: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:33.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:33.736: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:35.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:35.743: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:37.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:37.742: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:39.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:39.737: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:41.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:41.742: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:43.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:43.742: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:45.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:45.747: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:47.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:47.743: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:49.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:49.739: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:51.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:51.741: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:53.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:53.743: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:55.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:55.748: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:57.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:57.746: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:47:59.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:47:59.740: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:48:01.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:48:01.738: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:48:03.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:48:03.739: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:48:05.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:48:05.738: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:48:07.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:48:07.741: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:48:09.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:48:09.746: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:48:11.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:48:11.750: INFO: Pod pod-with-prestop-exec-hook still exists
Feb  9 10:48:13.729: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb  9 10:48:13.736: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:48:13.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6347" for this suite.

• [SLOW TEST:68.268 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":192,"skipped":3196,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:48:13.764: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8069.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8069.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8069.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8069.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8069.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8069.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8069.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8069.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8069.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8069.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  9 10:48:17.983: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local from pod dns-8069/dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6: the server could not find the requested resource (get pods dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6)
Feb  9 10:48:17.999: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8069/dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6: the server could not find the requested resource (get pods dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6)
Feb  9 10:48:18.004: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8069/dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6: the server could not find the requested resource (get pods dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6)
Feb  9 10:48:18.009: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local from pod dns-8069/dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6: the server could not find the requested resource (get pods dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6)
Feb  9 10:48:18.014: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local from pod dns-8069/dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6: the server could not find the requested resource (get pods dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6)
Feb  9 10:48:18.018: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8069.svc.cluster.local from pod dns-8069/dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6: the server could not find the requested resource (get pods dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6)
Feb  9 10:48:18.026: INFO: Unable to read jessie_udp@PodARecord from pod dns-8069/dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6: the server could not find the requested resource (get pods dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6)
Feb  9 10:48:18.030: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8069/dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6: the server could not find the requested resource (get pods dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6)
Feb  9 10:48:18.030: INFO: Lookups using dns-8069/dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8069.svc.cluster.local jessie_udp@dns-test-service-2.dns-8069.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb  9 10:48:23.076: INFO: DNS probes using dns-8069/dns-test-cc01975d-accf-4df8-be11-4306f8a1acc6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:48:23.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8069" for this suite.

• [SLOW TEST:9.388 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":193,"skipped":3222,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:48:23.153: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-xs7g
STEP: Creating a pod to test atomic-volume-subpath
Feb  9 10:48:23.343: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-xs7g" in namespace "subpath-9165" to be "Succeeded or Failed"
Feb  9 10:48:23.349: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.953167ms
Feb  9 10:48:25.374: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Running", Reason="", readiness=true. Elapsed: 2.030732398s
Feb  9 10:48:27.391: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Running", Reason="", readiness=true. Elapsed: 4.047040873s
Feb  9 10:48:29.399: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Running", Reason="", readiness=true. Elapsed: 6.055214305s
Feb  9 10:48:31.412: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Running", Reason="", readiness=true. Elapsed: 8.068141456s
Feb  9 10:48:33.425: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Running", Reason="", readiness=true. Elapsed: 10.081105886s
Feb  9 10:48:35.437: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Running", Reason="", readiness=true. Elapsed: 12.093356444s
Feb  9 10:48:37.449: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Running", Reason="", readiness=true. Elapsed: 14.105302016s
Feb  9 10:48:39.458: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Running", Reason="", readiness=true. Elapsed: 16.114466539s
Feb  9 10:48:41.468: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Running", Reason="", readiness=true. Elapsed: 18.124448511s
Feb  9 10:48:43.481: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Running", Reason="", readiness=true. Elapsed: 20.137084986s
Feb  9 10:48:45.497: INFO: Pod "pod-subpath-test-projected-xs7g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.153572248s
STEP: Saw pod success
Feb  9 10:48:45.497: INFO: Pod "pod-subpath-test-projected-xs7g" satisfied condition "Succeeded or Failed"
Feb  9 10:48:45.500: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-subpath-test-projected-xs7g container test-container-subpath-projected-xs7g: <nil>
STEP: delete the pod
Feb  9 10:48:45.574: INFO: Waiting for pod pod-subpath-test-projected-xs7g to disappear
Feb  9 10:48:45.579: INFO: Pod pod-subpath-test-projected-xs7g no longer exists
STEP: Deleting pod pod-subpath-test-projected-xs7g
Feb  9 10:48:45.579: INFO: Deleting pod "pod-subpath-test-projected-xs7g" in namespace "subpath-9165"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:48:45.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9165" for this suite.

• [SLOW TEST:22.442 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":194,"skipped":3224,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:48:45.596: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9664
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-a7b08bf2-2ffe-498f-93f6-f7386fa14a58
STEP: Creating a pod to test consume configMaps
Feb  9 10:48:45.804: INFO: Waiting up to 5m0s for pod "pod-configmaps-32ef1560-f5d0-46e2-b0f2-e91ee40dbe42" in namespace "configmap-9664" to be "Succeeded or Failed"
Feb  9 10:48:45.814: INFO: Pod "pod-configmaps-32ef1560-f5d0-46e2-b0f2-e91ee40dbe42": Phase="Pending", Reason="", readiness=false. Elapsed: 10.308519ms
Feb  9 10:48:47.828: INFO: Pod "pod-configmaps-32ef1560-f5d0-46e2-b0f2-e91ee40dbe42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024066022s
STEP: Saw pod success
Feb  9 10:48:47.828: INFO: Pod "pod-configmaps-32ef1560-f5d0-46e2-b0f2-e91ee40dbe42" satisfied condition "Succeeded or Failed"
Feb  9 10:48:47.831: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-configmaps-32ef1560-f5d0-46e2-b0f2-e91ee40dbe42 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 10:48:47.860: INFO: Waiting for pod pod-configmaps-32ef1560-f5d0-46e2-b0f2-e91ee40dbe42 to disappear
Feb  9 10:48:47.864: INFO: Pod pod-configmaps-32ef1560-f5d0-46e2-b0f2-e91ee40dbe42 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:48:47.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9664" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3232,"failed":0}
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:48:47.873: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:48:50.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4901" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3233,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:48:50.096: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-954
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb  9 10:48:50.879: INFO: starting watch
STEP: patching
STEP: updating
Feb  9 10:48:50.899: INFO: waiting for watch events with expected annotations
Feb  9 10:48:50.899: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:48:50.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-954" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":197,"skipped":3243,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:48:50.988: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-3c26d2d0-20ea-4fe1-a9db-9e12e668f883
STEP: Creating a pod to test consume secrets
Feb  9 10:48:51.163: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1fb8e8fc-d391-4bb4-b556-d1ad93aa6664" in namespace "projected-7903" to be "Succeeded or Failed"
Feb  9 10:48:51.170: INFO: Pod "pod-projected-secrets-1fb8e8fc-d391-4bb4-b556-d1ad93aa6664": Phase="Pending", Reason="", readiness=false. Elapsed: 7.318111ms
Feb  9 10:48:53.179: INFO: Pod "pod-projected-secrets-1fb8e8fc-d391-4bb4-b556-d1ad93aa6664": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016622313s
Feb  9 10:48:55.191: INFO: Pod "pod-projected-secrets-1fb8e8fc-d391-4bb4-b556-d1ad93aa6664": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028030872s
STEP: Saw pod success
Feb  9 10:48:55.191: INFO: Pod "pod-projected-secrets-1fb8e8fc-d391-4bb4-b556-d1ad93aa6664" satisfied condition "Succeeded or Failed"
Feb  9 10:48:55.203: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-projected-secrets-1fb8e8fc-d391-4bb4-b556-d1ad93aa6664 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  9 10:48:55.230: INFO: Waiting for pod pod-projected-secrets-1fb8e8fc-d391-4bb4-b556-d1ad93aa6664 to disappear
Feb  9 10:48:55.239: INFO: Pod pod-projected-secrets-1fb8e8fc-d391-4bb4-b556-d1ad93aa6664 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:48:55.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7903" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3259,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:48:55.257: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:48:55.808: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 10:48:57.829: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464535, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464535, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464535, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464535, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:49:00.849: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:49:11.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7226" for this suite.
STEP: Destroying namespace "webhook-7226-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:15.849 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":199,"skipped":3273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:49:11.108: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:49:11.376: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-30c8181f-ae42-4b3b-8619-e2c09d1955db" in namespace "security-context-test-1883" to be "Succeeded or Failed"
Feb  9 10:49:11.383: INFO: Pod "busybox-readonly-false-30c8181f-ae42-4b3b-8619-e2c09d1955db": Phase="Pending", Reason="", readiness=false. Elapsed: 7.045305ms
Feb  9 10:49:13.393: INFO: Pod "busybox-readonly-false-30c8181f-ae42-4b3b-8619-e2c09d1955db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017244574s
Feb  9 10:49:15.406: INFO: Pod "busybox-readonly-false-30c8181f-ae42-4b3b-8619-e2c09d1955db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029823849s
Feb  9 10:49:15.406: INFO: Pod "busybox-readonly-false-30c8181f-ae42-4b3b-8619-e2c09d1955db" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:49:15.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1883" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":200,"skipped":3308,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:49:15.418: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Feb  9 10:49:15.585: INFO: Waiting up to 5m0s for pod "var-expansion-9d0ca1f9-9094-4a88-ad37-997860601095" in namespace "var-expansion-3799" to be "Succeeded or Failed"
Feb  9 10:49:15.598: INFO: Pod "var-expansion-9d0ca1f9-9094-4a88-ad37-997860601095": Phase="Pending", Reason="", readiness=false. Elapsed: 12.935501ms
Feb  9 10:49:17.609: INFO: Pod "var-expansion-9d0ca1f9-9094-4a88-ad37-997860601095": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02409362s
Feb  9 10:49:19.617: INFO: Pod "var-expansion-9d0ca1f9-9094-4a88-ad37-997860601095": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032063064s
STEP: Saw pod success
Feb  9 10:49:19.618: INFO: Pod "var-expansion-9d0ca1f9-9094-4a88-ad37-997860601095" satisfied condition "Succeeded or Failed"
Feb  9 10:49:19.623: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod var-expansion-9d0ca1f9-9094-4a88-ad37-997860601095 container dapi-container: <nil>
STEP: delete the pod
Feb  9 10:49:19.646: INFO: Waiting for pod var-expansion-9d0ca1f9-9094-4a88-ad37-997860601095 to disappear
Feb  9 10:49:19.653: INFO: Pod var-expansion-9d0ca1f9-9094-4a88-ad37-997860601095 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:49:19.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3799" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":201,"skipped":3308,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:49:19.663: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Feb  9 10:49:19.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2188 create -f -'
Feb  9 10:49:20.225: INFO: stderr: ""
Feb  9 10:49:20.225: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Feb  9 10:49:21.237: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 10:49:21.237: INFO: Found 0 / 1
Feb  9 10:49:22.234: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 10:49:22.234: INFO: Found 1 / 1
Feb  9 10:49:22.234: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb  9 10:49:22.237: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 10:49:22.237: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb  9 10:49:22.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-2188 patch pod agnhost-primary-b2zlf -p {"metadata":{"annotations":{"x":"y"}}}'
Feb  9 10:49:22.371: INFO: stderr: ""
Feb  9 10:49:22.371: INFO: stdout: "pod/agnhost-primary-b2zlf patched\n"
STEP: checking annotations
Feb  9 10:49:22.376: INFO: Selector matched 1 pods for map[app:agnhost]
Feb  9 10:49:22.376: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:49:22.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2188" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":202,"skipped":3333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:49:22.386: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb  9 10:50:02.619: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb  9 10:50:02.619: INFO: Deleting pod "simpletest.rc-5wcsh" in namespace "gc-3423"
W0209 10:50:02.618971      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0209 10:50:02.619008      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0209 10:50:02.619018      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb  9 10:50:02.631: INFO: Deleting pod "simpletest.rc-bj4dp" in namespace "gc-3423"
Feb  9 10:50:02.656: INFO: Deleting pod "simpletest.rc-j44xj" in namespace "gc-3423"
Feb  9 10:50:02.667: INFO: Deleting pod "simpletest.rc-j4l8s" in namespace "gc-3423"
Feb  9 10:50:02.676: INFO: Deleting pod "simpletest.rc-kgldc" in namespace "gc-3423"
Feb  9 10:50:02.697: INFO: Deleting pod "simpletest.rc-q8q8v" in namespace "gc-3423"
Feb  9 10:50:02.711: INFO: Deleting pod "simpletest.rc-s9zxs" in namespace "gc-3423"
Feb  9 10:50:02.724: INFO: Deleting pod "simpletest.rc-srv7j" in namespace "gc-3423"
Feb  9 10:50:02.740: INFO: Deleting pod "simpletest.rc-trxxz" in namespace "gc-3423"
Feb  9 10:50:02.752: INFO: Deleting pod "simpletest.rc-vnp89" in namespace "gc-3423"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:50:02.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3423" for this suite.

• [SLOW TEST:40.505 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":203,"skipped":3391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:50:02.892: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:50:04.173: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 10:50:06.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464604, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464604, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464604, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464604, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:50:09.224: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:50:09.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4850" for this suite.
STEP: Destroying namespace "webhook-4850-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.443 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":204,"skipped":3415,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:50:09.335: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2060
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3643
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2884
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:50:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2060" for this suite.
STEP: Destroying namespace "nsdeletetest-3643" for this suite.
Feb  9 10:50:15.996: INFO: Namespace nsdeletetest-3643 was already deleted
STEP: Destroying namespace "nsdeletetest-2884" for this suite.

• [SLOW TEST:6.666 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":205,"skipped":3416,"failed":0}
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:50:16.001: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2230
STEP: creating service affinity-clusterip in namespace services-2230
STEP: creating replication controller affinity-clusterip in namespace services-2230
I0209 10:50:16.169447      25 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-2230, replica count: 3
I0209 10:50:19.219956      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 10:50:19.240: INFO: Creating new exec pod
Feb  9 10:50:22.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-2230 exec execpod-affinityfh5h2 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Feb  9 10:50:22.627: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb  9 10:50:22.627: INFO: stdout: ""
Feb  9 10:50:22.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-2230 exec execpod-affinityfh5h2 -- /bin/sh -x -c nc -zv -t -w 2 10.254.1.218 80'
Feb  9 10:50:22.916: INFO: stderr: "+ nc -zv -t -w 2 10.254.1.218 80\nConnection to 10.254.1.218 80 port [tcp/http] succeeded!\n"
Feb  9 10:50:22.916: INFO: stdout: ""
Feb  9 10:50:22.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-2230 exec execpod-affinityfh5h2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.1.218:80/ ; done'
Feb  9 10:50:23.321: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.1.218:80/\n"
Feb  9 10:50:23.322: INFO: stdout: "\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr\naffinity-clusterip-gp4sr"
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Received response from host: affinity-clusterip-gp4sr
Feb  9 10:50:23.322: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2230, will wait for the garbage collector to delete the pods
Feb  9 10:50:23.416: INFO: Deleting ReplicationController affinity-clusterip took: 9.396199ms
Feb  9 10:50:23.517: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.287437ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:51:34.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2230" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:78.962 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":206,"skipped":3416,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:51:34.964: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4747
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Feb  9 10:51:35.163: INFO: Waiting up to 5m0s for pod "var-expansion-0d69c489-c2cc-4833-bded-ccdaff80d826" in namespace "var-expansion-4747" to be "Succeeded or Failed"
Feb  9 10:51:35.170: INFO: Pod "var-expansion-0d69c489-c2cc-4833-bded-ccdaff80d826": Phase="Pending", Reason="", readiness=false. Elapsed: 6.263962ms
Feb  9 10:51:37.185: INFO: Pod "var-expansion-0d69c489-c2cc-4833-bded-ccdaff80d826": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021427821s
Feb  9 10:51:39.200: INFO: Pod "var-expansion-0d69c489-c2cc-4833-bded-ccdaff80d826": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036261223s
STEP: Saw pod success
Feb  9 10:51:39.200: INFO: Pod "var-expansion-0d69c489-c2cc-4833-bded-ccdaff80d826" satisfied condition "Succeeded or Failed"
Feb  9 10:51:39.204: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod var-expansion-0d69c489-c2cc-4833-bded-ccdaff80d826 container dapi-container: <nil>
STEP: delete the pod
Feb  9 10:51:39.281: INFO: Waiting for pod var-expansion-0d69c489-c2cc-4833-bded-ccdaff80d826 to disappear
Feb  9 10:51:39.284: INFO: Pod var-expansion-0d69c489-c2cc-4833-bded-ccdaff80d826 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:51:39.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4747" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:51:39.295: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-9257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb  9 10:51:39.445: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Feb  9 10:51:39.871: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb  9 10:51:41.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 10:51:43.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 10:51:45.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 10:51:47.942: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 10:51:49.945: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 10:51:51.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748464699, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 10:51:57.601: INFO: Waited 3.643492087s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:51:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9257" for this suite.

• [SLOW TEST:19.515 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":208,"skipped":3475,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:51:58.811: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2011
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:52:06.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2011" for this suite.

• [SLOW TEST:7.211 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":209,"skipped":3480,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:52:06.022: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3185
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3185
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  9 10:52:06.178: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb  9 10:52:06.250: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  9 10:52:08.264: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  9 10:52:10.260: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:52:12.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:52:14.265: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:52:16.259: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:52:18.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:52:20.264: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:52:22.254: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:52:24.270: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 10:52:26.263: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb  9 10:52:26.270: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb  9 10:52:26.279: INFO: The status of Pod netserver-2 is Running (Ready = false)
Feb  9 10:52:28.300: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb  9 10:52:30.342: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb  9 10:52:30.342: INFO: Breadth first check of 10.100.239.179 on host 10.0.0.248...
Feb  9 10:52:30.346: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.239.180:9080/dial?request=hostname&protocol=udp&host=10.100.239.179&port=8081&tries=1'] Namespace:pod-network-test-3185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 10:52:30.346: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 10:52:30.551: INFO: Waiting for responses: map[]
Feb  9 10:52:30.551: INFO: reached 10.100.239.179 after 0/1 tries
Feb  9 10:52:30.551: INFO: Breadth first check of 10.100.46.170 on host 10.0.0.5...
Feb  9 10:52:30.594: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.239.180:9080/dial?request=hostname&protocol=udp&host=10.100.46.170&port=8081&tries=1'] Namespace:pod-network-test-3185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 10:52:30.594: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 10:52:30.776: INFO: Waiting for responses: map[]
Feb  9 10:52:30.776: INFO: reached 10.100.46.170 after 0/1 tries
Feb  9 10:52:30.776: INFO: Breadth first check of 10.100.80.5 on host 10.0.0.176...
Feb  9 10:52:30.783: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.239.180:9080/dial?request=hostname&protocol=udp&host=10.100.80.5&port=8081&tries=1'] Namespace:pod-network-test-3185 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 10:52:30.783: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 10:52:30.961: INFO: Waiting for responses: map[]
Feb  9 10:52:30.961: INFO: reached 10.100.80.5 after 0/1 tries
Feb  9 10:52:30.961: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:52:30.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3185" for this suite.

• [SLOW TEST:24.953 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3485,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:52:30.975: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-a8909b6c-d7f9-4ccd-b635-5606efb50c7f
STEP: Creating a pod to test consume configMaps
Feb  9 10:52:31.147: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2a1f12e-3928-4d34-9943-e2e2c6c5f8c6" in namespace "projected-2401" to be "Succeeded or Failed"
Feb  9 10:52:31.153: INFO: Pod "pod-projected-configmaps-f2a1f12e-3928-4d34-9943-e2e2c6c5f8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.987015ms
Feb  9 10:52:33.163: INFO: Pod "pod-projected-configmaps-f2a1f12e-3928-4d34-9943-e2e2c6c5f8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015161337s
Feb  9 10:52:35.178: INFO: Pod "pod-projected-configmaps-f2a1f12e-3928-4d34-9943-e2e2c6c5f8c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030537401s
STEP: Saw pod success
Feb  9 10:52:35.178: INFO: Pod "pod-projected-configmaps-f2a1f12e-3928-4d34-9943-e2e2c6c5f8c6" satisfied condition "Succeeded or Failed"
Feb  9 10:52:35.181: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-projected-configmaps-f2a1f12e-3928-4d34-9943-e2e2c6c5f8c6 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 10:52:35.263: INFO: Waiting for pod pod-projected-configmaps-f2a1f12e-3928-4d34-9943-e2e2c6c5f8c6 to disappear
Feb  9 10:52:35.268: INFO: Pod pod-projected-configmaps-f2a1f12e-3928-4d34-9943-e2e2c6c5f8c6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:52:35.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2401" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":211,"skipped":3511,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:52:35.276: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8649
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-8649
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8649 to expose endpoints map[]
Feb  9 10:52:35.465: INFO: successfully validated that service endpoint-test2 in namespace services-8649 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8649
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8649 to expose endpoints map[pod1:[80]]
Feb  9 10:52:38.496: INFO: successfully validated that service endpoint-test2 in namespace services-8649 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-8649
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8649 to expose endpoints map[pod1:[80] pod2:[80]]
Feb  9 10:52:40.532: INFO: successfully validated that service endpoint-test2 in namespace services-8649 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-8649
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8649 to expose endpoints map[pod2:[80]]
Feb  9 10:52:40.577: INFO: successfully validated that service endpoint-test2 in namespace services-8649 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-8649
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8649 to expose endpoints map[]
Feb  9 10:52:41.681: INFO: successfully validated that service endpoint-test2 in namespace services-8649 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:52:41.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8649" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.453 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":212,"skipped":3525,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:52:41.731: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1074
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Feb  9 10:52:42.004: INFO: Waiting up to 5m0s for pod "client-containers-c1d43d87-8886-46b9-a6c5-38148eb72462" in namespace "containers-1074" to be "Succeeded or Failed"
Feb  9 10:52:42.010: INFO: Pod "client-containers-c1d43d87-8886-46b9-a6c5-38148eb72462": Phase="Pending", Reason="", readiness=false. Elapsed: 4.973336ms
Feb  9 10:52:44.022: INFO: Pod "client-containers-c1d43d87-8886-46b9-a6c5-38148eb72462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01757496s
STEP: Saw pod success
Feb  9 10:52:44.023: INFO: Pod "client-containers-c1d43d87-8886-46b9-a6c5-38148eb72462" satisfied condition "Succeeded or Failed"
Feb  9 10:52:44.025: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod client-containers-c1d43d87-8886-46b9-a6c5-38148eb72462 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 10:52:44.055: INFO: Waiting for pod client-containers-c1d43d87-8886-46b9-a6c5-38148eb72462 to disappear
Feb  9 10:52:44.059: INFO: Pod client-containers-c1d43d87-8886-46b9-a6c5-38148eb72462 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:52:44.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1074" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":213,"skipped":3544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:52:44.076: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-2101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Feb  9 10:52:44.233: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  9 10:53:44.307: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:53:44.316: INFO: Starting informer...
STEP: Starting pod...
Feb  9 10:53:44.543: INFO: Pod is running on v1-kube1-20-2-apco5j2qoq5i-node-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb  9 10:53:44.575: INFO: Pod wasn't evicted. Proceeding
Feb  9 10:53:44.575: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb  9 10:54:59.603: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:54:59.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2101" for this suite.

• [SLOW TEST:135.570 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":214,"skipped":3570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:54:59.649: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8882
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb  9 10:55:02.358: INFO: Successfully updated pod "pod-update-c7d7af54-1d16-4f72-b58f-675e7eef3666"
STEP: verifying the updated pod is in kubernetes
Feb  9 10:55:02.365: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:02.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8882" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":215,"skipped":3604,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:02.381: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Feb  9 10:55:02.534: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  9 10:55:02.542: INFO: Waiting for terminating namespaces to be deleted...
Feb  9 10:55:02.547: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-0 before test
Feb  9 10:55:02.553: INFO: calico-node-p8hnn from kube-system started at 2021-02-09 08:44:14 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.553: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 10:55:02.553: INFO: csi-cinder-nodeplugin-sx9zs from kube-system started at 2021-02-09 08:44:34 +0000 UTC (2 container statuses recorded)
Feb  9 10:55:02.553: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 10:55:02.553: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 10:55:02.553: INFO: kube-dns-autoscaler-f57cd985f-9pfc8 from kube-system started at 2021-02-09 08:44:35 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.553: INFO: 	Container autoscaler ready: true, restart count 0
Feb  9 10:55:02.553: INFO: magnum-metrics-server-7ccb6f57c7-2bhgn from kube-system started at 2021-02-09 08:44:36 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.554: INFO: 	Container metrics-server ready: true, restart count 0
Feb  9 10:55:02.554: INFO: npd-xtrnn from kube-system started at 2021-02-09 08:44:34 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.554: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 10:55:02.554: INFO: pod-update-c7d7af54-1d16-4f72-b58f-675e7eef3666 from pods-8882 started at 2021-02-09 10:54:59 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.554: INFO: 	Container nginx ready: true, restart count 0
Feb  9 10:55:02.554: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-g88r4 from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:55:02.554: INFO: 	Container sonobuoy-worker ready: false, restart count 14
Feb  9 10:55:02.554: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  9 10:55:02.554: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-1 before test
Feb  9 10:55:02.559: INFO: calico-node-4xppr from kube-system started at 2021-02-09 08:54:53 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.559: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 10:55:02.559: INFO: csi-cinder-nodeplugin-v9fgx from kube-system started at 2021-02-09 10:53:53 +0000 UTC (2 container statuses recorded)
Feb  9 10:55:02.559: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 10:55:02.559: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 10:55:02.559: INFO: npd-hhqz5 from kube-system started at 2021-02-09 08:55:13 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.559: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 10:55:02.559: INFO: sonobuoy from sonobuoy started at 2021-02-09 09:07:16 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.559: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  9 10:55:02.559: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-6vf7q from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:55:02.559: INFO: 	Container sonobuoy-worker ready: false, restart count 14
Feb  9 10:55:02.559: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  9 10:55:02.559: INFO: taint-eviction-4 from taint-single-pod-2101 started at 2021-02-09 10:53:44 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.559: INFO: 	Container pause ready: true, restart count 0
Feb  9 10:55:02.559: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-2 before test
Feb  9 10:55:02.568: INFO: calico-node-6klgd from kube-system started at 2021-02-09 08:55:40 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.569: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 10:55:02.569: INFO: csi-cinder-nodeplugin-b6x2m from kube-system started at 2021-02-09 08:56:00 +0000 UTC (2 container statuses recorded)
Feb  9 10:55:02.569: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 10:55:02.569: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 10:55:02.569: INFO: npd-9f7sk from kube-system started at 2021-02-09 08:56:00 +0000 UTC (1 container statuses recorded)
Feb  9 10:55:02.569: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 10:55:02.569: INFO: sonobuoy-e2e-job-cba60cb6bac9483e from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:55:02.569: INFO: 	Container e2e ready: true, restart count 0
Feb  9 10:55:02.569: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  9 10:55:02.569: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-kqw79 from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 10:55:02.569: INFO: 	Container sonobuoy-worker ready: false, restart count 13
Feb  9 10:55:02.569: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16620efe7547b2ba], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 node(s) didn't match Pod's node affinity.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16620efe75c791a3], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:03.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5897" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":216,"skipped":3624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:03.628: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5353
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:19.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5353" for this suite.

• [SLOW TEST:16.349 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":217,"skipped":3668,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:19.978: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Feb  9 10:55:20.151: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  9 10:55:20.151: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  9 10:55:20.155: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  9 10:55:20.155: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  9 10:55:20.176: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  9 10:55:20.176: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  9 10:55:20.243: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  9 10:55:20.243: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb  9 10:55:22.234: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb  9 10:55:22.234: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb  9 10:55:22.315: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Feb  9 10:55:22.328: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 0
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 2
Feb  9 10:55:22.333: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 2
Feb  9 10:55:22.334: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 2
Feb  9 10:55:22.334: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 2
Feb  9 10:55:22.340: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 2
Feb  9 10:55:22.340: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 2
Feb  9 10:55:22.358: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 2
Feb  9 10:55:22.358: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 2
Feb  9 10:55:22.376: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
STEP: listing Deployments
Feb  9 10:55:22.383: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Feb  9 10:55:22.401: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Feb  9 10:55:22.418: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1 and labels map[test-deployment:patched test-deployment-static:true]
Feb  9 10:55:22.418: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  9 10:55:22.422: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  9 10:55:22.436: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  9 10:55:22.461: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  9 10:55:22.494: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb  9 10:55:22.506: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Feb  9 10:55:24.242: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
Feb  9 10:55:24.242: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
Feb  9 10:55:24.242: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
Feb  9 10:55:24.242: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
Feb  9 10:55:24.242: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
Feb  9 10:55:24.242: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
Feb  9 10:55:24.243: INFO: observed Deployment test-deployment in namespace deployment-9646 with ReadyReplicas 1
STEP: deleting the Deployment
Feb  9 10:55:24.259: INFO: observed event type MODIFIED
Feb  9 10:55:24.260: INFO: observed event type MODIFIED
Feb  9 10:55:24.260: INFO: observed event type MODIFIED
Feb  9 10:55:24.260: INFO: observed event type MODIFIED
Feb  9 10:55:24.260: INFO: observed event type MODIFIED
Feb  9 10:55:24.261: INFO: observed event type MODIFIED
Feb  9 10:55:24.261: INFO: observed event type MODIFIED
Feb  9 10:55:24.261: INFO: observed event type MODIFIED
Feb  9 10:55:24.261: INFO: observed event type MODIFIED
Feb  9 10:55:24.261: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb  9 10:55:24.272: INFO: Log out all the ReplicaSets if there is no deployment created
Feb  9 10:55:24.282: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-9646  8f0b9fb1-f9ec-49e1-9c5c-0150a534519b 40885 3 2021-02-09 10:55:22 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 0ac1beee-9aa4-47a6-9a02-ac904a5bba17 0xc00756a9a7 0xc00756a9a8}] []  [{kube-controller-manager Update apps/v1 2021-02-09 10:55:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ac1beee-9aa4-47a6-9a02-ac904a5bba17\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00756aa10 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Feb  9 10:55:24.288: INFO: pod: "test-deployment-768947d6f5-2b6qk":
&Pod{ObjectMeta:{test-deployment-768947d6f5-2b6qk test-deployment-768947d6f5- deployment-9646  e062416a-c080-4efe-a231-19328daac9c4 40864 0 2021-02-09 10:55:22 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[cni.projectcalico.org/podIP:10.100.239.182/32 cni.projectcalico.org/podIPs:10.100.239.182/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-768947d6f5 8f0b9fb1-f9ec-49e1-9c5c-0150a534519b 0xc00756ae07 0xc00756ae08}] []  [{kube-controller-manager Update v1 2021-02-09 10:55:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f0b9fb1-f9ec-49e1-9c5c-0150a534519b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 10:55:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 10:55:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.239.182\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8pczd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8pczd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8pczd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:10.100.239.182,StartTime:2021-02-09 10:55:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 10:55:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://534ca2a0cd77d88511392920d91a5bd4ae2f976b32488104ad9ae39d2d80d649,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.239.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb  9 10:55:24.288: INFO: pod: "test-deployment-768947d6f5-rt4q7":
&Pod{ObjectMeta:{test-deployment-768947d6f5-rt4q7 test-deployment-768947d6f5- deployment-9646  50bafcd2-1a92-4fc4-b435-55fcebe1a569 40884 0 2021-02-09 10:55:24 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-768947d6f5 8f0b9fb1-f9ec-49e1-9c5c-0150a534519b 0xc00756afc7 0xc00756afc8}] []  [{kube-controller-manager Update v1 2021-02-09 10:55:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f0b9fb1-f9ec-49e1-9c5c-0150a534519b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 10:55:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8pczd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8pczd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8pczd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2021-02-09 10:55:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb  9 10:55:24.288: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-9646  1b233f2b-026a-4df1-a0c0-16ab65f1d36d 40873 4 2021-02-09 10:55:22 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 0ac1beee-9aa4-47a6-9a02-ac904a5bba17 0xc00756aa77 0xc00756aa78}] []  [{kube-controller-manager Update apps/v1 2021-02-09 10:55:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ac1beee-9aa4-47a6-9a02-ac904a5bba17\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00756aaf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb  9 10:55:24.292: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-9646  80c559ac-6de6-408a-9d03-8fd2ad7e74e9 40811 2 2021-02-09 10:55:20 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 0ac1beee-9aa4-47a6-9a02-ac904a5bba17 0xc00756ab57 0xc00756ab58}] []  [{kube-controller-manager Update apps/v1 2021-02-09 10:55:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ac1beee-9aa4-47a6-9a02-ac904a5bba17\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00756abc0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Feb  9 10:55:24.295: INFO: pod: "test-deployment-8b6954bfb-6g297":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-6g297 test-deployment-8b6954bfb- deployment-9646  34533d5e-0afe-483f-9df9-db3e9a75b10c 40780 0 2021-02-09 10:55:20 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[cni.projectcalico.org/podIP:10.100.46.178/32 cni.projectcalico.org/podIPs:10.100.46.178/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 80c559ac-6de6-408a-9d03-8fd2ad7e74e9 0xc00235c1e7 0xc00235c1e8}] []  [{kube-controller-manager Update v1 2021-02-09 10:55:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"80c559ac-6de6-408a-9d03-8fd2ad7e74e9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 10:55:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 10:55:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.46.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8pczd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8pczd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8pczd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 10:55:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.100.46.178,StartTime:2021-02-09 10:55:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 10:55:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://c7d8af1264e762e87d38c0846c102cbc24dbd0c2ce9521e3e97c982a36f974b9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.46.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:24.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9646" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":218,"skipped":3684,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:24.314: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Feb  9 10:55:24.477: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:27.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4222" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":219,"skipped":3708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:27.880: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2795
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 10:55:28.140: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ace6b7e-d9b8-4a4d-a849-4ca4996dcf25" in namespace "projected-2795" to be "Succeeded or Failed"
Feb  9 10:55:28.155: INFO: Pod "downwardapi-volume-9ace6b7e-d9b8-4a4d-a849-4ca4996dcf25": Phase="Pending", Reason="", readiness=false. Elapsed: 14.82038ms
Feb  9 10:55:30.169: INFO: Pod "downwardapi-volume-9ace6b7e-d9b8-4a4d-a849-4ca4996dcf25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028406013s
Feb  9 10:55:32.177: INFO: Pod "downwardapi-volume-9ace6b7e-d9b8-4a4d-a849-4ca4996dcf25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036018962s
STEP: Saw pod success
Feb  9 10:55:32.177: INFO: Pod "downwardapi-volume-9ace6b7e-d9b8-4a4d-a849-4ca4996dcf25" satisfied condition "Succeeded or Failed"
Feb  9 10:55:32.180: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downwardapi-volume-9ace6b7e-d9b8-4a4d-a849-4ca4996dcf25 container client-container: <nil>
STEP: delete the pod
Feb  9 10:55:32.249: INFO: Waiting for pod downwardapi-volume-9ace6b7e-d9b8-4a4d-a849-4ca4996dcf25 to disappear
Feb  9 10:55:32.252: INFO: Pod downwardapi-volume-9ace6b7e-d9b8-4a4d-a849-4ca4996dcf25 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:32.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2795" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":220,"skipped":3737,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:32.260: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 10:55:32.433: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd6f3548-0c25-4725-a1a5-5b12018a62f0" in namespace "downward-api-8222" to be "Succeeded or Failed"
Feb  9 10:55:32.441: INFO: Pod "downwardapi-volume-dd6f3548-0c25-4725-a1a5-5b12018a62f0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.638997ms
Feb  9 10:55:34.455: INFO: Pod "downwardapi-volume-dd6f3548-0c25-4725-a1a5-5b12018a62f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022038296s
STEP: Saw pod success
Feb  9 10:55:34.455: INFO: Pod "downwardapi-volume-dd6f3548-0c25-4725-a1a5-5b12018a62f0" satisfied condition "Succeeded or Failed"
Feb  9 10:55:34.459: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downwardapi-volume-dd6f3548-0c25-4725-a1a5-5b12018a62f0 container client-container: <nil>
STEP: delete the pod
Feb  9 10:55:34.483: INFO: Waiting for pod downwardapi-volume-dd6f3548-0c25-4725-a1a5-5b12018a62f0 to disappear
Feb  9 10:55:34.489: INFO: Pod downwardapi-volume-dd6f3548-0c25-4725-a1a5-5b12018a62f0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:34.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8222" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3739,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:34.505: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb  9 10:55:34.673: INFO: Pod name pod-release: Found 0 pods out of 1
Feb  9 10:55:39.689: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:40.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-511" for this suite.

• [SLOW TEST:6.241 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":222,"skipped":3775,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:40.747: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1042
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:40.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1042" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":223,"skipped":3786,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:40.913: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb  9 10:55:41.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-7385 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Feb  9 10:55:41.528: INFO: stderr: ""
Feb  9 10:55:41.528: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Feb  9 10:55:41.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-7385 delete pods e2e-test-httpd-pod'
Feb  9 10:55:50.167: INFO: stderr: ""
Feb  9 10:55:50.167: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:50.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7385" for this suite.

• [SLOW TEST:9.287 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":224,"skipped":3788,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:50.200: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Feb  9 10:55:50.374: INFO: Waiting up to 5m0s for pod "downward-api-66cbd1e5-d1aa-45be-afaa-5f52e831be60" in namespace "downward-api-7800" to be "Succeeded or Failed"
Feb  9 10:55:50.384: INFO: Pod "downward-api-66cbd1e5-d1aa-45be-afaa-5f52e831be60": Phase="Pending", Reason="", readiness=false. Elapsed: 10.057011ms
Feb  9 10:55:52.392: INFO: Pod "downward-api-66cbd1e5-d1aa-45be-afaa-5f52e831be60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017930518s
Feb  9 10:55:54.401: INFO: Pod "downward-api-66cbd1e5-d1aa-45be-afaa-5f52e831be60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026472856s
STEP: Saw pod success
Feb  9 10:55:54.401: INFO: Pod "downward-api-66cbd1e5-d1aa-45be-afaa-5f52e831be60" satisfied condition "Succeeded or Failed"
Feb  9 10:55:54.406: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downward-api-66cbd1e5-d1aa-45be-afaa-5f52e831be60 container dapi-container: <nil>
STEP: delete the pod
Feb  9 10:55:54.443: INFO: Waiting for pod downward-api-66cbd1e5-d1aa-45be-afaa-5f52e831be60 to disappear
Feb  9 10:55:54.450: INFO: Pod downward-api-66cbd1e5-d1aa-45be-afaa-5f52e831be60 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:54.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7800" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":225,"skipped":3803,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:54.464: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-a23610e5-184b-457e-a66a-c7b6c7bc77ca
STEP: Creating a pod to test consume configMaps
Feb  9 10:55:54.646: INFO: Waiting up to 5m0s for pod "pod-configmaps-6182b400-2507-429f-95f2-0f46aab50334" in namespace "configmap-3213" to be "Succeeded or Failed"
Feb  9 10:55:54.655: INFO: Pod "pod-configmaps-6182b400-2507-429f-95f2-0f46aab50334": Phase="Pending", Reason="", readiness=false. Elapsed: 9.389459ms
Feb  9 10:55:56.671: INFO: Pod "pod-configmaps-6182b400-2507-429f-95f2-0f46aab50334": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024796653s
Feb  9 10:55:58.685: INFO: Pod "pod-configmaps-6182b400-2507-429f-95f2-0f46aab50334": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03901452s
STEP: Saw pod success
Feb  9 10:55:58.685: INFO: Pod "pod-configmaps-6182b400-2507-429f-95f2-0f46aab50334" satisfied condition "Succeeded or Failed"
Feb  9 10:55:58.688: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-configmaps-6182b400-2507-429f-95f2-0f46aab50334 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 10:55:58.720: INFO: Waiting for pod pod-configmaps-6182b400-2507-429f-95f2-0f46aab50334 to disappear
Feb  9 10:55:58.726: INFO: Pod pod-configmaps-6182b400-2507-429f-95f2-0f46aab50334 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:58.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3213" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":226,"skipped":3806,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:58.736: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:55:58.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6693" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":227,"skipped":3825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:55:58.954: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9051
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-fbd65ff3-f510-4aa3-a5ef-51fd43f748cc
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-fbd65ff3-f510-4aa3-a5ef-51fd43f748cc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:57:35.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9051" for this suite.

• [SLOW TEST:96.941 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":3874,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:57:35.895: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:57:36.522: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 10:57:38.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465056, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465056, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465056, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465056, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:57:41.573: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:57:41.585: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-202-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:57:42.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-248" for this suite.
STEP: Destroying namespace "webhook-248-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.910 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":229,"skipped":3878,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:57:42.805: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2051
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 10:57:43.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 10:57:46.624: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:57:46.639: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5410-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:57:47.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2051" for this suite.
STEP: Destroying namespace "webhook-2051-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.149 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":230,"skipped":3892,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:57:47.954: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-9529/configmap-test-d9ae3088-8370-4dad-93d6-557c1a640aa5
STEP: Creating a pod to test consume configMaps
Feb  9 10:57:48.399: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f86d685-b0a6-45c8-8a74-f2aa2fcb4eec" in namespace "configmap-9529" to be "Succeeded or Failed"
Feb  9 10:57:48.405: INFO: Pod "pod-configmaps-0f86d685-b0a6-45c8-8a74-f2aa2fcb4eec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.233101ms
Feb  9 10:57:50.424: INFO: Pod "pod-configmaps-0f86d685-b0a6-45c8-8a74-f2aa2fcb4eec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025049207s
STEP: Saw pod success
Feb  9 10:57:50.424: INFO: Pod "pod-configmaps-0f86d685-b0a6-45c8-8a74-f2aa2fcb4eec" satisfied condition "Succeeded or Failed"
Feb  9 10:57:50.426: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-configmaps-0f86d685-b0a6-45c8-8a74-f2aa2fcb4eec container env-test: <nil>
STEP: delete the pod
Feb  9 10:57:50.450: INFO: Waiting for pod pod-configmaps-0f86d685-b0a6-45c8-8a74-f2aa2fcb4eec to disappear
Feb  9 10:57:50.457: INFO: Pod pod-configmaps-0f86d685-b0a6-45c8-8a74-f2aa2fcb4eec no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:57:50.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9529" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":231,"skipped":3897,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:57:50.474: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5045
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0209 10:57:51.731844      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0209 10:57:51.731990      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0209 10:57:51.732077      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Feb  9 10:57:51.732: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:57:51.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5045" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":232,"skipped":3903,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:57:51.745: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-a0e8b173-35ae-417f-a89f-a4f595d696eb
STEP: Creating secret with name secret-projected-all-test-volume-e9979d3a-e2c4-4293-946f-85a0ad4b471f
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb  9 10:57:51.920: INFO: Waiting up to 5m0s for pod "projected-volume-2b4b87d5-0d0b-491e-a74a-8d736aabf8fa" in namespace "projected-1824" to be "Succeeded or Failed"
Feb  9 10:57:51.927: INFO: Pod "projected-volume-2b4b87d5-0d0b-491e-a74a-8d736aabf8fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493839ms
Feb  9 10:57:53.939: INFO: Pod "projected-volume-2b4b87d5-0d0b-491e-a74a-8d736aabf8fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018093034s
STEP: Saw pod success
Feb  9 10:57:53.939: INFO: Pod "projected-volume-2b4b87d5-0d0b-491e-a74a-8d736aabf8fa" satisfied condition "Succeeded or Failed"
Feb  9 10:57:53.943: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod projected-volume-2b4b87d5-0d0b-491e-a74a-8d736aabf8fa container projected-all-volume-test: <nil>
STEP: delete the pod
Feb  9 10:57:53.982: INFO: Waiting for pod projected-volume-2b4b87d5-0d0b-491e-a74a-8d736aabf8fa to disappear
Feb  9 10:57:53.987: INFO: Pod projected-volume-2b4b87d5-0d0b-491e-a74a-8d736aabf8fa no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:57:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1824" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":233,"skipped":3918,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:57:54.003: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb  9 10:57:54.220: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  9 10:58:54.286: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Feb  9 10:58:54.324: INFO: Created pod: pod0-sched-preemption-low-priority
Feb  9 10:58:54.355: INFO: Created pod: pod1-sched-preemption-medium-priority
Feb  9 10:58:54.384: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:59:38.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3679" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:104.527 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":234,"skipped":3965,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:59:38.532: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb  9 10:59:40.737: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:59:40.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4432" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":235,"skipped":3973,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:59:40.770: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 10:59:40.950: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77345dcb-6dc3-48f4-b23e-858b00850c29" in namespace "downward-api-9648" to be "Succeeded or Failed"
Feb  9 10:59:40.954: INFO: Pod "downwardapi-volume-77345dcb-6dc3-48f4-b23e-858b00850c29": Phase="Pending", Reason="", readiness=false. Elapsed: 3.67871ms
Feb  9 10:59:42.967: INFO: Pod "downwardapi-volume-77345dcb-6dc3-48f4-b23e-858b00850c29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016862466s
STEP: Saw pod success
Feb  9 10:59:42.967: INFO: Pod "downwardapi-volume-77345dcb-6dc3-48f4-b23e-858b00850c29" satisfied condition "Succeeded or Failed"
Feb  9 10:59:42.970: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod downwardapi-volume-77345dcb-6dc3-48f4-b23e-858b00850c29 container client-container: <nil>
STEP: delete the pod
Feb  9 10:59:42.995: INFO: Waiting for pod downwardapi-volume-77345dcb-6dc3-48f4-b23e-858b00850c29 to disappear
Feb  9 10:59:43.000: INFO: Pod downwardapi-volume-77345dcb-6dc3-48f4-b23e-858b00850c29 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:59:43.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9648" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":4025,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:59:43.016: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7676
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 10:59:43.177: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:59:44.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7676" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":237,"skipped":4050,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:59:44.450: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-1125
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1125
STEP: Deleting pre-stop pod
Feb  9 10:59:55.830: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 10:59:55.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1125" for this suite.

• [SLOW TEST:11.407 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":238,"skipped":4062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 10:59:55.863: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8839
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-21dc79dc-b00e-4916-8a88-baa141059156
STEP: Creating a pod to test consume configMaps
Feb  9 10:59:56.048: INFO: Waiting up to 5m0s for pod "pod-configmaps-53ea7425-2c3a-46fc-9104-eed7cf24b3e5" in namespace "configmap-8839" to be "Succeeded or Failed"
Feb  9 10:59:56.057: INFO: Pod "pod-configmaps-53ea7425-2c3a-46fc-9104-eed7cf24b3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.634616ms
Feb  9 10:59:58.071: INFO: Pod "pod-configmaps-53ea7425-2c3a-46fc-9104-eed7cf24b3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022781044s
Feb  9 11:00:00.086: INFO: Pod "pod-configmaps-53ea7425-2c3a-46fc-9104-eed7cf24b3e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037443855s
STEP: Saw pod success
Feb  9 11:00:00.086: INFO: Pod "pod-configmaps-53ea7425-2c3a-46fc-9104-eed7cf24b3e5" satisfied condition "Succeeded or Failed"
Feb  9 11:00:00.090: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-configmaps-53ea7425-2c3a-46fc-9104-eed7cf24b3e5 container configmap-volume-test: <nil>
STEP: delete the pod
Feb  9 11:00:00.159: INFO: Waiting for pod pod-configmaps-53ea7425-2c3a-46fc-9104-eed7cf24b3e5 to disappear
Feb  9 11:00:00.166: INFO: Pod pod-configmaps-53ea7425-2c3a-46fc-9104-eed7cf24b3e5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:00:00.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8839" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":239,"skipped":4108,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:00:00.180: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb  9 11:00:00.357: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6070  d2f5144a-b4bb-4fea-951b-45c78f523d3c 42695 0 2021-02-09 11:00:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-09 11:00:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 11:00:00.358: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6070  d2f5144a-b4bb-4fea-951b-45c78f523d3c 42696 0 2021-02-09 11:00:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-09 11:00:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb  9 11:00:00.371: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6070  d2f5144a-b4bb-4fea-951b-45c78f523d3c 42697 0 2021-02-09 11:00:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-09 11:00:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 11:00:00.372: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6070  d2f5144a-b4bb-4fea-951b-45c78f523d3c 42698 0 2021-02-09 11:00:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-02-09 11:00:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:00:00.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6070" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":240,"skipped":4128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:00:00.386: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4497
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4497.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4497.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  9 11:00:04.607: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4497/dns-test-2e6af27f-b6c3-4b4d-a602-2d32dbaa9c2f: the server could not find the requested resource (get pods dns-test-2e6af27f-b6c3-4b4d-a602-2d32dbaa9c2f)
Feb  9 11:00:04.612: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4497/dns-test-2e6af27f-b6c3-4b4d-a602-2d32dbaa9c2f: the server could not find the requested resource (get pods dns-test-2e6af27f-b6c3-4b4d-a602-2d32dbaa9c2f)
Feb  9 11:00:04.626: INFO: Unable to read jessie_udp@PodARecord from pod dns-4497/dns-test-2e6af27f-b6c3-4b4d-a602-2d32dbaa9c2f: the server could not find the requested resource (get pods dns-test-2e6af27f-b6c3-4b4d-a602-2d32dbaa9c2f)
Feb  9 11:00:04.630: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4497/dns-test-2e6af27f-b6c3-4b4d-a602-2d32dbaa9c2f: the server could not find the requested resource (get pods dns-test-2e6af27f-b6c3-4b4d-a602-2d32dbaa9c2f)
Feb  9 11:00:04.630: INFO: Lookups using dns-4497/dns-test-2e6af27f-b6c3-4b4d-a602-2d32dbaa9c2f failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb  9 11:00:09.662: INFO: DNS probes using dns-4497/dns-test-2e6af27f-b6c3-4b4d-a602-2d32dbaa9c2f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:00:09.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4497" for this suite.

• [SLOW TEST:9.331 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":241,"skipped":4232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:00:09.717: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1474
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb  9 11:00:09.891: INFO: Waiting up to 5m0s for pod "pod-6ce52db7-2e7f-4189-b858-27eb0bb7761a" in namespace "emptydir-1474" to be "Succeeded or Failed"
Feb  9 11:00:09.895: INFO: Pod "pod-6ce52db7-2e7f-4189-b858-27eb0bb7761a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.132964ms
Feb  9 11:00:11.905: INFO: Pod "pod-6ce52db7-2e7f-4189-b858-27eb0bb7761a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014089139s
STEP: Saw pod success
Feb  9 11:00:11.905: INFO: Pod "pod-6ce52db7-2e7f-4189-b858-27eb0bb7761a" satisfied condition "Succeeded or Failed"
Feb  9 11:00:11.909: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-6ce52db7-2e7f-4189-b858-27eb0bb7761a container test-container: <nil>
STEP: delete the pod
Feb  9 11:00:11.940: INFO: Waiting for pod pod-6ce52db7-2e7f-4189-b858-27eb0bb7761a to disappear
Feb  9 11:00:11.946: INFO: Pod pod-6ce52db7-2e7f-4189-b858-27eb0bb7761a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:00:11.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1474" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4270,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:00:11.961: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9247
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 11:00:12.141: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40197117-fe7e-41e0-b938-e0c889f8db45" in namespace "downward-api-9247" to be "Succeeded or Failed"
Feb  9 11:00:12.148: INFO: Pod "downwardapi-volume-40197117-fe7e-41e0-b938-e0c889f8db45": Phase="Pending", Reason="", readiness=false. Elapsed: 7.803145ms
Feb  9 11:00:14.156: INFO: Pod "downwardapi-volume-40197117-fe7e-41e0-b938-e0c889f8db45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014944879s
Feb  9 11:00:16.162: INFO: Pod "downwardapi-volume-40197117-fe7e-41e0-b938-e0c889f8db45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021812486s
STEP: Saw pod success
Feb  9 11:00:16.162: INFO: Pod "downwardapi-volume-40197117-fe7e-41e0-b938-e0c889f8db45" satisfied condition "Succeeded or Failed"
Feb  9 11:00:16.166: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downwardapi-volume-40197117-fe7e-41e0-b938-e0c889f8db45 container client-container: <nil>
STEP: delete the pod
Feb  9 11:00:16.205: INFO: Waiting for pod downwardapi-volume-40197117-fe7e-41e0-b938-e0c889f8db45 to disappear
Feb  9 11:00:16.208: INFO: Pod downwardapi-volume-40197117-fe7e-41e0-b938-e0c889f8db45 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:00:16.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9247" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4276,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:00:16.220: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Feb  9 11:00:16.371: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  9 11:00:16.380: INFO: Waiting for terminating namespaces to be deleted...
Feb  9 11:00:16.385: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-0 before test
Feb  9 11:00:16.394: INFO: calico-node-p8hnn from kube-system started at 2021-02-09 08:44:14 +0000 UTC (1 container statuses recorded)
Feb  9 11:00:16.394: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 11:00:16.394: INFO: csi-cinder-nodeplugin-sx9zs from kube-system started at 2021-02-09 08:44:34 +0000 UTC (2 container statuses recorded)
Feb  9 11:00:16.394: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 11:00:16.394: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 11:00:16.394: INFO: kube-dns-autoscaler-f57cd985f-9pfc8 from kube-system started at 2021-02-09 08:44:35 +0000 UTC (1 container statuses recorded)
Feb  9 11:00:16.394: INFO: 	Container autoscaler ready: true, restart count 0
Feb  9 11:00:16.394: INFO: magnum-metrics-server-7ccb6f57c7-2bhgn from kube-system started at 2021-02-09 08:44:36 +0000 UTC (1 container statuses recorded)
Feb  9 11:00:16.394: INFO: 	Container metrics-server ready: true, restart count 0
Feb  9 11:00:16.394: INFO: npd-xtrnn from kube-system started at 2021-02-09 08:44:34 +0000 UTC (1 container statuses recorded)
Feb  9 11:00:16.394: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 11:00:16.394: INFO: tester from prestop-1125 started at 2021-02-09 10:59:46 +0000 UTC (1 container statuses recorded)
Feb  9 11:00:16.394: INFO: 	Container tester ready: true, restart count 0
Feb  9 11:00:16.394: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-g88r4 from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 11:00:16.394: INFO: 	Container sonobuoy-worker ready: false, restart count 15
Feb  9 11:00:16.394: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  9 11:00:16.394: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-1 before test
Feb  9 11:00:16.402: INFO: calico-node-4xppr from kube-system started at 2021-02-09 08:54:53 +0000 UTC (1 container statuses recorded)
Feb  9 11:00:16.402: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 11:00:16.402: INFO: csi-cinder-nodeplugin-v9fgx from kube-system started at 2021-02-09 10:53:53 +0000 UTC (2 container statuses recorded)
Feb  9 11:00:16.402: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 11:00:16.402: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 11:00:16.402: INFO: npd-hhqz5 from kube-system started at 2021-02-09 08:55:13 +0000 UTC (1 container statuses recorded)
Feb  9 11:00:16.402: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 11:00:16.402: INFO: sonobuoy from sonobuoy started at 2021-02-09 09:07:16 +0000 UTC (1 container statuses recorded)
Feb  9 11:00:16.402: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  9 11:00:16.402: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-6vf7q from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 11:00:16.402: INFO: 	Container sonobuoy-worker ready: false, restart count 15
Feb  9 11:00:16.402: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  9 11:00:16.402: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-2 before test
Feb  9 11:00:16.408: INFO: calico-node-6klgd from kube-system started at 2021-02-09 08:55:40 +0000 UTC (1 container statuses recorded)
Feb  9 11:00:16.408: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 11:00:16.408: INFO: csi-cinder-nodeplugin-b6x2m from kube-system started at 2021-02-09 08:56:00 +0000 UTC (2 container statuses recorded)
Feb  9 11:00:16.408: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 11:00:16.408: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 11:00:16.408: INFO: npd-9f7sk from kube-system started at 2021-02-09 08:56:00 +0000 UTC (1 container statuses recorded)
Feb  9 11:00:16.408: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 11:00:16.408: INFO: sonobuoy-e2e-job-cba60cb6bac9483e from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 11:00:16.408: INFO: 	Container e2e ready: true, restart count 0
Feb  9 11:00:16.408: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  9 11:00:16.408: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-kqw79 from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 11:00:16.408: INFO: 	Container sonobuoy-worker ready: false, restart count 15
Feb  9 11:00:16.408: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e527ec81-848e-4c74-a624-87d3c0c2638b 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.0.0.5 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.0.0.5 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Feb  9 11:00:32.611: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.5 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:32.611: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54321
Feb  9 11:00:32.851: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.5:54321/hostname] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:32.851: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54321 UDP
Feb  9 11:00:33.046: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.5 54321] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:33.047: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Feb  9 11:00:38.247: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.5 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:38.247: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54321
Feb  9 11:00:38.430: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.5:54321/hostname] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:38.430: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54321 UDP
Feb  9 11:00:38.614: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.5 54321] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:38.614: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Feb  9 11:00:43.810: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.5 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:43.810: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54321
Feb  9 11:00:44.003: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.5:54321/hostname] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:44.003: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54321 UDP
Feb  9 11:00:44.203: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.5 54321] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:44.203: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Feb  9 11:00:49.388: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.5 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:49.388: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54321
Feb  9 11:00:49.705: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.5:54321/hostname] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:49.705: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54321 UDP
Feb  9 11:00:49.893: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.5 54321] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:49.893: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Feb  9 11:00:55.069: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.5 http://127.0.0.1:54321/hostname] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:55.069: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54321
Feb  9 11:00:55.255: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.5:54321/hostname] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:55.255: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54321 UDP
Feb  9 11:00:55.444: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.5 54321] Namespace:sched-pred-4460 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:00:55.444: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: removing the label kubernetes.io/e2e-e527ec81-848e-4c74-a624-87d3c0c2638b off the node v1-kube1-20-2-apco5j2qoq5i-node-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e527ec81-848e-4c74-a624-87d3c0c2638b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:01:00.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4460" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:44.468 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":244,"skipped":4327,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:01:00.688: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Feb  9 11:01:04.874: INFO: Pod pod-hostip-10fda1db-1a43-46e4-881c-241155c43140 has hostIP: 10.0.0.248
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:01:04.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5060" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4344,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:01:04.888: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7043
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb  9 11:01:05.057: INFO: Waiting up to 5m0s for pod "pod-b2ff9f79-a83b-466b-990b-d75853c570f4" in namespace "emptydir-7043" to be "Succeeded or Failed"
Feb  9 11:01:05.066: INFO: Pod "pod-b2ff9f79-a83b-466b-990b-d75853c570f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.458198ms
Feb  9 11:01:07.079: INFO: Pod "pod-b2ff9f79-a83b-466b-990b-d75853c570f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021722414s
Feb  9 11:01:09.089: INFO: Pod "pod-b2ff9f79-a83b-466b-990b-d75853c570f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031513588s
STEP: Saw pod success
Feb  9 11:01:09.089: INFO: Pod "pod-b2ff9f79-a83b-466b-990b-d75853c570f4" satisfied condition "Succeeded or Failed"
Feb  9 11:01:09.101: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-2 pod pod-b2ff9f79-a83b-466b-990b-d75853c570f4 container test-container: <nil>
STEP: delete the pod
Feb  9 11:01:09.179: INFO: Waiting for pod pod-b2ff9f79-a83b-466b-990b-d75853c570f4 to disappear
Feb  9 11:01:09.181: INFO: Pod pod-b2ff9f79-a83b-466b-990b-d75853c570f4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:01:09.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7043" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":246,"skipped":4427,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:01:09.189: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9480
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb  9 11:01:13.905: INFO: Successfully updated pod "pod-update-activedeadlineseconds-172c779d-627b-4c7d-9f5b-9b8229d72d9e"
Feb  9 11:01:13.905: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-172c779d-627b-4c7d-9f5b-9b8229d72d9e" in namespace "pods-9480" to be "terminated due to deadline exceeded"
Feb  9 11:01:13.913: INFO: Pod "pod-update-activedeadlineseconds-172c779d-627b-4c7d-9f5b-9b8229d72d9e": Phase="Running", Reason="", readiness=true. Elapsed: 7.856004ms
Feb  9 11:01:15.928: INFO: Pod "pod-update-activedeadlineseconds-172c779d-627b-4c7d-9f5b-9b8229d72d9e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.022667114s
Feb  9 11:01:15.928: INFO: Pod "pod-update-activedeadlineseconds-172c779d-627b-4c7d-9f5b-9b8229d72d9e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:01:15.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9480" for this suite.

• [SLOW TEST:6.817 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":247,"skipped":4428,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:01:16.009: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Feb  9 11:01:16.187: INFO: Waiting up to 5m0s for pod "client-containers-c0a63d09-3a91-4c04-9883-929424bc123d" in namespace "containers-8843" to be "Succeeded or Failed"
Feb  9 11:01:16.193: INFO: Pod "client-containers-c0a63d09-3a91-4c04-9883-929424bc123d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.257932ms
Feb  9 11:01:18.211: INFO: Pod "client-containers-c0a63d09-3a91-4c04-9883-929424bc123d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024106587s
Feb  9 11:01:20.227: INFO: Pod "client-containers-c0a63d09-3a91-4c04-9883-929424bc123d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039806279s
STEP: Saw pod success
Feb  9 11:01:20.228: INFO: Pod "client-containers-c0a63d09-3a91-4c04-9883-929424bc123d" satisfied condition "Succeeded or Failed"
Feb  9 11:01:20.231: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-2 pod client-containers-c0a63d09-3a91-4c04-9883-929424bc123d container agnhost-container: <nil>
STEP: delete the pod
Feb  9 11:01:20.261: INFO: Waiting for pod client-containers-c0a63d09-3a91-4c04-9883-929424bc123d to disappear
Feb  9 11:01:20.264: INFO: Pod client-containers-c0a63d09-3a91-4c04-9883-929424bc123d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:01:20.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8843" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4441,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:01:20.275: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:01:20.456: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb  9 11:01:25.469: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  9 11:01:25.469: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb  9 11:01:27.487: INFO: Creating deployment "test-rollover-deployment"
Feb  9 11:01:27.502: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb  9 11:01:29.513: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb  9 11:01:29.527: INFO: Ensure that both replica sets have 1 created replica
Feb  9 11:01:29.532: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb  9 11:01:29.543: INFO: Updating deployment test-rollover-deployment
Feb  9 11:01:29.543: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb  9 11:01:31.560: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb  9 11:01:31.565: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb  9 11:01:31.571: INFO: all replica sets need to contain the pod-template-hash label
Feb  9 11:01:31.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465289, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 11:01:33.589: INFO: all replica sets need to contain the pod-template-hash label
Feb  9 11:01:33.590: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465291, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 11:01:35.590: INFO: all replica sets need to contain the pod-template-hash label
Feb  9 11:01:35.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465291, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 11:01:37.591: INFO: all replica sets need to contain the pod-template-hash label
Feb  9 11:01:37.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465291, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 11:01:39.586: INFO: all replica sets need to contain the pod-template-hash label
Feb  9 11:01:39.586: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465291, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 11:01:41.586: INFO: all replica sets need to contain the pod-template-hash label
Feb  9 11:01:41.586: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465291, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748465287, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 11:01:43.588: INFO: 
Feb  9 11:01:43.588: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb  9 11:01:43.599: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4867  509cd68b-df1f-4b37-95bb-345250cd03dd 43505 2 2021-02-09 11:01:27 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-02-09 11:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-09 11:01:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030472f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-02-09 11:01:27 +0000 UTC,LastTransitionTime:2021-02-09 11:01:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-02-09 11:01:42 +0000 UTC,LastTransitionTime:2021-02-09 11:01:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb  9 11:01:43.603: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-4867  4faf8d5a-ec7f-40f8-b8a7-9abc68236390 43494 2 2021-02-09 11:01:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 509cd68b-df1f-4b37-95bb-345250cd03dd 0xc003047767 0xc003047768}] []  [{kube-controller-manager Update apps/v1 2021-02-09 11:01:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"509cd68b-df1f-4b37-95bb-345250cd03dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030477f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb  9 11:01:43.603: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb  9 11:01:43.603: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4867  14e9ca2c-b950-47b9-ba73-31d78673c2da 43503 2 2021-02-09 11:01:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 509cd68b-df1f-4b37-95bb-345250cd03dd 0xc003047657 0xc003047658}] []  [{e2e.test Update apps/v1 2021-02-09 11:01:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-09 11:01:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"509cd68b-df1f-4b37-95bb-345250cd03dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0030476f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  9 11:01:43.603: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-4867  e3137cd1-ae58-4564-a60c-9bee8390b6c9 43433 2 2021-02-09 11:01:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 509cd68b-df1f-4b37-95bb-345250cd03dd 0xc003047867 0xc003047868}] []  [{kube-controller-manager Update apps/v1 2021-02-09 11:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"509cd68b-df1f-4b37-95bb-345250cd03dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030478f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  9 11:01:43.606: INFO: Pod "test-rollover-deployment-668db69979-nqwgk" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-nqwgk test-rollover-deployment-668db69979- deployment-4867  5e968505-e2a3-4a73-828c-ccdddb605c68 43460 0 2021-02-09 11:01:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:10.100.80.16/32 cni.projectcalico.org/podIPs:10.100.80.16/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 4faf8d5a-ec7f-40f8-b8a7-9abc68236390 0xc003047df7 0xc003047df8}] []  [{kube-controller-manager Update v1 2021-02-09 11:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4faf8d5a-ec7f-40f8-b8a7-9abc68236390\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-02-09 11:01:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-02-09 11:01:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.80.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sqffw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sqffw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sqffw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:01:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:01:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:01:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:01:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.176,PodIP:10.100.80.16,StartTime:2021-02-09 11:01:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 11:01:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://f31e5ecfc4bbad05c4020914668fba587156f1f1b5e8348d5c8a3b40045227e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.80.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:01:43.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4867" for this suite.

• [SLOW TEST:23.346 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":249,"skipped":4450,"failed":0}
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:01:43.620: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:01:43.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7940" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":250,"skipped":4450,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:01:43.791: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:01:43.944: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb  9 11:01:43.957: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb  9 11:01:48.980: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb  9 11:01:48.980: INFO: Creating deployment "test-rolling-update-deployment"
Feb  9 11:01:48.987: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb  9 11:01:48.995: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb  9 11:01:51.011: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb  9 11:01:51.015: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb  9 11:01:51.029: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9257  4e7a174e-6378-4f4f-b954-ae65065e3b65 43647 1 2021-02-09 11:01:48 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-02-09 11:01:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-09 11:01:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032c3428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-02-09 11:01:49 +0000 UTC,LastTransitionTime:2021-02-09 11:01:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-02-09 11:01:50 +0000 UTC,LastTransitionTime:2021-02-09 11:01:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb  9 11:01:51.034: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-9257  9ba981ee-239b-4279-9bcb-61c3e547452b 43636 1 2021-02-09 11:01:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 4e7a174e-6378-4f4f-b954-ae65065e3b65 0xc002c399e7 0xc002c399e8}] []  [{kube-controller-manager Update apps/v1 2021-02-09 11:01:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e7a174e-6378-4f4f-b954-ae65065e3b65\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c39a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb  9 11:01:51.034: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb  9 11:01:51.034: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9257  c55709f4-c805-49c4-b57c-1d9f584ad1f3 43646 2 2021-02-09 11:01:43 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 4e7a174e-6378-4f4f-b954-ae65065e3b65 0xc002c39707 0xc002c39708}] []  [{e2e.test Update apps/v1 2021-02-09 11:01:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-09 11:01:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e7a174e-6378-4f4f-b954-ae65065e3b65\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002c398f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  9 11:01:51.038: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-42p7d" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-42p7d test-rolling-update-deployment-6b6bf9df46- deployment-9257  128d0d16-0e9f-42fd-b713-bd6b2e64306a 43634 0 2021-02-09 11:01:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:10.100.239.131/32 cni.projectcalico.org/podIPs:10.100.239.131/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 9ba981ee-239b-4279-9bcb-61c3e547452b 0xc0039ce027 0xc0039ce028}] []  [{calico Update v1 2021-02-09 11:01:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-02-09 11:01:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ba981ee-239b-4279-9bcb-61c3e547452b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 11:01:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.239.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kkghw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kkghw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kkghw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:01:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:01:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:01:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:01:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.248,PodIP:10.100.239.131,StartTime:2021-02-09 11:01:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-02-09 11:01:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://59f5c26b91d5e428c0b6edb99cd5fd74073700e22fb86a87d1c031bfa1aa1ea9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.239.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:01:51.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9257" for this suite.

• [SLOW TEST:7.259 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":251,"skipped":4460,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:01:51.050: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7423
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:01:51.211: INFO: Creating ReplicaSet my-hostname-basic-61e78918-4e2c-4db5-804e-e8071f81209a
Feb  9 11:01:51.224: INFO: Pod name my-hostname-basic-61e78918-4e2c-4db5-804e-e8071f81209a: Found 0 pods out of 1
Feb  9 11:01:56.241: INFO: Pod name my-hostname-basic-61e78918-4e2c-4db5-804e-e8071f81209a: Found 1 pods out of 1
Feb  9 11:01:56.241: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-61e78918-4e2c-4db5-804e-e8071f81209a" is running
Feb  9 11:01:56.245: INFO: Pod "my-hostname-basic-61e78918-4e2c-4db5-804e-e8071f81209a-lg44s" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-09 11:01:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-09 11:01:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-09 11:01:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-09 11:01:51 +0000 UTC Reason: Message:}])
Feb  9 11:01:56.246: INFO: Trying to dial the pod
Feb  9 11:02:01.277: INFO: Controller my-hostname-basic-61e78918-4e2c-4db5-804e-e8071f81209a: Got expected result from replica 1 [my-hostname-basic-61e78918-4e2c-4db5-804e-e8071f81209a-lg44s]: "my-hostname-basic-61e78918-4e2c-4db5-804e-e8071f81209a-lg44s", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:02:01.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7423" for this suite.

• [SLOW TEST:10.242 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":252,"skipped":4461,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:02:01.293: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:02:01.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7318" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":253,"skipped":4472,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:02:01.536: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7256
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:02:01.765: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4b0479b8-65dd-4f1b-b3c0-0be088636fa4", Controller:(*bool)(0xc002d3e7aa), BlockOwnerDeletion:(*bool)(0xc002d3e7ab)}}
Feb  9 11:02:01.775: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0545da33-5a90-4f7c-91fe-cc9af25d0904", Controller:(*bool)(0xc004e278aa), BlockOwnerDeletion:(*bool)(0xc004e278ab)}}
Feb  9 11:02:01.781: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4a9ef5f4-5249-46f4-aa31-ebe380b88718", Controller:(*bool)(0xc00235c32e), BlockOwnerDeletion:(*bool)(0xc00235c32f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:02:06.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7256" for this suite.

• [SLOW TEST:5.287 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":254,"skipped":4483,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:02:06.823: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9510
STEP: creating service affinity-nodeport in namespace services-9510
STEP: creating replication controller affinity-nodeport in namespace services-9510
I0209 11:02:07.008748      25 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-9510, replica count: 3
I0209 11:02:10.059143      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 11:02:10.087: INFO: Creating new exec pod
Feb  9 11:02:13.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9510 exec execpod-affinitybcqlk -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Feb  9 11:02:13.518: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb  9 11:02:13.518: INFO: stdout: ""
Feb  9 11:02:13.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9510 exec execpod-affinitybcqlk -- /bin/sh -x -c nc -zv -t -w 2 10.254.78.171 80'
Feb  9 11:02:13.853: INFO: stderr: "+ nc -zv -t -w 2 10.254.78.171 80\nConnection to 10.254.78.171 80 port [tcp/http] succeeded!\n"
Feb  9 11:02:13.853: INFO: stdout: ""
Feb  9 11:02:13.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9510 exec execpod-affinitybcqlk -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.5 32644'
Feb  9 11:02:14.168: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.5 32644\nConnection to 10.0.0.5 32644 port [tcp/32644] succeeded!\n"
Feb  9 11:02:14.168: INFO: stdout: ""
Feb  9 11:02:14.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9510 exec execpod-affinitybcqlk -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.248 32644'
Feb  9 11:02:14.494: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.248 32644\nConnection to 10.0.0.248 32644 port [tcp/32644] succeeded!\n"
Feb  9 11:02:14.494: INFO: stdout: ""
Feb  9 11:02:14.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9510 exec execpod-affinitybcqlk -- /bin/sh -x -c nc -zv -t -w 2 10.6.0.92 32644'
Feb  9 11:02:14.800: INFO: stderr: "+ nc -zv -t -w 2 10.6.0.92 32644\nConnection to 10.6.0.92 32644 port [tcp/32644] succeeded!\n"
Feb  9 11:02:14.800: INFO: stdout: ""
Feb  9 11:02:14.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9510 exec execpod-affinitybcqlk -- /bin/sh -x -c nc -zv -t -w 2 10.6.0.146 32644'
Feb  9 11:02:15.125: INFO: stderr: "+ nc -zv -t -w 2 10.6.0.146 32644\nConnection to 10.6.0.146 32644 port [tcp/32644] succeeded!\n"
Feb  9 11:02:15.125: INFO: stdout: ""
Feb  9 11:02:15.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-9510 exec execpod-affinitybcqlk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.248:32644/ ; done'
Feb  9 11:02:15.537: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.248:32644/\n"
Feb  9 11:02:15.538: INFO: stdout: "\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl\naffinity-nodeport-pzjnl"
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Received response from host: affinity-nodeport-pzjnl
Feb  9 11:02:15.538: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9510, will wait for the garbage collector to delete the pods
Feb  9 11:02:15.624: INFO: Deleting ReplicationController affinity-nodeport took: 10.483235ms
Feb  9 11:02:15.724: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.280098ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:03:00.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9510" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:53.468 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":255,"skipped":4494,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:03:00.291: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-6196
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:03:01.159: INFO: Checking APIGroup: apiregistration.k8s.io
Feb  9 11:03:01.162: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb  9 11:03:01.162: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.162: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb  9 11:03:01.162: INFO: Checking APIGroup: apps
Feb  9 11:03:01.166: INFO: PreferredVersion.GroupVersion: apps/v1
Feb  9 11:03:01.166: INFO: Versions found [{apps/v1 v1}]
Feb  9 11:03:01.166: INFO: apps/v1 matches apps/v1
Feb  9 11:03:01.166: INFO: Checking APIGroup: events.k8s.io
Feb  9 11:03:01.168: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb  9 11:03:01.168: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.168: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb  9 11:03:01.168: INFO: Checking APIGroup: authentication.k8s.io
Feb  9 11:03:01.171: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb  9 11:03:01.171: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.172: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb  9 11:03:01.172: INFO: Checking APIGroup: authorization.k8s.io
Feb  9 11:03:01.175: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb  9 11:03:01.175: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.175: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb  9 11:03:01.175: INFO: Checking APIGroup: autoscaling
Feb  9 11:03:01.178: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Feb  9 11:03:01.178: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Feb  9 11:03:01.178: INFO: autoscaling/v1 matches autoscaling/v1
Feb  9 11:03:01.178: INFO: Checking APIGroup: batch
Feb  9 11:03:01.180: INFO: PreferredVersion.GroupVersion: batch/v1
Feb  9 11:03:01.181: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1} {batch/v2alpha1 v2alpha1}]
Feb  9 11:03:01.181: INFO: batch/v1 matches batch/v1
Feb  9 11:03:01.181: INFO: Checking APIGroup: certificates.k8s.io
Feb  9 11:03:01.184: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb  9 11:03:01.185: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.185: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb  9 11:03:01.185: INFO: Checking APIGroup: networking.k8s.io
Feb  9 11:03:01.188: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb  9 11:03:01.188: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.188: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb  9 11:03:01.188: INFO: Checking APIGroup: extensions
Feb  9 11:03:01.191: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Feb  9 11:03:01.191: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Feb  9 11:03:01.191: INFO: extensions/v1beta1 matches extensions/v1beta1
Feb  9 11:03:01.191: INFO: Checking APIGroup: policy
Feb  9 11:03:01.194: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Feb  9 11:03:01.194: INFO: Versions found [{policy/v1beta1 v1beta1}]
Feb  9 11:03:01.194: INFO: policy/v1beta1 matches policy/v1beta1
Feb  9 11:03:01.194: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb  9 11:03:01.199: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb  9 11:03:01.201: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1} {rbac.authorization.k8s.io/v1alpha1 v1alpha1}]
Feb  9 11:03:01.201: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb  9 11:03:01.201: INFO: Checking APIGroup: storage.k8s.io
Feb  9 11:03:01.206: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb  9 11:03:01.206: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1} {storage.k8s.io/v1alpha1 v1alpha1}]
Feb  9 11:03:01.206: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb  9 11:03:01.206: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb  9 11:03:01.209: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb  9 11:03:01.209: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.209: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb  9 11:03:01.210: INFO: Checking APIGroup: apiextensions.k8s.io
Feb  9 11:03:01.214: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb  9 11:03:01.214: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.214: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb  9 11:03:01.214: INFO: Checking APIGroup: scheduling.k8s.io
Feb  9 11:03:01.219: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb  9 11:03:01.219: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1} {scheduling.k8s.io/v1alpha1 v1alpha1}]
Feb  9 11:03:01.219: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb  9 11:03:01.220: INFO: Checking APIGroup: coordination.k8s.io
Feb  9 11:03:01.224: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb  9 11:03:01.224: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.224: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb  9 11:03:01.224: INFO: Checking APIGroup: node.k8s.io
Feb  9 11:03:01.228: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb  9 11:03:01.228: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1} {node.k8s.io/v1alpha1 v1alpha1}]
Feb  9 11:03:01.228: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb  9 11:03:01.228: INFO: Checking APIGroup: discovery.k8s.io
Feb  9 11:03:01.231: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Feb  9 11:03:01.231: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.231: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Feb  9 11:03:01.231: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb  9 11:03:01.236: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Feb  9 11:03:01.236: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1} {flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Feb  9 11:03:01.236: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Feb  9 11:03:01.236: INFO: Checking APIGroup: internal.apiserver.k8s.io
Feb  9 11:03:01.238: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
Feb  9 11:03:01.238: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
Feb  9 11:03:01.238: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
Feb  9 11:03:01.238: INFO: Checking APIGroup: crd.projectcalico.org
Feb  9 11:03:01.242: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb  9 11:03:01.242: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb  9 11:03:01.242: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Feb  9 11:03:01.242: INFO: Checking APIGroup: snapshot.storage.k8s.io
Feb  9 11:03:01.246: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1alpha1
Feb  9 11:03:01.246: INFO: Versions found [{snapshot.storage.k8s.io/v1alpha1 v1alpha1}]
Feb  9 11:03:01.246: INFO: snapshot.storage.k8s.io/v1alpha1 matches snapshot.storage.k8s.io/v1alpha1
Feb  9 11:03:01.246: INFO: Checking APIGroup: metrics.k8s.io
Feb  9 11:03:01.249: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb  9 11:03:01.249: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb  9 11:03:01.250: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:03:01.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6196" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":256,"skipped":4499,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:03:01.265: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-75cad4e0-12fe-412b-b875-544e99e4c19e
STEP: Creating a pod to test consume secrets
Feb  9 11:03:01.624: INFO: Waiting up to 5m0s for pod "pod-secrets-539f12da-616a-4faf-bf6d-6f2891fbf4c8" in namespace "secrets-9238" to be "Succeeded or Failed"
Feb  9 11:03:01.631: INFO: Pod "pod-secrets-539f12da-616a-4faf-bf6d-6f2891fbf4c8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.438068ms
Feb  9 11:03:03.643: INFO: Pod "pod-secrets-539f12da-616a-4faf-bf6d-6f2891fbf4c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018927684s
STEP: Saw pod success
Feb  9 11:03:03.644: INFO: Pod "pod-secrets-539f12da-616a-4faf-bf6d-6f2891fbf4c8" satisfied condition "Succeeded or Failed"
Feb  9 11:03:03.647: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-secrets-539f12da-616a-4faf-bf6d-6f2891fbf4c8 container secret-volume-test: <nil>
STEP: delete the pod
Feb  9 11:03:03.724: INFO: Waiting for pod pod-secrets-539f12da-616a-4faf-bf6d-6f2891fbf4c8 to disappear
Feb  9 11:03:03.729: INFO: Pod pod-secrets-539f12da-616a-4faf-bf6d-6f2891fbf4c8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:03:03.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9238" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":257,"skipped":4512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:03:03.746: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7401
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb  9 11:03:03.984: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 11:03:08.499: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:03:24.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7401" for this suite.

• [SLOW TEST:20.443 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":258,"skipped":4557,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:03:24.189: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7554
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-6e235190-407f-4a61-b47d-ec301e552207
STEP: Creating configMap with name cm-test-opt-upd-6deb8b34-f75c-4469-b1df-3dc798246fe3
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-6e235190-407f-4a61-b47d-ec301e552207
STEP: Updating configmap cm-test-opt-upd-6deb8b34-f75c-4469-b1df-3dc798246fe3
STEP: Creating configMap with name cm-test-opt-create-1a6b336b-d988-44a6-84fb-aef4a1344ee8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:04:53.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7554" for this suite.

• [SLOW TEST:88.980 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":259,"skipped":4565,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:04:53.169: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9389
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-3c1f7adb-577e-43a9-977c-3d01956385d1
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:04:53.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9389" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":260,"skipped":4572,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:04:53.341: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb  9 11:04:53.494: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:05:13.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1117" for this suite.

• [SLOW TEST:20.279 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":261,"skipped":4587,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:05:13.619: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:05:24.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-827" for this suite.

• [SLOW TEST:11.266 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":262,"skipped":4598,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:05:24.886: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6017
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Feb  9 11:05:27.078: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6017 PodName:pod-sharedvolume-a0dc75ea-b0d3-4428-985a-0c8c1107d3bc ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:05:27.078: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 11:05:27.258: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:05:27.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6017" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":263,"skipped":4606,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:05:27.275: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:05:27.454: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb  9 11:05:27.476: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:27.480: INFO: Number of nodes with available pods: 0
Feb  9 11:05:27.480: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 11:05:28.539: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:28.544: INFO: Number of nodes with available pods: 0
Feb  9 11:05:28.544: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 11:05:29.487: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:29.492: INFO: Number of nodes with available pods: 1
Feb  9 11:05:29.492: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-1 is running more than one daemon pod
Feb  9 11:05:30.493: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:30.497: INFO: Number of nodes with available pods: 3
Feb  9 11:05:30.497: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb  9 11:05:30.533: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:30.533: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:30.533: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:30.543: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:31.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:31.554: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:31.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:31.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:32.552: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:32.552: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:32.552: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:32.556: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:33.553: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:33.553: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:33.553: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:33.557: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:34.552: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:34.552: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:34.552: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:34.552: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:34.556: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:35.551: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:35.551: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:35.551: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:35.551: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:35.556: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:36.555: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:36.555: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:36.555: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:36.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:36.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:37.555: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:37.555: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:37.555: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:37.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:37.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:38.555: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:38.555: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:38.555: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:38.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:38.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:39.555: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:39.555: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:39.555: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:39.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:39.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:40.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:40.554: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:40.554: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:40.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:40.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:41.555: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:41.556: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:41.556: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:41.556: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:41.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:42.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:42.554: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:42.554: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:42.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:42.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:43.553: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:43.553: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:43.553: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:43.553: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:43.557: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:44.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:44.554: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:44.555: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:44.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:44.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:45.551: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:45.551: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:45.551: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:45.551: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:45.556: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:46.551: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:46.551: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:46.551: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:46.551: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:46.556: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:47.555: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:47.555: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:47.555: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:47.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:47.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:48.553: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:48.553: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:48.553: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:48.553: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:48.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:49.559: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:49.559: INFO: Wrong image for pod: daemon-set-mljzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:49.559: INFO: Pod daemon-set-mljzf is not available
Feb  9 11:05:49.559: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:49.564: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:50.552: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:50.552: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:50.552: INFO: Pod daemon-set-w7vgq is not available
Feb  9 11:05:50.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:51.549: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:51.549: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:51.549: INFO: Pod daemon-set-w7vgq is not available
Feb  9 11:05:51.555: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:52.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:52.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:52.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:53.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:53.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:53.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:54.553: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:54.554: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:05:54.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:54.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:55.550: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:55.551: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:05:55.551: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:55.557: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:56.552: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:56.552: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:05:56.552: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:56.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:57.555: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:57.555: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:05:57.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:57.561: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:58.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:58.554: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:05:58.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:58.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:05:59.555: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:59.556: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:05:59.556: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:05:59.561: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:00.552: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:00.552: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:00.552: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:00.557: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:01.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:01.554: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:01.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:01.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:02.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:02.554: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:02.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:02.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:03.556: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:03.556: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:03.556: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:03.562: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:04.553: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:04.553: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:04.553: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:04.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:05.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:05.554: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:05.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:05.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:06.551: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:06.552: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:06.552: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:06.557: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:07.558: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:07.558: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:07.558: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:07.563: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:08.549: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:08.549: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:08.549: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:08.554: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:09.556: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:09.557: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:09.557: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:09.561: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:10.556: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:10.556: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:10.556: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:10.565: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:11.554: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:11.554: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:11.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:11.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:12.555: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:12.555: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:12.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:12.561: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:13.553: INFO: Wrong image for pod: daemon-set-dllk6. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:13.554: INFO: Pod daemon-set-dllk6 is not available
Feb  9 11:06:13.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:13.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:14.549: INFO: Pod daemon-set-2cpcq is not available
Feb  9 11:06:14.549: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:14.553: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:15.557: INFO: Pod daemon-set-2cpcq is not available
Feb  9 11:06:15.558: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:15.562: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:16.550: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:16.554: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:17.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:17.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:18.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:18.555: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:18.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:19.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:19.555: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:19.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:20.556: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:20.556: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:20.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:21.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:21.554: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:21.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:22.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:22.554: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:22.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:23.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:23.555: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:23.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:24.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:24.555: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:24.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:25.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:25.555: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:25.562: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:26.551: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:26.551: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:26.557: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:27.557: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:27.558: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:27.562: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:28.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:28.554: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:28.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:29.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:29.555: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:29.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:30.554: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:30.554: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:30.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:31.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:31.556: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:31.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:32.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:32.555: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:32.560: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:33.555: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:33.556: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:33.559: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:34.553: INFO: Wrong image for pod: daemon-set-rvxn8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Feb  9 11:06:34.553: INFO: Pod daemon-set-rvxn8 is not available
Feb  9 11:06:34.558: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:35.552: INFO: Pod daemon-set-hpnw7 is not available
Feb  9 11:06:35.556: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb  9 11:06:35.562: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:35.570: INFO: Number of nodes with available pods: 2
Feb  9 11:06:35.570: INFO: Node v1-kube1-20-2-apco5j2qoq5i-node-0 is running more than one daemon pod
Feb  9 11:06:36.577: INFO: DaemonSet pods can't tolerate node v1-kube1-20-2-apco5j2qoq5i-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb  9 11:06:36.581: INFO: Number of nodes with available pods: 3
Feb  9 11:06:36.581: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9553, will wait for the garbage collector to delete the pods
Feb  9 11:06:36.673: INFO: Deleting DaemonSet.extensions daemon-set took: 13.168296ms
Feb  9 11:06:37.673: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000322902s
Feb  9 11:07:34.989: INFO: Number of nodes with available pods: 0
Feb  9 11:07:34.989: INFO: Number of running nodes: 0, number of available pods: 0
Feb  9 11:07:34.992: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45311"},"items":null}

Feb  9 11:07:34.995: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45311"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:07:35.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9553" for this suite.

• [SLOW TEST:127.751 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":264,"skipped":4610,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:07:35.027: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-3342/configmap-test-e968941d-abe2-4cbb-9071-a85831484022
STEP: Creating a pod to test consume configMaps
Feb  9 11:07:35.210: INFO: Waiting up to 5m0s for pod "pod-configmaps-9c37c00d-16ea-41fc-b86c-99195c10e037" in namespace "configmap-3342" to be "Succeeded or Failed"
Feb  9 11:07:35.215: INFO: Pod "pod-configmaps-9c37c00d-16ea-41fc-b86c-99195c10e037": Phase="Pending", Reason="", readiness=false. Elapsed: 4.62949ms
Feb  9 11:07:37.222: INFO: Pod "pod-configmaps-9c37c00d-16ea-41fc-b86c-99195c10e037": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011222139s
STEP: Saw pod success
Feb  9 11:07:37.222: INFO: Pod "pod-configmaps-9c37c00d-16ea-41fc-b86c-99195c10e037" satisfied condition "Succeeded or Failed"
Feb  9 11:07:37.224: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-configmaps-9c37c00d-16ea-41fc-b86c-99195c10e037 container env-test: <nil>
STEP: delete the pod
Feb  9 11:07:37.305: INFO: Waiting for pod pod-configmaps-9c37c00d-16ea-41fc-b86c-99195c10e037 to disappear
Feb  9 11:07:37.308: INFO: Pod pod-configmaps-9c37c00d-16ea-41fc-b86c-99195c10e037 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:07:37.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3342" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4617,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:07:37.316: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Feb  9 11:07:37.466: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb  9 11:07:37.476: INFO: Waiting for terminating namespaces to be deleted...
Feb  9 11:07:37.480: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-0 before test
Feb  9 11:07:37.488: INFO: calico-node-p8hnn from kube-system started at 2021-02-09 08:44:14 +0000 UTC (1 container statuses recorded)
Feb  9 11:07:37.488: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 11:07:37.488: INFO: csi-cinder-nodeplugin-sx9zs from kube-system started at 2021-02-09 08:44:34 +0000 UTC (2 container statuses recorded)
Feb  9 11:07:37.488: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 11:07:37.489: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 11:07:37.489: INFO: kube-dns-autoscaler-f57cd985f-9pfc8 from kube-system started at 2021-02-09 08:44:35 +0000 UTC (1 container statuses recorded)
Feb  9 11:07:37.489: INFO: 	Container autoscaler ready: true, restart count 0
Feb  9 11:07:37.489: INFO: magnum-metrics-server-7ccb6f57c7-2bhgn from kube-system started at 2021-02-09 08:44:36 +0000 UTC (1 container statuses recorded)
Feb  9 11:07:37.489: INFO: 	Container metrics-server ready: true, restart count 0
Feb  9 11:07:37.489: INFO: npd-xtrnn from kube-system started at 2021-02-09 08:44:34 +0000 UTC (1 container statuses recorded)
Feb  9 11:07:37.489: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 11:07:37.489: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-g88r4 from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 11:07:37.489: INFO: 	Container sonobuoy-worker ready: false, restart count 16
Feb  9 11:07:37.489: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  9 11:07:37.489: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-1 before test
Feb  9 11:07:37.496: INFO: calico-node-4xppr from kube-system started at 2021-02-09 08:54:53 +0000 UTC (1 container statuses recorded)
Feb  9 11:07:37.496: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 11:07:37.496: INFO: csi-cinder-nodeplugin-v9fgx from kube-system started at 2021-02-09 10:53:53 +0000 UTC (2 container statuses recorded)
Feb  9 11:07:37.496: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 11:07:37.496: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 11:07:37.496: INFO: npd-hhqz5 from kube-system started at 2021-02-09 08:55:13 +0000 UTC (1 container statuses recorded)
Feb  9 11:07:37.496: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 11:07:37.496: INFO: sonobuoy from sonobuoy started at 2021-02-09 09:07:16 +0000 UTC (1 container statuses recorded)
Feb  9 11:07:37.496: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb  9 11:07:37.496: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-6vf7q from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 11:07:37.496: INFO: 	Container sonobuoy-worker ready: false, restart count 16
Feb  9 11:07:37.496: INFO: 	Container systemd-logs ready: true, restart count 0
Feb  9 11:07:37.496: INFO: 
Logging pods the apiserver thinks is on node v1-kube1-20-2-apco5j2qoq5i-node-2 before test
Feb  9 11:07:37.506: INFO: calico-node-6klgd from kube-system started at 2021-02-09 08:55:40 +0000 UTC (1 container statuses recorded)
Feb  9 11:07:37.506: INFO: 	Container calico-node ready: true, restart count 0
Feb  9 11:07:37.506: INFO: csi-cinder-nodeplugin-b6x2m from kube-system started at 2021-02-09 08:56:00 +0000 UTC (2 container statuses recorded)
Feb  9 11:07:37.506: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb  9 11:07:37.506: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb  9 11:07:37.506: INFO: npd-9f7sk from kube-system started at 2021-02-09 08:56:00 +0000 UTC (1 container statuses recorded)
Feb  9 11:07:37.506: INFO: 	Container node-problem-detector ready: true, restart count 0
Feb  9 11:07:37.506: INFO: sonobuoy-e2e-job-cba60cb6bac9483e from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 11:07:37.506: INFO: 	Container e2e ready: true, restart count 0
Feb  9 11:07:37.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb  9 11:07:37.507: INFO: sonobuoy-systemd-logs-daemon-set-abb31ddc9084467a-kqw79 from sonobuoy started at 2021-02-09 09:07:23 +0000 UTC (2 container statuses recorded)
Feb  9 11:07:37.507: INFO: 	Container sonobuoy-worker ready: false, restart count 16
Feb  9 11:07:37.507: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d3c7aa1f-7fe1-4042-9a29-7c9d89884805 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-d3c7aa1f-7fe1-4042-9a29-7c9d89884805 off the node v1-kube1-20-2-apco5j2qoq5i-node-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d3c7aa1f-7fe1-4042-9a29-7c9d89884805
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:07:41.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1249" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":266,"skipped":4639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:07:41.637: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5228
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:07:41.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5228" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":267,"skipped":4665,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:07:41.836: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2193
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:07:41.997: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb  9 11:07:45.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-2193 --namespace=crd-publish-openapi-2193 create -f -'
Feb  9 11:07:46.771: INFO: stderr: ""
Feb  9 11:07:46.771: INFO: stdout: "e2e-test-crd-publish-openapi-7531-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb  9 11:07:46.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-2193 --namespace=crd-publish-openapi-2193 delete e2e-test-crd-publish-openapi-7531-crds test-cr'
Feb  9 11:07:46.911: INFO: stderr: ""
Feb  9 11:07:46.911: INFO: stdout: "e2e-test-crd-publish-openapi-7531-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb  9 11:07:46.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-2193 --namespace=crd-publish-openapi-2193 apply -f -'
Feb  9 11:07:47.223: INFO: stderr: ""
Feb  9 11:07:47.223: INFO: stdout: "e2e-test-crd-publish-openapi-7531-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb  9 11:07:47.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-2193 --namespace=crd-publish-openapi-2193 delete e2e-test-crd-publish-openapi-7531-crds test-cr'
Feb  9 11:07:47.361: INFO: stderr: ""
Feb  9 11:07:47.361: INFO: stdout: "e2e-test-crd-publish-openapi-7531-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb  9 11:07:47.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=crd-publish-openapi-2193 explain e2e-test-crd-publish-openapi-7531-crds'
Feb  9 11:07:47.691: INFO: stderr: ""
Feb  9 11:07:47.691: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7531-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:07:51.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2193" for this suite.

• [SLOW TEST:9.764 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":268,"skipped":4666,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:07:51.601: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 11:07:51.771: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f307c14e-f206-4b04-86b1-ca8104c93275" in namespace "projected-4436" to be "Succeeded or Failed"
Feb  9 11:07:51.782: INFO: Pod "downwardapi-volume-f307c14e-f206-4b04-86b1-ca8104c93275": Phase="Pending", Reason="", readiness=false. Elapsed: 10.92725ms
Feb  9 11:07:53.796: INFO: Pod "downwardapi-volume-f307c14e-f206-4b04-86b1-ca8104c93275": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025004818s
STEP: Saw pod success
Feb  9 11:07:53.796: INFO: Pod "downwardapi-volume-f307c14e-f206-4b04-86b1-ca8104c93275" satisfied condition "Succeeded or Failed"
Feb  9 11:07:53.800: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod downwardapi-volume-f307c14e-f206-4b04-86b1-ca8104c93275 container client-container: <nil>
STEP: delete the pod
Feb  9 11:07:53.872: INFO: Waiting for pod downwardapi-volume-f307c14e-f206-4b04-86b1-ca8104c93275 to disappear
Feb  9 11:07:53.879: INFO: Pod downwardapi-volume-f307c14e-f206-4b04-86b1-ca8104c93275 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:07:53.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4436" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:07:53.892: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-a02a9081-5f49-436a-b623-6ed94e496783 in namespace container-probe-7592
Feb  9 11:07:56.087: INFO: Started pod liveness-a02a9081-5f49-436a-b623-6ed94e496783 in namespace container-probe-7592
STEP: checking the pod's current state and verifying that restartCount is present
Feb  9 11:07:56.089: INFO: Initial restart count of pod liveness-a02a9081-5f49-436a-b623-6ed94e496783 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:11:57.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7592" for this suite.

• [SLOW TEST:243.964 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":270,"skipped":4701,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:11:57.856: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8011
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:12:02.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8011" for this suite.

• [SLOW TEST:5.021 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":271,"skipped":4707,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:12:02.878: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-ba97e5f9-359b-45e3-826f-deef881cb7e5
STEP: Creating a pod to test consume secrets
Feb  9 11:12:03.062: INFO: Waiting up to 5m0s for pod "pod-secrets-0889f3bc-2b00-4154-ba52-1391c21fdc32" in namespace "secrets-3214" to be "Succeeded or Failed"
Feb  9 11:12:03.068: INFO: Pod "pod-secrets-0889f3bc-2b00-4154-ba52-1391c21fdc32": Phase="Pending", Reason="", readiness=false. Elapsed: 5.729148ms
Feb  9 11:12:05.083: INFO: Pod "pod-secrets-0889f3bc-2b00-4154-ba52-1391c21fdc32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020855114s
Feb  9 11:12:07.097: INFO: Pod "pod-secrets-0889f3bc-2b00-4154-ba52-1391c21fdc32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034909022s
STEP: Saw pod success
Feb  9 11:12:07.097: INFO: Pod "pod-secrets-0889f3bc-2b00-4154-ba52-1391c21fdc32" satisfied condition "Succeeded or Failed"
Feb  9 11:12:07.099: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-secrets-0889f3bc-2b00-4154-ba52-1391c21fdc32 container secret-env-test: <nil>
STEP: delete the pod
Feb  9 11:12:07.230: INFO: Waiting for pod pod-secrets-0889f3bc-2b00-4154-ba52-1391c21fdc32 to disappear
Feb  9 11:12:07.235: INFO: Pod pod-secrets-0889f3bc-2b00-4154-ba52-1391c21fdc32 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:12:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3214" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4745,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:12:07.248: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-182862d6-e484-404a-a1f3-6d3bcb2cdf92
STEP: Creating a pod to test consume secrets
Feb  9 11:12:07.437: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c2b34640-1382-4def-9fe1-b5487a078ac9" in namespace "projected-7172" to be "Succeeded or Failed"
Feb  9 11:12:07.447: INFO: Pod "pod-projected-secrets-c2b34640-1382-4def-9fe1-b5487a078ac9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.391077ms
Feb  9 11:12:09.459: INFO: Pod "pod-projected-secrets-c2b34640-1382-4def-9fe1-b5487a078ac9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021451608s
Feb  9 11:12:11.475: INFO: Pod "pod-projected-secrets-c2b34640-1382-4def-9fe1-b5487a078ac9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037091759s
STEP: Saw pod success
Feb  9 11:12:11.475: INFO: Pod "pod-projected-secrets-c2b34640-1382-4def-9fe1-b5487a078ac9" satisfied condition "Succeeded or Failed"
Feb  9 11:12:11.478: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-projected-secrets-c2b34640-1382-4def-9fe1-b5487a078ac9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  9 11:12:11.507: INFO: Waiting for pod pod-projected-secrets-c2b34640-1382-4def-9fe1-b5487a078ac9 to disappear
Feb  9 11:12:11.516: INFO: Pod pod-projected-secrets-c2b34640-1382-4def-9fe1-b5487a078ac9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:12:11.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7172" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4745,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:12:11.529: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb  9 11:12:11.695: INFO: Waiting up to 5m0s for pod "pod-d17e5e5f-8a9e-4f18-802e-48466e1bffc9" in namespace "emptydir-4196" to be "Succeeded or Failed"
Feb  9 11:12:11.701: INFO: Pod "pod-d17e5e5f-8a9e-4f18-802e-48466e1bffc9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.704387ms
Feb  9 11:12:13.721: INFO: Pod "pod-d17e5e5f-8a9e-4f18-802e-48466e1bffc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026107657s
Feb  9 11:12:15.734: INFO: Pod "pod-d17e5e5f-8a9e-4f18-802e-48466e1bffc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039235265s
STEP: Saw pod success
Feb  9 11:12:15.734: INFO: Pod "pod-d17e5e5f-8a9e-4f18-802e-48466e1bffc9" satisfied condition "Succeeded or Failed"
Feb  9 11:12:15.736: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-d17e5e5f-8a9e-4f18-802e-48466e1bffc9 container test-container: <nil>
STEP: delete the pod
Feb  9 11:12:15.770: INFO: Waiting for pod pod-d17e5e5f-8a9e-4f18-802e-48466e1bffc9 to disappear
Feb  9 11:12:15.773: INFO: Pod pod-d17e5e5f-8a9e-4f18-802e-48466e1bffc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:12:15.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4196" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4765,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:12:15.795: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9560
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Feb  9 11:12:15.965: INFO: Waiting up to 5m0s for pod "downward-api-9a63c723-80ff-44f8-aad2-069b75731e0e" in namespace "downward-api-9560" to be "Succeeded or Failed"
Feb  9 11:12:15.970: INFO: Pod "downward-api-9a63c723-80ff-44f8-aad2-069b75731e0e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146834ms
Feb  9 11:12:17.985: INFO: Pod "downward-api-9a63c723-80ff-44f8-aad2-069b75731e0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020113822s
STEP: Saw pod success
Feb  9 11:12:17.985: INFO: Pod "downward-api-9a63c723-80ff-44f8-aad2-069b75731e0e" satisfied condition "Succeeded or Failed"
Feb  9 11:12:17.988: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downward-api-9a63c723-80ff-44f8-aad2-069b75731e0e container dapi-container: <nil>
STEP: delete the pod
Feb  9 11:12:18.019: INFO: Waiting for pod downward-api-9a63c723-80ff-44f8-aad2-069b75731e0e to disappear
Feb  9 11:12:18.026: INFO: Pod downward-api-9a63c723-80ff-44f8-aad2-069b75731e0e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:12:18.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9560" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4841,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:12:18.043: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Feb  9 11:12:18.213: INFO: Waiting up to 5m0s for pod "var-expansion-07f132ec-8aa2-456c-b51c-46bc5eaffbaa" in namespace "var-expansion-6753" to be "Succeeded or Failed"
Feb  9 11:12:18.221: INFO: Pod "var-expansion-07f132ec-8aa2-456c-b51c-46bc5eaffbaa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.19849ms
Feb  9 11:12:20.234: INFO: Pod "var-expansion-07f132ec-8aa2-456c-b51c-46bc5eaffbaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021236508s
STEP: Saw pod success
Feb  9 11:12:20.235: INFO: Pod "var-expansion-07f132ec-8aa2-456c-b51c-46bc5eaffbaa" satisfied condition "Succeeded or Failed"
Feb  9 11:12:20.238: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod var-expansion-07f132ec-8aa2-456c-b51c-46bc5eaffbaa container dapi-container: <nil>
STEP: delete the pod
Feb  9 11:12:20.273: INFO: Waiting for pod var-expansion-07f132ec-8aa2-456c-b51c-46bc5eaffbaa to disappear
Feb  9 11:12:20.278: INFO: Pod var-expansion-07f132ec-8aa2-456c-b51c-46bc5eaffbaa no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:12:20.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6753" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4868,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:12:20.288: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-2786
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Feb  9 11:12:20.444: INFO: Major version: 1
STEP: Confirm minor version
Feb  9 11:12:20.444: INFO: cleanMinorVersion: 20
Feb  9 11:12:20.444: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:12:20.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2786" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":277,"skipped":4874,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:12:20.458: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 11:12:20.648: INFO: Waiting up to 5m0s for pod "downwardapi-volume-efd85e74-397c-40fb-a0e2-4aa25475ccce" in namespace "downward-api-9215" to be "Succeeded or Failed"
Feb  9 11:12:20.655: INFO: Pod "downwardapi-volume-efd85e74-397c-40fb-a0e2-4aa25475ccce": Phase="Pending", Reason="", readiness=false. Elapsed: 6.921095ms
Feb  9 11:12:22.671: INFO: Pod "downwardapi-volume-efd85e74-397c-40fb-a0e2-4aa25475ccce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023217679s
STEP: Saw pod success
Feb  9 11:12:22.671: INFO: Pod "downwardapi-volume-efd85e74-397c-40fb-a0e2-4aa25475ccce" satisfied condition "Succeeded or Failed"
Feb  9 11:12:22.675: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downwardapi-volume-efd85e74-397c-40fb-a0e2-4aa25475ccce container client-container: <nil>
STEP: delete the pod
Feb  9 11:12:22.707: INFO: Waiting for pod downwardapi-volume-efd85e74-397c-40fb-a0e2-4aa25475ccce to disappear
Feb  9 11:12:22.710: INFO: Pod downwardapi-volume-efd85e74-397c-40fb-a0e2-4aa25475ccce no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:12:22.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9215" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":278,"skipped":4884,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:12:22.721: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5405
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-c75a9925-f8f5-4a13-b6c9-bb8a005ea3c7
STEP: Creating secret with name s-test-opt-upd-c6e6d8c5-6b79-4954-af27-9e2c44002095
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c75a9925-f8f5-4a13-b6c9-bb8a005ea3c7
STEP: Updating secret s-test-opt-upd-c6e6d8c5-6b79-4954-af27-9e2c44002095
STEP: Creating secret with name s-test-opt-create-047b5ae5-4069-4950-aa0b-21560aef5d04
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:13:49.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5405" for this suite.

• [SLOW TEST:86.839 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":279,"skipped":4902,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:13:49.560: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb  9 11:13:49.731: INFO: Waiting up to 5m0s for pod "pod-3a707daf-2d17-4f30-be15-e4ba5666d4a8" in namespace "emptydir-9575" to be "Succeeded or Failed"
Feb  9 11:13:49.736: INFO: Pod "pod-3a707daf-2d17-4f30-be15-e4ba5666d4a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525601ms
Feb  9 11:13:51.745: INFO: Pod "pod-3a707daf-2d17-4f30-be15-e4ba5666d4a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013573117s
Feb  9 11:13:53.761: INFO: Pod "pod-3a707daf-2d17-4f30-be15-e4ba5666d4a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029729106s
STEP: Saw pod success
Feb  9 11:13:53.761: INFO: Pod "pod-3a707daf-2d17-4f30-be15-e4ba5666d4a8" satisfied condition "Succeeded or Failed"
Feb  9 11:13:53.765: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-3a707daf-2d17-4f30-be15-e4ba5666d4a8 container test-container: <nil>
STEP: delete the pod
Feb  9 11:13:53.899: INFO: Waiting for pod pod-3a707daf-2d17-4f30-be15-e4ba5666d4a8 to disappear
Feb  9 11:13:53.902: INFO: Pod pod-3a707daf-2d17-4f30-be15-e4ba5666d4a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:13:53.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9575" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":280,"skipped":4909,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:13:53.914: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:14:02.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1716" for this suite.

• [SLOW TEST:8.204 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":281,"skipped":4919,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:14:02.120: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7492
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:14:02.278: INFO: Creating deployment "test-recreate-deployment"
Feb  9 11:14:02.284: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb  9 11:14:02.301: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb  9 11:14:04.318: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb  9 11:14:04.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748466042, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748466042, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748466042, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748466042, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb  9 11:14:06.330: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb  9 11:14:06.344: INFO: Updating deployment test-recreate-deployment
Feb  9 11:14:06.344: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Feb  9 11:14:06.454: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7492  31e37650-9b97-4a2a-af8e-df08d60a23cf 47259 2 2021-02-09 11:14:02 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-02-09 11:14:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-02-09 11:14:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043a5fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-02-09 11:14:06 +0000 UTC,LastTransitionTime:2021-02-09 11:14:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-02-09 11:14:06 +0000 UTC,LastTransitionTime:2021-02-09 11:14:02 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb  9 11:14:06.458: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-7492  b63f7c23-7e41-448e-9d7a-462476ba5a6d 47257 1 2021-02-09 11:14:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 31e37650-9b97-4a2a-af8e-df08d60a23cf 0xc001536cd0 0xc001536cd1}] []  [{kube-controller-manager Update apps/v1 2021-02-09 11:14:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31e37650-9b97-4a2a-af8e-df08d60a23cf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001536fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  9 11:14:06.458: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb  9 11:14:06.459: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-7492  563925e1-16e6-46d0-af22-1e69691f9e74 47247 2 2021-02-09 11:14:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 31e37650-9b97-4a2a-af8e-df08d60a23cf 0xc001536ad7 0xc001536ad8}] []  [{kube-controller-manager Update apps/v1 2021-02-09 11:14:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31e37650-9b97-4a2a-af8e-df08d60a23cf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001536c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb  9 11:14:06.464: INFO: Pod "test-recreate-deployment-f79dd4667-rcr9b" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-rcr9b test-recreate-deployment-f79dd4667- deployment-7492  73b2f0aa-24f1-433b-b4fe-e5464040ae54 47258 0 2021-02-09 11:14:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 b63f7c23-7e41-448e-9d7a-462476ba5a6d 0xc002c389f0 0xc002c389f1}] []  [{kube-controller-manager Update v1 2021-02-09 11:14:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b63f7c23-7e41-448e-9d7a-462476ba5a6d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-09 11:14:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8k9zc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8k9zc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8k9zc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:v1-kube1-20-2-apco5j2qoq5i-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:14:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:14:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:14:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-09 11:14:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2021-02-09 11:14:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:14:06.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7492" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":282,"skipped":4921,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:14:06.476: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2859
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-5003f029-d6d5-49ec-880e-7778e5c9504c
STEP: Creating a pod to test consume configMaps
Feb  9 11:14:06.657: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-729523a2-e4cf-4fbb-a619-0a49b4246422" in namespace "projected-2859" to be "Succeeded or Failed"
Feb  9 11:14:06.661: INFO: Pod "pod-projected-configmaps-729523a2-e4cf-4fbb-a619-0a49b4246422": Phase="Pending", Reason="", readiness=false. Elapsed: 4.57441ms
Feb  9 11:14:08.677: INFO: Pod "pod-projected-configmaps-729523a2-e4cf-4fbb-a619-0a49b4246422": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02064049s
STEP: Saw pod success
Feb  9 11:14:08.678: INFO: Pod "pod-projected-configmaps-729523a2-e4cf-4fbb-a619-0a49b4246422" satisfied condition "Succeeded or Failed"
Feb  9 11:14:08.682: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod pod-projected-configmaps-729523a2-e4cf-4fbb-a619-0a49b4246422 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 11:14:08.710: INFO: Waiting for pod pod-projected-configmaps-729523a2-e4cf-4fbb-a619-0a49b4246422 to disappear
Feb  9 11:14:08.714: INFO: Pod pod-projected-configmaps-729523a2-e4cf-4fbb-a619-0a49b4246422 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:14:08.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2859" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4923,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:14:08.724: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1258
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Feb  9 11:14:12.979: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1258 PodName:var-expansion-75f0b797-4d98-4611-beb9-6a5dc44b5835 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:14:12.979: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: test for file in mounted path
Feb  9 11:14:13.177: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1258 PodName:var-expansion-75f0b797-4d98-4611-beb9-6a5dc44b5835 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:14:13.177: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: updating the annotation value
Feb  9 11:14:13.861: INFO: Successfully updated pod "var-expansion-75f0b797-4d98-4611-beb9-6a5dc44b5835"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Feb  9 11:14:13.866: INFO: Deleting pod "var-expansion-75f0b797-4d98-4611-beb9-6a5dc44b5835" in namespace "var-expansion-1258"
Feb  9 11:14:13.872: INFO: Wait up to 5m0s for pod "var-expansion-75f0b797-4d98-4611-beb9-6a5dc44b5835" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:15:35.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1258" for this suite.

• [SLOW TEST:87.183 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":284,"skipped":4928,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:15:35.907: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5930
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5930
I0209 11:15:36.118171      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5930, replica count: 2
Feb  9 11:15:39.168: INFO: Creating new exec pod
I0209 11:15:39.168596      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 11:15:44.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-5930 exec execpodkblt2 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb  9 11:15:44.606: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb  9 11:15:44.606: INFO: stdout: ""
Feb  9 11:15:44.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=services-5930 exec execpodkblt2 -- /bin/sh -x -c nc -zv -t -w 2 10.254.241.93 80'
Feb  9 11:15:44.941: INFO: stderr: "+ nc -zv -t -w 2 10.254.241.93 80\nConnection to 10.254.241.93 80 port [tcp/http] succeeded!\n"
Feb  9 11:15:44.941: INFO: stdout: ""
Feb  9 11:15:44.941: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:15:44.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5930" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:9.074 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":285,"skipped":4928,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:15:44.982: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Feb  9 11:15:45.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5497 create -f -'
Feb  9 11:15:45.597: INFO: stderr: ""
Feb  9 11:15:45.597: INFO: stdout: "pod/pause created\n"
Feb  9 11:15:45.597: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb  9 11:15:45.597: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5497" to be "running and ready"
Feb  9 11:15:45.606: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.443788ms
Feb  9 11:15:47.620: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022626729s
Feb  9 11:15:49.637: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.039879066s
Feb  9 11:15:49.637: INFO: Pod "pause" satisfied condition "running and ready"
Feb  9 11:15:49.637: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Feb  9 11:15:49.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5497 label pods pause testing-label=testing-label-value'
Feb  9 11:15:49.778: INFO: stderr: ""
Feb  9 11:15:49.779: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb  9 11:15:49.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5497 get pod pause -L testing-label'
Feb  9 11:15:49.912: INFO: stderr: ""
Feb  9 11:15:49.912: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb  9 11:15:49.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5497 label pods pause testing-label-'
Feb  9 11:15:50.051: INFO: stderr: ""
Feb  9 11:15:50.051: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb  9 11:15:50.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5497 get pod pause -L testing-label'
Feb  9 11:15:50.169: INFO: stderr: ""
Feb  9 11:15:50.169: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Feb  9 11:15:50.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5497 delete --grace-period=0 --force -f -'
Feb  9 11:15:50.337: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb  9 11:15:50.337: INFO: stdout: "pod \"pause\" force deleted\n"
Feb  9 11:15:50.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5497 get rc,svc -l name=pause --no-headers'
Feb  9 11:15:50.488: INFO: stderr: "No resources found in kubectl-5497 namespace.\n"
Feb  9 11:15:50.488: INFO: stdout: ""
Feb  9 11:15:50.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-5497 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb  9 11:15:50.688: INFO: stderr: ""
Feb  9 11:15:50.688: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:15:50.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5497" for this suite.

• [SLOW TEST:5.717 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":286,"skipped":4951,"failed":0}
SSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:15:50.699: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-4846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Feb  9 11:15:50.894: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Feb  9 11:15:50.902: INFO: starting watch
STEP: patching
STEP: updating
Feb  9 11:15:50.920: INFO: waiting for watch events with expected annotations
Feb  9 11:15:50.920: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:15:50.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4846" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":287,"skipped":4954,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:15:51.000: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-0f784565-cd2e-4bb9-96f9-7ae2b9944aed
Feb  9 11:15:51.176: INFO: Pod name my-hostname-basic-0f784565-cd2e-4bb9-96f9-7ae2b9944aed: Found 0 pods out of 1
Feb  9 11:15:56.191: INFO: Pod name my-hostname-basic-0f784565-cd2e-4bb9-96f9-7ae2b9944aed: Found 1 pods out of 1
Feb  9 11:15:56.191: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0f784565-cd2e-4bb9-96f9-7ae2b9944aed" are running
Feb  9 11:15:56.195: INFO: Pod "my-hostname-basic-0f784565-cd2e-4bb9-96f9-7ae2b9944aed-wr2kx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-09 11:15:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-09 11:15:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-09 11:15:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-02-09 11:15:51 +0000 UTC Reason: Message:}])
Feb  9 11:15:56.196: INFO: Trying to dial the pod
Feb  9 11:16:01.214: INFO: Controller my-hostname-basic-0f784565-cd2e-4bb9-96f9-7ae2b9944aed: Got expected result from replica 1 [my-hostname-basic-0f784565-cd2e-4bb9-96f9-7ae2b9944aed-wr2kx]: "my-hostname-basic-0f784565-cd2e-4bb9-96f9-7ae2b9944aed-wr2kx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:16:01.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1344" for this suite.

• [SLOW TEST:10.228 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":288,"skipped":4963,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:16:01.231: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3487
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:16:01.395: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:16:02.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3487" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":289,"skipped":4965,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:16:02.472: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb  9 11:16:05.219: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8495 pod-service-account-69e84c5d-9638-4241-a393-b4f8ebbe2554 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb  9 11:16:05.518: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8495 pod-service-account-69e84c5d-9638-4241-a393-b4f8ebbe2554 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb  9 11:16:05.830: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8495 pod-service-account-69e84c5d-9638-4241-a393-b4f8ebbe2554 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:16:06.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8495" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":290,"skipped":5027,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:16:06.193: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 11:16:06.381: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e56b119-7670-42cf-8f36-ffd82ecd3e39" in namespace "downward-api-4608" to be "Succeeded or Failed"
Feb  9 11:16:06.388: INFO: Pod "downwardapi-volume-7e56b119-7670-42cf-8f36-ffd82ecd3e39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.654307ms
Feb  9 11:16:08.395: INFO: Pod "downwardapi-volume-7e56b119-7670-42cf-8f36-ffd82ecd3e39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013463177s
STEP: Saw pod success
Feb  9 11:16:08.395: INFO: Pod "downwardapi-volume-7e56b119-7670-42cf-8f36-ffd82ecd3e39" satisfied condition "Succeeded or Failed"
Feb  9 11:16:08.397: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod downwardapi-volume-7e56b119-7670-42cf-8f36-ffd82ecd3e39 container client-container: <nil>
STEP: delete the pod
Feb  9 11:16:08.614: INFO: Waiting for pod downwardapi-volume-7e56b119-7670-42cf-8f36-ffd82ecd3e39 to disappear
Feb  9 11:16:08.622: INFO: Pod downwardapi-volume-7e56b119-7670-42cf-8f36-ffd82ecd3e39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:16:08.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4608" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":291,"skipped":5028,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:16:08.632: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-7439
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  9 11:16:08.780: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb  9 11:16:08.835: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  9 11:16:10.844: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  9 11:16:12.849: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 11:16:14.847: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 11:16:16.847: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 11:16:18.851: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 11:16:20.842: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb  9 11:16:20.849: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb  9 11:16:22.863: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb  9 11:16:24.866: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb  9 11:16:26.865: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb  9 11:16:28.864: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb  9 11:16:28.870: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb  9 11:16:32.921: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb  9 11:16:32.922: INFO: Going to poll 10.100.239.148 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Feb  9 11:16:32.925: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.239.148:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7439 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:16:32.925: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 11:16:33.111: INFO: Found all 1 expected endpoints: [netserver-0]
Feb  9 11:16:33.112: INFO: Going to poll 10.100.46.157 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Feb  9 11:16:33.117: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.46.157:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7439 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:16:33.118: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 11:16:33.293: INFO: Found all 1 expected endpoints: [netserver-1]
Feb  9 11:16:33.293: INFO: Going to poll 10.100.80.26 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Feb  9 11:16:33.306: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.80.26:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7439 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:16:33.306: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 11:16:33.504: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:16:33.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7439" for this suite.

• [SLOW TEST:24.888 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":5036,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:16:33.521: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7877
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-361028a6-4f85-4897-b68e-79cdb3c4a2e6
STEP: Creating a pod to test consume configMaps
Feb  9 11:16:33.722: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d1265340-7f1d-4155-9a6b-b526b3490517" in namespace "projected-7877" to be "Succeeded or Failed"
Feb  9 11:16:33.727: INFO: Pod "pod-projected-configmaps-d1265340-7f1d-4155-9a6b-b526b3490517": Phase="Pending", Reason="", readiness=false. Elapsed: 4.815289ms
Feb  9 11:16:35.741: INFO: Pod "pod-projected-configmaps-d1265340-7f1d-4155-9a6b-b526b3490517": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018705281s
STEP: Saw pod success
Feb  9 11:16:35.741: INFO: Pod "pod-projected-configmaps-d1265340-7f1d-4155-9a6b-b526b3490517" satisfied condition "Succeeded or Failed"
Feb  9 11:16:35.881: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-2 pod pod-projected-configmaps-d1265340-7f1d-4155-9a6b-b526b3490517 container agnhost-container: <nil>
STEP: delete the pod
Feb  9 11:16:36.025: INFO: Waiting for pod pod-projected-configmaps-d1265340-7f1d-4155-9a6b-b526b3490517 to disappear
Feb  9 11:16:36.028: INFO: Pod pod-projected-configmaps-d1265340-7f1d-4155-9a6b-b526b3490517 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:16:36.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7877" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":293,"skipped":5050,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:16:36.038: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7336
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4fsqm in namespace proxy-7336
I0209 11:16:36.222624      25 runners.go:190] Created replication controller with name: proxy-service-4fsqm, namespace: proxy-7336, replica count: 1
I0209 11:16:37.273039      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 11:16:38.273513      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0209 11:16:39.273703      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0209 11:16:40.273945      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0209 11:16:41.274312      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0209 11:16:42.274782      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0209 11:16:43.275166      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0209 11:16:44.275414      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0209 11:16:45.275825      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0209 11:16:46.276131      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0209 11:16:47.276446      25 runners.go:190] proxy-service-4fsqm Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb  9 11:16:47.311: INFO: setup took 11.113592531s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb  9 11:16:47.418: INFO: (0) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 107.488216ms)
Feb  9 11:16:47.418: INFO: (0) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 107.187028ms)
Feb  9 11:16:47.424: INFO: (0) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 112.805289ms)
Feb  9 11:16:47.424: INFO: (0) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 112.749708ms)
Feb  9 11:16:47.425: INFO: (0) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 113.011896ms)
Feb  9 11:16:47.425: INFO: (0) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 112.839188ms)
Feb  9 11:16:47.425: INFO: (0) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 113.526451ms)
Feb  9 11:16:47.425: INFO: (0) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 113.296723ms)
Feb  9 11:16:47.425: INFO: (0) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 113.049295ms)
Feb  9 11:16:47.425: INFO: (0) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 113.107435ms)
Feb  9 11:16:47.432: INFO: (0) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 120.942198ms)
Feb  9 11:16:47.432: INFO: (0) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 120.8754ms)
Feb  9 11:16:47.435: INFO: (0) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 123.964406ms)
Feb  9 11:16:47.436: INFO: (0) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 124.893085ms)
Feb  9 11:16:47.436: INFO: (0) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 124.507691ms)
Feb  9 11:16:47.438: INFO: (0) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 126.40405ms)
Feb  9 11:16:47.444: INFO: (1) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 5.757878ms)
Feb  9 11:16:47.445: INFO: (1) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 6.5145ms)
Feb  9 11:16:47.447: INFO: (1) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 8.25105ms)
Feb  9 11:16:47.447: INFO: (1) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 8.783774ms)
Feb  9 11:16:47.448: INFO: (1) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 8.797304ms)
Feb  9 11:16:47.448: INFO: (1) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 8.656875ms)
Feb  9 11:16:47.448: INFO: (1) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 9.353988ms)
Feb  9 11:16:47.450: INFO: (1) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 11.254707ms)
Feb  9 11:16:47.449: INFO: (1) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 10.534245ms)
Feb  9 11:16:47.450: INFO: (1) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 11.240307ms)
Feb  9 11:16:47.450: INFO: (1) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 11.674042ms)
Feb  9 11:16:47.450: INFO: (1) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 11.663802ms)
Feb  9 11:16:47.449: INFO: (1) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 9.957552ms)
Feb  9 11:16:47.451: INFO: (1) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 12.052649ms)
Feb  9 11:16:47.451: INFO: (1) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 11.794522ms)
Feb  9 11:16:47.451: INFO: (1) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 11.9046ms)
Feb  9 11:16:47.459: INFO: (2) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 7.920523ms)
Feb  9 11:16:47.459: INFO: (2) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 6.745847ms)
Feb  9 11:16:47.459: INFO: (2) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 6.560298ms)
Feb  9 11:16:47.459: INFO: (2) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 6.619417ms)
Feb  9 11:16:47.459: INFO: (2) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 7.086433ms)
Feb  9 11:16:47.460: INFO: (2) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 7.472018ms)
Feb  9 11:16:47.461: INFO: (2) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 9.798673ms)
Feb  9 11:16:47.461: INFO: (2) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 8.940743ms)
Feb  9 11:16:47.461: INFO: (2) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 9.327879ms)
Feb  9 11:16:47.461: INFO: (2) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 8.651615ms)
Feb  9 11:16:47.461: INFO: (2) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 10.06303ms)
Feb  9 11:16:47.461: INFO: (2) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 8.600586ms)
Feb  9 11:16:47.461: INFO: (2) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 8.690825ms)
Feb  9 11:16:47.461: INFO: (2) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 8.835883ms)
Feb  9 11:16:47.461: INFO: (2) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 9.754084ms)
Feb  9 11:16:47.461: INFO: (2) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 8.456888ms)
Feb  9 11:16:47.466: INFO: (3) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 3.873497ms)
Feb  9 11:16:47.466: INFO: (3) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 3.915297ms)
Feb  9 11:16:47.469: INFO: (3) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 7.110613ms)
Feb  9 11:16:47.469: INFO: (3) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 7.441989ms)
Feb  9 11:16:47.469: INFO: (3) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 7.34135ms)
Feb  9 11:16:47.469: INFO: (3) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 7.390139ms)
Feb  9 11:16:47.471: INFO: (3) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 8.791894ms)
Feb  9 11:16:47.471: INFO: (3) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 9.308739ms)
Feb  9 11:16:47.472: INFO: (3) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 9.252218ms)
Feb  9 11:16:47.472: INFO: (3) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 9.280779ms)
Feb  9 11:16:47.472: INFO: (3) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 9.351778ms)
Feb  9 11:16:47.472: INFO: (3) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 10.032931ms)
Feb  9 11:16:47.472: INFO: (3) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 9.451567ms)
Feb  9 11:16:47.472: INFO: (3) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 9.519865ms)
Feb  9 11:16:47.473: INFO: (3) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 10.436775ms)
Feb  9 11:16:47.473: INFO: (3) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 10.595044ms)
Feb  9 11:16:47.477: INFO: (4) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 3.655321ms)
Feb  9 11:16:47.477: INFO: (4) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 3.955946ms)
Feb  9 11:16:47.478: INFO: (4) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 4.461762ms)
Feb  9 11:16:47.478: INFO: (4) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 4.964496ms)
Feb  9 11:16:47.479: INFO: (4) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 5.189473ms)
Feb  9 11:16:47.480: INFO: (4) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 6.001154ms)
Feb  9 11:16:47.481: INFO: (4) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 7.061343ms)
Feb  9 11:16:47.481: INFO: (4) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 7.682885ms)
Feb  9 11:16:47.481: INFO: (4) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 7.426888ms)
Feb  9 11:16:47.481: INFO: (4) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 7.299999ms)
Feb  9 11:16:47.481: INFO: (4) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 7.40924ms)
Feb  9 11:16:47.481: INFO: (4) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 7.673277ms)
Feb  9 11:16:47.481: INFO: (4) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 7.352049ms)
Feb  9 11:16:47.481: INFO: (4) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 7.921593ms)
Feb  9 11:16:47.481: INFO: (4) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 7.587508ms)
Feb  9 11:16:47.482: INFO: (4) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 8.832423ms)
Feb  9 11:16:47.488: INFO: (5) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 5.212793ms)
Feb  9 11:16:47.488: INFO: (5) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 5.575559ms)
Feb  9 11:16:47.490: INFO: (5) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 7.530278ms)
Feb  9 11:16:47.490: INFO: (5) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 7.555267ms)
Feb  9 11:16:47.490: INFO: (5) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 7.885044ms)
Feb  9 11:16:47.491: INFO: (5) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 8.18578ms)
Feb  9 11:16:47.491: INFO: (5) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 8.218671ms)
Feb  9 11:16:47.491: INFO: (5) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 8.482727ms)
Feb  9 11:16:47.492: INFO: (5) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 8.703925ms)
Feb  9 11:16:47.492: INFO: (5) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 9.380458ms)
Feb  9 11:16:47.493: INFO: (5) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 10.156929ms)
Feb  9 11:16:47.494: INFO: (5) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 10.979441ms)
Feb  9 11:16:47.494: INFO: (5) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 10.805113ms)
Feb  9 11:16:47.494: INFO: (5) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 10.745923ms)
Feb  9 11:16:47.494: INFO: (5) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 11.103728ms)
Feb  9 11:16:47.494: INFO: (5) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 10.814623ms)
Feb  9 11:16:47.499: INFO: (6) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 4.708108ms)
Feb  9 11:16:47.501: INFO: (6) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 6.807786ms)
Feb  9 11:16:47.501: INFO: (6) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 6.835435ms)
Feb  9 11:16:47.502: INFO: (6) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 7.812275ms)
Feb  9 11:16:47.502: INFO: (6) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 7.643647ms)
Feb  9 11:16:47.502: INFO: (6) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 7.575567ms)
Feb  9 11:16:47.503: INFO: (6) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 8.798575ms)
Feb  9 11:16:47.503: INFO: (6) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 8.759214ms)
Feb  9 11:16:47.503: INFO: (6) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 8.640706ms)
Feb  9 11:16:47.503: INFO: (6) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 9.074821ms)
Feb  9 11:16:47.504: INFO: (6) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 9.453257ms)
Feb  9 11:16:47.506: INFO: (6) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 11.672312ms)
Feb  9 11:16:47.506: INFO: (6) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 11.775122ms)
Feb  9 11:16:47.507: INFO: (6) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 12.623392ms)
Feb  9 11:16:47.508: INFO: (6) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 13.7324ms)
Feb  9 11:16:47.508: INFO: (6) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 13.594842ms)
Feb  9 11:16:47.512: INFO: (7) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 3.964067ms)
Feb  9 11:16:47.513: INFO: (7) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 4.950877ms)
Feb  9 11:16:47.514: INFO: (7) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 5.738627ms)
Feb  9 11:16:47.515: INFO: (7) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 6.483989ms)
Feb  9 11:16:47.515: INFO: (7) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 6.479919ms)
Feb  9 11:16:47.515: INFO: (7) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 6.754296ms)
Feb  9 11:16:47.515: INFO: (7) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 7.148712ms)
Feb  9 11:16:47.515: INFO: (7) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 6.887194ms)
Feb  9 11:16:47.516: INFO: (7) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 7.097542ms)
Feb  9 11:16:47.516: INFO: (7) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 7.644176ms)
Feb  9 11:16:47.516: INFO: (7) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 7.874093ms)
Feb  9 11:16:47.517: INFO: (7) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 8.015653ms)
Feb  9 11:16:47.517: INFO: (7) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 8.415167ms)
Feb  9 11:16:47.519: INFO: (7) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 10.363606ms)
Feb  9 11:16:47.519: INFO: (7) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 10.692733ms)
Feb  9 11:16:47.519: INFO: (7) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 10.793292ms)
Feb  9 11:16:47.524: INFO: (8) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 3.949757ms)
Feb  9 11:16:47.524: INFO: (8) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 4.936106ms)
Feb  9 11:16:47.524: INFO: (8) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 4.094175ms)
Feb  9 11:16:47.524: INFO: (8) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 4.58866ms)
Feb  9 11:16:47.524: INFO: (8) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 5.101353ms)
Feb  9 11:16:47.525: INFO: (8) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 4.968245ms)
Feb  9 11:16:47.525: INFO: (8) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 4.5763ms)
Feb  9 11:16:47.525: INFO: (8) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 4.937175ms)
Feb  9 11:16:47.526: INFO: (8) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 5.236663ms)
Feb  9 11:16:47.526: INFO: (8) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 6.602768ms)
Feb  9 11:16:47.526: INFO: (8) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 6.019144ms)
Feb  9 11:16:47.526: INFO: (8) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 5.389311ms)
Feb  9 11:16:47.527: INFO: (8) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 6.341621ms)
Feb  9 11:16:47.527: INFO: (8) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 5.749948ms)
Feb  9 11:16:47.527: INFO: (8) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 6.944515ms)
Feb  9 11:16:47.527: INFO: (8) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 6.686677ms)
Feb  9 11:16:47.532: INFO: (9) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 4.904426ms)
Feb  9 11:16:47.532: INFO: (9) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 4.583579ms)
Feb  9 11:16:47.532: INFO: (9) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 4.592559ms)
Feb  9 11:16:47.533: INFO: (9) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 5.066334ms)
Feb  9 11:16:47.534: INFO: (9) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 6.644437ms)
Feb  9 11:16:47.535: INFO: (9) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 6.839485ms)
Feb  9 11:16:47.535: INFO: (9) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 7.446669ms)
Feb  9 11:16:47.535: INFO: (9) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 7.509738ms)
Feb  9 11:16:47.535: INFO: (9) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 7.623177ms)
Feb  9 11:16:47.535: INFO: (9) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 7.457638ms)
Feb  9 11:16:47.536: INFO: (9) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 8.421467ms)
Feb  9 11:16:47.536: INFO: (9) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 8.644246ms)
Feb  9 11:16:47.536: INFO: (9) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 8.587736ms)
Feb  9 11:16:47.536: INFO: (9) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 8.596046ms)
Feb  9 11:16:47.537: INFO: (9) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 8.705005ms)
Feb  9 11:16:47.537: INFO: (9) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 8.830123ms)
Feb  9 11:16:47.540: INFO: (10) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 2.893929ms)
Feb  9 11:16:47.541: INFO: (10) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 3.259705ms)
Feb  9 11:16:47.541: INFO: (10) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 3.637971ms)
Feb  9 11:16:47.543: INFO: (10) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 5.665638ms)
Feb  9 11:16:47.543: INFO: (10) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 5.790367ms)
Feb  9 11:16:47.543: INFO: (10) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 5.719277ms)
Feb  9 11:16:47.543: INFO: (10) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 5.813906ms)
Feb  9 11:16:47.543: INFO: (10) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 5.672707ms)
Feb  9 11:16:47.543: INFO: (10) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 5.787117ms)
Feb  9 11:16:47.543: INFO: (10) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 5.928695ms)
Feb  9 11:16:47.543: INFO: (10) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 6.285581ms)
Feb  9 11:16:47.544: INFO: (10) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 6.533309ms)
Feb  9 11:16:47.544: INFO: (10) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 7.118962ms)
Feb  9 11:16:47.544: INFO: (10) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 6.943504ms)
Feb  9 11:16:47.545: INFO: (10) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 7.482528ms)
Feb  9 11:16:47.545: INFO: (10) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 7.32687ms)
Feb  9 11:16:47.548: INFO: (11) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 3.735549ms)
Feb  9 11:16:47.549: INFO: (11) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 3.595642ms)
Feb  9 11:16:47.549: INFO: (11) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 3.719829ms)
Feb  9 11:16:47.550: INFO: (11) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 4.840337ms)
Feb  9 11:16:47.550: INFO: (11) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 4.848996ms)
Feb  9 11:16:47.550: INFO: (11) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 5.047264ms)
Feb  9 11:16:47.550: INFO: (11) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 4.823107ms)
Feb  9 11:16:47.550: INFO: (11) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 5.063374ms)
Feb  9 11:16:47.550: INFO: (11) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 5.172142ms)
Feb  9 11:16:47.551: INFO: (11) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 5.155674ms)
Feb  9 11:16:47.551: INFO: (11) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 5.55321ms)
Feb  9 11:16:47.551: INFO: (11) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 5.822486ms)
Feb  9 11:16:47.551: INFO: (11) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 6.135533ms)
Feb  9 11:16:47.552: INFO: (11) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 6.819146ms)
Feb  9 11:16:47.552: INFO: (11) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 6.281912ms)
Feb  9 11:16:47.552: INFO: (11) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 6.856505ms)
Feb  9 11:16:47.555: INFO: (12) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 2.350285ms)
Feb  9 11:16:47.555: INFO: (12) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 2.914059ms)
Feb  9 11:16:47.555: INFO: (12) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 3.444383ms)
Feb  9 11:16:47.558: INFO: (12) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 5.464031ms)
Feb  9 11:16:47.558: INFO: (12) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 5.691897ms)
Feb  9 11:16:47.558: INFO: (12) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 5.796616ms)
Feb  9 11:16:47.558: INFO: (12) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 5.873315ms)
Feb  9 11:16:47.559: INFO: (12) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 5.771447ms)
Feb  9 11:16:47.559: INFO: (12) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 6.717938ms)
Feb  9 11:16:47.559: INFO: (12) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 6.44971ms)
Feb  9 11:16:47.560: INFO: (12) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 7.113392ms)
Feb  9 11:16:47.560: INFO: (12) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 7.36457ms)
Feb  9 11:16:47.560: INFO: (12) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 7.137722ms)
Feb  9 11:16:47.560: INFO: (12) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 7.016764ms)
Feb  9 11:16:47.561: INFO: (12) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 7.787225ms)
Feb  9 11:16:47.561: INFO: (12) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 7.760385ms)
Feb  9 11:16:47.567: INFO: (13) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 6.227042ms)
Feb  9 11:16:47.567: INFO: (13) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 4.792898ms)
Feb  9 11:16:47.568: INFO: (13) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 6.008415ms)
Feb  9 11:16:47.568: INFO: (13) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 6.880446ms)
Feb  9 11:16:47.569: INFO: (13) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 7.685377ms)
Feb  9 11:16:47.569: INFO: (13) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 7.143492ms)
Feb  9 11:16:47.570: INFO: (13) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 8.269799ms)
Feb  9 11:16:47.570: INFO: (13) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 8.466497ms)
Feb  9 11:16:47.570: INFO: (13) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 8.030553ms)
Feb  9 11:16:47.570: INFO: (13) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 8.841174ms)
Feb  9 11:16:47.570: INFO: (13) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 9.642655ms)
Feb  9 11:16:47.570: INFO: (13) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 8.062473ms)
Feb  9 11:16:47.570: INFO: (13) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 8.583636ms)
Feb  9 11:16:47.571: INFO: (13) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 8.067902ms)
Feb  9 11:16:47.571: INFO: (13) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 8.2239ms)
Feb  9 11:16:47.571: INFO: (13) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 8.542096ms)
Feb  9 11:16:47.576: INFO: (14) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 4.770118ms)
Feb  9 11:16:47.576: INFO: (14) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 4.676039ms)
Feb  9 11:16:47.576: INFO: (14) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 4.903816ms)
Feb  9 11:16:47.576: INFO: (14) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 4.785048ms)
Feb  9 11:16:47.577: INFO: (14) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 5.234243ms)
Feb  9 11:16:47.577: INFO: (14) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 5.218734ms)
Feb  9 11:16:47.577: INFO: (14) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 5.678889ms)
Feb  9 11:16:47.580: INFO: (14) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 7.995963ms)
Feb  9 11:16:47.580: INFO: (14) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 8.18186ms)
Feb  9 11:16:47.580: INFO: (14) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 8.532787ms)
Feb  9 11:16:47.580: INFO: (14) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 8.416409ms)
Feb  9 11:16:47.580: INFO: (14) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 8.616726ms)
Feb  9 11:16:47.580: INFO: (14) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 8.33789ms)
Feb  9 11:16:47.580: INFO: (14) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 8.440829ms)
Feb  9 11:16:47.581: INFO: (14) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 9.302909ms)
Feb  9 11:16:47.581: INFO: (14) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 9.089012ms)
Feb  9 11:16:47.584: INFO: (15) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 3.054197ms)
Feb  9 11:16:47.585: INFO: (15) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 3.686661ms)
Feb  9 11:16:47.585: INFO: (15) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 4.069487ms)
Feb  9 11:16:47.586: INFO: (15) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 5.007906ms)
Feb  9 11:16:47.586: INFO: (15) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 4.766938ms)
Feb  9 11:16:47.586: INFO: (15) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 4.727119ms)
Feb  9 11:16:47.586: INFO: (15) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 4.938415ms)
Feb  9 11:16:47.586: INFO: (15) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 5.022256ms)
Feb  9 11:16:47.587: INFO: (15) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 5.805497ms)
Feb  9 11:16:47.588: INFO: (15) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 5.992964ms)
Feb  9 11:16:47.588: INFO: (15) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 6.888166ms)
Feb  9 11:16:47.588: INFO: (15) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 6.635577ms)
Feb  9 11:16:47.589: INFO: (15) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 7.198371ms)
Feb  9 11:16:47.590: INFO: (15) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 8.790264ms)
Feb  9 11:16:47.590: INFO: (15) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 9.001853ms)
Feb  9 11:16:47.591: INFO: (15) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 9.640264ms)
Feb  9 11:16:47.597: INFO: (16) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 6.063493ms)
Feb  9 11:16:47.598: INFO: (16) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 6.975695ms)
Feb  9 11:16:47.598: INFO: (16) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 6.742086ms)
Feb  9 11:16:47.598: INFO: (16) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 7.120642ms)
Feb  9 11:16:47.598: INFO: (16) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 6.888394ms)
Feb  9 11:16:47.598: INFO: (16) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 6.889404ms)
Feb  9 11:16:47.600: INFO: (16) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 9.15921ms)
Feb  9 11:16:47.600: INFO: (16) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 8.772744ms)
Feb  9 11:16:47.600: INFO: (16) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 8.833483ms)
Feb  9 11:16:47.600: INFO: (16) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 9.325019ms)
Feb  9 11:16:47.601: INFO: (16) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 9.653384ms)
Feb  9 11:16:47.601: INFO: (16) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 9.622395ms)
Feb  9 11:16:47.601: INFO: (16) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 9.718223ms)
Feb  9 11:16:47.601: INFO: (16) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 9.558345ms)
Feb  9 11:16:47.601: INFO: (16) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 10.140469ms)
Feb  9 11:16:47.602: INFO: (16) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 10.527934ms)
Feb  9 11:16:47.610: INFO: (17) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 7.706276ms)
Feb  9 11:16:47.610: INFO: (17) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 7.361809ms)
Feb  9 11:16:47.610: INFO: (17) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 7.685186ms)
Feb  9 11:16:47.610: INFO: (17) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 7.845745ms)
Feb  9 11:16:47.610: INFO: (17) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 7.777345ms)
Feb  9 11:16:47.610: INFO: (17) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 7.873444ms)
Feb  9 11:16:47.610: INFO: (17) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 8.532847ms)
Feb  9 11:16:47.610: INFO: (17) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 7.639747ms)
Feb  9 11:16:47.610: INFO: (17) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 8.554976ms)
Feb  9 11:16:47.610: INFO: (17) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 7.625058ms)
Feb  9 11:16:47.611: INFO: (17) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 8.508977ms)
Feb  9 11:16:47.611: INFO: (17) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 8.482488ms)
Feb  9 11:16:47.611: INFO: (17) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 9.146469ms)
Feb  9 11:16:47.611: INFO: (17) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 8.946252ms)
Feb  9 11:16:47.613: INFO: (17) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 11.028039ms)
Feb  9 11:16:47.613: INFO: (17) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 11.279327ms)
Feb  9 11:16:47.617: INFO: (18) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 3.898757ms)
Feb  9 11:16:47.619: INFO: (18) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 4.463502ms)
Feb  9 11:16:47.620: INFO: (18) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 5.723158ms)
Feb  9 11:16:47.620: INFO: (18) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 5.722197ms)
Feb  9 11:16:47.620: INFO: (18) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 6.4907ms)
Feb  9 11:16:47.620: INFO: (18) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 5.891895ms)
Feb  9 11:16:47.620: INFO: (18) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 5.412051ms)
Feb  9 11:16:47.620: INFO: (18) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 6.34587ms)
Feb  9 11:16:47.620: INFO: (18) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 6.579359ms)
Feb  9 11:16:47.621: INFO: (18) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 5.746508ms)
Feb  9 11:16:47.621: INFO: (18) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 5.671868ms)
Feb  9 11:16:47.622: INFO: (18) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 8.201611ms)
Feb  9 11:16:47.623: INFO: (18) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 7.856964ms)
Feb  9 11:16:47.623: INFO: (18) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 8.051331ms)
Feb  9 11:16:47.627: INFO: (18) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 11.90723ms)
Feb  9 11:16:47.627: INFO: (18) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 12.744901ms)
Feb  9 11:16:47.634: INFO: (19) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 6.39252ms)
Feb  9 11:16:47.634: INFO: (19) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw/proxy/rewriteme">test</a> (200; 6.42421ms)
Feb  9 11:16:47.634: INFO: (19) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:160/proxy/: foo (200; 6.986663ms)
Feb  9 11:16:47.634: INFO: (19) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">test<... (200; 6.687527ms)
Feb  9 11:16:47.634: INFO: (19) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:460/proxy/: tls baz (200; 6.956135ms)
Feb  9 11:16:47.636: INFO: (19) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:1080/proxy/rewriteme">... (200; 8.372338ms)
Feb  9 11:16:47.639: INFO: (19) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname1/proxy/: foo (200; 10.838622ms)
Feb  9 11:16:47.639: INFO: (19) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname2/proxy/: bar (200; 11.085899ms)
Feb  9 11:16:47.639: INFO: (19) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname2/proxy/: tls qux (200; 10.866341ms)
Feb  9 11:16:47.640: INFO: (19) /api/v1/namespaces/proxy-7336/pods/proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 12.041608ms)
Feb  9 11:16:47.641: INFO: (19) /api/v1/namespaces/proxy-7336/pods/http:proxy-service-4fsqm-9b7fw:162/proxy/: bar (200; 13.534923ms)
Feb  9 11:16:47.644: INFO: (19) /api/v1/namespaces/proxy-7336/services/https:proxy-service-4fsqm:tlsportname1/proxy/: tls baz (200; 16.343891ms)
Feb  9 11:16:47.644: INFO: (19) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:462/proxy/: tls qux (200; 16.535019ms)
Feb  9 11:16:47.644: INFO: (19) /api/v1/namespaces/proxy-7336/services/http:proxy-service-4fsqm:portname1/proxy/: foo (200; 16.738647ms)
Feb  9 11:16:47.644: INFO: (19) /api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/: <a href="/api/v1/namespaces/proxy-7336/pods/https:proxy-service-4fsqm-9b7fw:443/proxy/tlsrewritem... (200; 16.828786ms)
Feb  9 11:16:47.645: INFO: (19) /api/v1/namespaces/proxy-7336/services/proxy-service-4fsqm:portname2/proxy/: bar (200; 16.861685ms)
STEP: deleting ReplicationController proxy-service-4fsqm in namespace proxy-7336, will wait for the garbage collector to delete the pods
Feb  9 11:16:47.712: INFO: Deleting ReplicationController proxy-service-4fsqm took: 11.745391ms
Feb  9 11:16:48.713: INFO: Terminating ReplicationController proxy-service-4fsqm pods took: 1.000974238s
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:18:00.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7336" for this suite.

• [SLOW TEST:84.208 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":294,"skipped":5059,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:18:00.248: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-1425
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Feb  9 11:18:00.420: INFO: Waiting up to 1m0s for all nodes to be ready
Feb  9 11:19:00.483: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Feb  9 11:19:00.524: INFO: Created pod: pod0-sched-preemption-low-priority
Feb  9 11:19:00.575: INFO: Created pod: pod1-sched-preemption-medium-priority
Feb  9 11:19:00.596: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:19:36.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1425" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:96.535 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":295,"skipped":5059,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:19:36.783: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb  9 11:19:37.598: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb  9 11:19:39.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748466377, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748466377, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63748466377, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63748466377, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb  9 11:19:42.645: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:19:42.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8361" for this suite.
STEP: Destroying namespace "webhook-8361-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.066 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":296,"skipped":5064,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:19:42.850: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-2882a5d7-6836-4074-88cf-61ae63fa34df
STEP: Creating a pod to test consume secrets
Feb  9 11:19:43.166: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e75c336-4339-4173-aed4-9c98a787ebbd" in namespace "projected-536" to be "Succeeded or Failed"
Feb  9 11:19:43.173: INFO: Pod "pod-projected-secrets-8e75c336-4339-4173-aed4-9c98a787ebbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.226762ms
Feb  9 11:19:45.180: INFO: Pod "pod-projected-secrets-8e75c336-4339-4173-aed4-9c98a787ebbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013719006s
Feb  9 11:19:47.193: INFO: Pod "pod-projected-secrets-8e75c336-4339-4173-aed4-9c98a787ebbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026818271s
STEP: Saw pod success
Feb  9 11:19:47.194: INFO: Pod "pod-projected-secrets-8e75c336-4339-4173-aed4-9c98a787ebbd" satisfied condition "Succeeded or Failed"
Feb  9 11:19:47.197: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-projected-secrets-8e75c336-4339-4173-aed4-9c98a787ebbd container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb  9 11:19:47.319: INFO: Waiting for pod pod-projected-secrets-8e75c336-4339-4173-aed4-9c98a787ebbd to disappear
Feb  9 11:19:47.324: INFO: Pod pod-projected-secrets-8e75c336-4339-4173-aed4-9c98a787ebbd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:19:47.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-536" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":5097,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:19:47.336: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 11:19:47.509: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e5057a4-6713-4a63-9601-7233c0742474" in namespace "projected-3867" to be "Succeeded or Failed"
Feb  9 11:19:47.517: INFO: Pod "downwardapi-volume-0e5057a4-6713-4a63-9601-7233c0742474": Phase="Pending", Reason="", readiness=false. Elapsed: 7.555968ms
Feb  9 11:19:49.526: INFO: Pod "downwardapi-volume-0e5057a4-6713-4a63-9601-7233c0742474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016978101s
Feb  9 11:19:51.540: INFO: Pod "downwardapi-volume-0e5057a4-6713-4a63-9601-7233c0742474": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030541468s
STEP: Saw pod success
Feb  9 11:19:51.540: INFO: Pod "downwardapi-volume-0e5057a4-6713-4a63-9601-7233c0742474" satisfied condition "Succeeded or Failed"
Feb  9 11:19:51.543: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod downwardapi-volume-0e5057a4-6713-4a63-9601-7233c0742474 container client-container: <nil>
STEP: delete the pod
Feb  9 11:19:51.669: INFO: Waiting for pod downwardapi-volume-0e5057a4-6713-4a63-9601-7233c0742474 to disappear
Feb  9 11:19:51.674: INFO: Pod downwardapi-volume-0e5057a4-6713-4a63-9601-7233c0742474 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:19:51.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3867" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":298,"skipped":5107,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:19:51.687: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb  9 11:20:01.922: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:20:01.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0209 11:20:01.922819      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0209 11:20:01.922875      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0209 11:20:01.922888      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4028" for this suite.

• [SLOW TEST:10.253 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":299,"skipped":5137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:20:01.949: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-913
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:20:04.234: INFO: Waiting up to 5m0s for pod "client-envvars-a67415f5-c538-4147-b9b3-04be6ae1baf9" in namespace "pods-913" to be "Succeeded or Failed"
Feb  9 11:20:04.242: INFO: Pod "client-envvars-a67415f5-c538-4147-b9b3-04be6ae1baf9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.181222ms
Feb  9 11:20:06.254: INFO: Pod "client-envvars-a67415f5-c538-4147-b9b3-04be6ae1baf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01967604s
STEP: Saw pod success
Feb  9 11:20:06.254: INFO: Pod "client-envvars-a67415f5-c538-4147-b9b3-04be6ae1baf9" satisfied condition "Succeeded or Failed"
Feb  9 11:20:06.256: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod client-envvars-a67415f5-c538-4147-b9b3-04be6ae1baf9 container env3cont: <nil>
STEP: delete the pod
Feb  9 11:20:06.325: INFO: Waiting for pod client-envvars-a67415f5-c538-4147-b9b3-04be6ae1baf9 to disappear
Feb  9 11:20:06.329: INFO: Pod client-envvars-a67415f5-c538-4147-b9b3-04be6ae1baf9 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:20:06.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-913" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":300,"skipped":5174,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:20:06.337: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4761
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-4761
Feb  9 11:20:06.513: INFO: Found 0 stateful pods, waiting for 1
Feb  9 11:20:16.523: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Feb  9 11:20:16.549: INFO: Deleting all statefulset in ns statefulset-4761
Feb  9 11:20:16.565: INFO: Scaling statefulset ss to 0
Feb  9 11:21:16.618: INFO: Waiting for statefulset status.replicas updated to 0
Feb  9 11:21:16.623: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:21:16.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4761" for this suite.

• [SLOW TEST:70.328 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":301,"skipped":5183,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:21:16.666: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2591
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb  9 11:21:22.894: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:21:22.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0209 11:21:22.894889      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0209 11:21:22.894915      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0209 11:21:22.894920      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2591" for this suite.

• [SLOW TEST:6.253 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":302,"skipped":5186,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:21:22.919: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8176
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8176.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8176.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8176.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8176.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8176.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8176.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb  9 11:21:27.136: INFO: Unable to read wheezy_udp@PodARecord from pod dns-8176/dns-test-a9ccda69-3f14-4b91-a0fc-8da4095805b3: the server could not find the requested resource (get pods dns-test-a9ccda69-3f14-4b91-a0fc-8da4095805b3)
Feb  9 11:21:27.140: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-8176/dns-test-a9ccda69-3f14-4b91-a0fc-8da4095805b3: the server could not find the requested resource (get pods dns-test-a9ccda69-3f14-4b91-a0fc-8da4095805b3)
Feb  9 11:21:27.152: INFO: Unable to read jessie_udp@PodARecord from pod dns-8176/dns-test-a9ccda69-3f14-4b91-a0fc-8da4095805b3: the server could not find the requested resource (get pods dns-test-a9ccda69-3f14-4b91-a0fc-8da4095805b3)
Feb  9 11:21:27.156: INFO: Unable to read jessie_tcp@PodARecord from pod dns-8176/dns-test-a9ccda69-3f14-4b91-a0fc-8da4095805b3: the server could not find the requested resource (get pods dns-test-a9ccda69-3f14-4b91-a0fc-8da4095805b3)
Feb  9 11:21:27.156: INFO: Lookups using dns-8176/dns-test-a9ccda69-3f14-4b91-a0fc-8da4095805b3 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

Feb  9 11:21:32.186: INFO: DNS probes using dns-8176/dns-test-a9ccda69-3f14-4b91-a0fc-8da4095805b3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:21:32.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8176" for this suite.

• [SLOW TEST:9.325 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":303,"skipped":5196,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:21:32.245: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8489
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-771cd1a7-7192-4a14-86a5-002a7a1094f7
STEP: Creating a pod to test consume secrets
Feb  9 11:21:32.431: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-84e060b8-d75c-4536-86e5-edfa6940976c" in namespace "projected-8489" to be "Succeeded or Failed"
Feb  9 11:21:32.446: INFO: Pod "pod-projected-secrets-84e060b8-d75c-4536-86e5-edfa6940976c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.406643ms
Feb  9 11:21:34.458: INFO: Pod "pod-projected-secrets-84e060b8-d75c-4536-86e5-edfa6940976c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026440586s
STEP: Saw pod success
Feb  9 11:21:34.458: INFO: Pod "pod-projected-secrets-84e060b8-d75c-4536-86e5-edfa6940976c" satisfied condition "Succeeded or Failed"
Feb  9 11:21:34.462: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-1 pod pod-projected-secrets-84e060b8-d75c-4536-86e5-edfa6940976c container secret-volume-test: <nil>
STEP: delete the pod
Feb  9 11:21:34.491: INFO: Waiting for pod pod-projected-secrets-84e060b8-d75c-4536-86e5-edfa6940976c to disappear
Feb  9 11:21:34.495: INFO: Pod pod-projected-secrets-84e060b8-d75c-4536-86e5-edfa6940976c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:21:34.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8489" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":304,"skipped":5206,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:21:34.508: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-6641
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb  9 11:21:34.656: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb  9 11:21:34.711: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  9 11:21:36.721: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb  9 11:21:38.728: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 11:21:40.728: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 11:21:42.726: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 11:21:44.724: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 11:21:46.727: INFO: The status of Pod netserver-0 is Running (Ready = false)
Feb  9 11:21:48.724: INFO: The status of Pod netserver-0 is Running (Ready = true)
Feb  9 11:21:48.731: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb  9 11:21:50.745: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb  9 11:21:52.748: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb  9 11:21:54.746: INFO: The status of Pod netserver-1 is Running (Ready = false)
Feb  9 11:21:56.747: INFO: The status of Pod netserver-1 is Running (Ready = true)
Feb  9 11:21:56.755: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Feb  9 11:21:58.789: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb  9 11:21:58.789: INFO: Breadth first check of 10.100.239.160 on host 10.0.0.248...
Feb  9 11:21:58.791: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.239.162:9080/dial?request=hostname&protocol=http&host=10.100.239.160&port=8080&tries=1'] Namespace:pod-network-test-6641 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:21:58.792: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 11:21:58.990: INFO: Waiting for responses: map[]
Feb  9 11:21:58.990: INFO: reached 10.100.239.160 after 0/1 tries
Feb  9 11:21:58.990: INFO: Breadth first check of 10.100.46.171 on host 10.0.0.5...
Feb  9 11:21:58.996: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.239.162:9080/dial?request=hostname&protocol=http&host=10.100.46.171&port=8080&tries=1'] Namespace:pod-network-test-6641 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:21:58.996: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 11:21:59.190: INFO: Waiting for responses: map[]
Feb  9 11:21:59.190: INFO: reached 10.100.46.171 after 0/1 tries
Feb  9 11:21:59.190: INFO: Breadth first check of 10.100.80.27 on host 10.0.0.176...
Feb  9 11:21:59.196: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.239.162:9080/dial?request=hostname&protocol=http&host=10.100.80.27&port=8080&tries=1'] Namespace:pod-network-test-6641 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb  9 11:21:59.196: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
Feb  9 11:21:59.390: INFO: Waiting for responses: map[]
Feb  9 11:21:59.390: INFO: reached 10.100.80.27 after 0/1 tries
Feb  9 11:21:59.390: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:21:59.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6641" for this suite.

• [SLOW TEST:24.898 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5225,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:21:59.407: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3154
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4779
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:22:28.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3608" for this suite.
STEP: Destroying namespace "nsdeletetest-3154" for this suite.
Feb  9 11:22:28.950: INFO: Namespace nsdeletetest-3154 was already deleted
STEP: Destroying namespace "nsdeletetest-4779" for this suite.

• [SLOW TEST:29.549 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":306,"skipped":5232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:22:28.957: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Feb  9 11:22:29.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-815025086 --namespace=kubectl-538 version'
Feb  9 11:22:29.231: INFO: stderr: ""
Feb  9 11:22:29.231: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.2\", GitCommit:\"faecb196815e248d3ecfb03c680a4507229c2a56\", GitTreeState:\"clean\", BuildDate:\"2021-01-13T13:28:09Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.2\", GitCommit:\"faecb196815e248d3ecfb03c680a4507229c2a56\", GitTreeState:\"clean\", BuildDate:\"2021-01-13T13:20:00Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:22:29.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-538" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":307,"skipped":5273,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:22:29.246: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-819
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Feb  9 11:22:29.442: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Feb  9 11:22:29.480: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:22:29.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-819" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":308,"skipped":5290,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:22:29.538: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb  9 11:22:29.718: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5339  c1725467-248f-48a6-adbc-eba0d1e2d84f 50441 0 2021-02-09 11:22:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-09 11:22:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 11:22:29.718: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5339  c1725467-248f-48a6-adbc-eba0d1e2d84f 50442 0 2021-02-09 11:22:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-09 11:22:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 11:22:29.719: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5339  c1725467-248f-48a6-adbc-eba0d1e2d84f 50443 0 2021-02-09 11:22:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-09 11:22:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb  9 11:22:39.769: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5339  c1725467-248f-48a6-adbc-eba0d1e2d84f 50505 0 2021-02-09 11:22:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-09 11:22:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 11:22:39.770: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5339  c1725467-248f-48a6-adbc-eba0d1e2d84f 50506 0 2021-02-09 11:22:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-09 11:22:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb  9 11:22:39.770: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5339  c1725467-248f-48a6-adbc-eba0d1e2d84f 50507 0 2021-02-09 11:22:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-02-09 11:22:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:22:39.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5339" for this suite.

• [SLOW TEST:10.245 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":309,"skipped":5318,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:22:39.784: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:22:41.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9597" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":310,"skipped":5328,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Feb  9 11:22:41.998: INFO: >>> kubeConfig: /tmp/kubeconfig-815025086
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Feb  9 11:22:42.165: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5824c047-bde1-4cd0-bbdd-07ef4e1ec66b" in namespace "projected-3725" to be "Succeeded or Failed"
Feb  9 11:22:42.171: INFO: Pod "downwardapi-volume-5824c047-bde1-4cd0-bbdd-07ef4e1ec66b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.282602ms
Feb  9 11:22:44.180: INFO: Pod "downwardapi-volume-5824c047-bde1-4cd0-bbdd-07ef4e1ec66b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014426628s
STEP: Saw pod success
Feb  9 11:22:44.180: INFO: Pod "downwardapi-volume-5824c047-bde1-4cd0-bbdd-07ef4e1ec66b" satisfied condition "Succeeded or Failed"
Feb  9 11:22:44.185: INFO: Trying to get logs from node v1-kube1-20-2-apco5j2qoq5i-node-0 pod downwardapi-volume-5824c047-bde1-4cd0-bbdd-07ef4e1ec66b container client-container: <nil>
STEP: delete the pod
Feb  9 11:22:44.258: INFO: Waiting for pod downwardapi-volume-5824c047-bde1-4cd0-bbdd-07ef4e1ec66b to disappear
Feb  9 11:22:44.262: INFO: Pod downwardapi-volume-5824c047-bde1-4cd0-bbdd-07ef4e1ec66b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Feb  9 11:22:44.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3725" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":311,"skipped":5331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSFeb  9 11:22:44.284: INFO: Running AfterSuite actions on all nodes
Feb  9 11:22:44.284: INFO: Running AfterSuite actions on node 1
Feb  9 11:22:44.284: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 8092.655 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 2h14m54.624430705s
Test Suite Passed
